{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SXRp4LcnC4I"
   },
   "source": [
    "# Extracting and storing RDF information and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7wQupQrnC4J"
   },
   "outputs": [],
   "source": [
    "# # internal note: uncomment these rows to install packages... colaboratory says they \n",
    "# # are not present. we only get max 12 hour connection to the cloud each time. \n",
    "# # whenever we reconnect, we get a new instance of the virtual machine, which \n",
    "# # won't have these non-standard packages. \n",
    "# ! pip install wikipedia\n",
    "# ! pip install wptools\n",
    "# ! pip install SPARQLWrapper\n",
    "# ! pip install textblob\n",
    "# ! pip install nltk\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wptools, wikipedia\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from rdflib import Graph\n",
    "import csv, datetime, time, random, collections, string, re, urllib  \n",
    "from textblob import TextBlob\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIVWTPMRexID"
   },
   "source": [
    "### General overview\n",
    "\n",
    "1.  2 sets of classes - 1 to handle PG author-title mining, filtering and selection. The other to pre-process author\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObFltmPBnC4M"
   },
   "outputs": [],
   "source": [
    "# TO DO LIST: \n",
    "\n",
    "# 2. write a function to randomly select min_books number of books from each author's pool of books. Effectively, this only affects \n",
    "# authors with more than the min_books level specified. \n",
    "\n",
    "# 4. complete populate_corpus in GutenbergCorpusBuilder\n",
    "\n",
    "# 5. complete the database question - MongoDB (2 options - load a full GutenbergCorpusBuilder output, or author by author)\n",
    "\n",
    "# 6. review and add to documentation and comments for clarity and completeness \n",
    "\n",
    "# 7. draw a diagram of the database model \n",
    "\n",
    "\n",
    "\n",
    "## DONE \n",
    "# 1. Complete comments and docstring for _cleansegment_book \n",
    "\n",
    "# 3. complete _get_literarymovement, _get_authorabstract, _build_subcorpus, populate_attributes and write_to_file of the Author class (note that for _get_authorabstract\n",
    "# the GutenbergCorpusBuilder's _get_bookswiki_info method has been expanded to grab wikipedia pages on the PG website, where available. This can be passed into \n",
    "# the wikipedia package to easily get the pages. However, a small problem exists, some of PG's wikilinks uses non-unicode symbols \n",
    "# e.g. href=\"http://en.wikipedia.org/wiki/Fern%C3%A1n_Caballero\" passing \"Fern%C3%A1n_Caballero\" takes us to the wrong wikipedia page (solved - with urllib.requests.unquote)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUOWBinlnC4Z"
   },
   "source": [
    "### 1. A class to store a corpus obtained from the Project Gutenberg website. \n",
    "\n",
    "1. The corpus is build with functions within the class that filter the authors and books on the Project Gutenberg website. \n",
    "2. It also calls on the Author class, to process and generate information about sentences from an author's books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfplnCWJnC4Z"
   },
   "outputs": [],
   "source": [
    "class GutenbergCorpusBuilder: \n",
    "    \n",
    "    def __init__(self, corpusname):\n",
    "        '''\n",
    "        initiates a Gutenberg object which stores information about selected authors available on the \n",
    "        PG website. Authors are stored based on their unique PG numerical code. For each author,  \n",
    "        filtered books and their respective PG URL is also stored. \n",
    "        '''\n",
    "        self.corpusname = corpusname\n",
    "        self.corpusversion = \"v\"+ str(datetime.datetime.now().year) + str(datetime.datetime.now().month) +\\\n",
    "        str(datetime.datetime.now().day)\n",
    "        \n",
    "        self.authors = {}\n",
    "        # a dictionary nested with dictionaries. the top level keys are the unique numbers for authors \n",
    "        # on the Project Gutenberg website, the values are dictionaries containing author information,\n",
    "        # names, books (in a dictionary). \n",
    "        \n",
    "        self.corpus = {}\n",
    "        # a dictionary containing sets of sentences selected from each author's filtered books.\n",
    "        # the top level keys are the unique numbers for authors, the values are sets containing \n",
    "        # sentences from an author's books (as strings. \n",
    "        \n",
    "    \n",
    "    def populate_corpus(self):\n",
    "        '''\n",
    "        for each of the author in self.authors, generate an Author class instance, populates all \n",
    "        attributes of the Author class, adds to self.corpus.\n",
    "        \n",
    "        inputs | \n",
    "        outputs | \n",
    "        \n",
    "        '''\n",
    "        if len(self.author) >= 0:\n",
    "            for author in self.author:\n",
    "                __author = Author()\n",
    "                __author.populate_attributes()\n",
    "        pass \n",
    "        \n",
    "    \n",
    "    def get_library(self, min_books = 1, languages = \"all\", roles = \"all\"):\n",
    "        '''\n",
    "        \n",
    "        Goes through the PG website's 'sort by author' pages. Extracts author and corresponding book \n",
    "        information that meet a number of selection criterion (see inputs). \n",
    "        inputs | \n",
    "        1. min_books:int - the minimum number of books available for an author, which meets the languages \n",
    "        and roles parameters. default value is 1. \n",
    "        5. languages: either a str \"all\", or a list containing the languages (in lowercase) to count towards \n",
    "        the author's min_books level. The list of languages available can be found here \n",
    "        https://www.gutenberg.org/catalog/. default is \"all\". \n",
    "        6. roles: either a str \"all\", or a list containing the roles that an author can have in a book. \n",
    "        These include: Commentator, Translator, Contributor, Photographer, Illustrator, Commentator, Editor\n",
    "        default value is \"all\".\n",
    "        outputs | saves the results to self.authors\n",
    "        \n",
    "        '''\n",
    "        charlist = []\n",
    "        charlist[:0] = [letter for letter in string.ascii_lowercase] + [\"other\"]\n",
    "\n",
    "        library = dict()\n",
    "        for char in charlist:\n",
    "            # Team comment: we select the authors and books via the \"Browse by Author\" lists instead of the \n",
    "            # \"Browse by Books\" list. Although the latter has a more predictable page structure \n",
    "            # (i.e. 1 book name, followed by 1 author name, recursively), the former includes \n",
    "            # information about the Author's role in the book. We believe that this could have\n",
    "            # a meaningful impact on the predictive capabilities for models on different tasks, \n",
    "            # especially at larger scale.  \n",
    "            link = 'https://www.gutenberg.org/browse/authors/'+ char\n",
    "            page = requests.get(link)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            one_letter = self._unite_authors_nums_books(self._get_authors_numsnames(soup)[0],\\\n",
    "                                                            self._get_authors_numsnames(soup)[1],\\\n",
    "                                                            self._get_bookswiki_info(soup)[0],\\\n",
    "                                                            self._get_bookswiki_info(soup)[1],\\\n",
    "                                                            min_books, languages, roles)\n",
    "            \n",
    "            library.update(one_letter)\n",
    "            print(\"{} authors from the {} alphabetical category have been added. \".format(len(one_letter),char))\n",
    "            \n",
    "            # put the function to sleep for a randomised number of seconds (non-integer number between 2 and 8)\n",
    "            # to mimic human surfing patterns. some ethical considerations here \n",
    "            time.sleep(random.uniform(2,8))\n",
    "            \n",
    "        self.authors = library \n",
    "    \n",
    "    def _get_authors_numsnames(self, soup):\n",
    "        '''\n",
    "        A helper function for __unite_authors_nums_books__. Extracts all author names from a BeautifulSoup copy  \n",
    "        of a 'Browse by Author' page on the PG website. \n",
    "        inputs | soup:a BeautifulSoup object - containg a copy of the PG 'Browse by Author' page. \n",
    "        outputs | a tuple containing two lists. the first contains author numbers on the page, the second contains\n",
    "        corresponding author names on the page. \n",
    "        '''\n",
    "        authornames = []\n",
    "        # the author names are stored within the \"name\" attribute under each \"a\" class\n",
    "        # use regex wildcard so that find_all will catch and return all \"a names\" with values\n",
    "        authorname_BSlist = soup.find_all('a', {\"name\":re.compile(\"\\w*\")})\n",
    "\n",
    "        for authorname in authorname_BSlist:\n",
    "            # \\- and \\? to escape special characters. .rstrip to remove trailing whitespaces. \n",
    "            authornames.append(re.sub(r'[0-9,\\-\\?]*', '', authorname.text).rstrip())\n",
    "\n",
    "        authornums = []\n",
    "        # the author numbers are stored within the \"href\" attribute. Every line for a book \n",
    "        # on the page has a \"title\" attribute with the value \"Link to this author\". We will use\n",
    "        # this to sift for only the lines with the the author number. \n",
    "        authornums_BSlist = soup.find_all('a', {\"title\":\"Link to this author\"})\n",
    "\n",
    "        for authornum in authornums_BSlist:\n",
    "            authornums.append(authornum[\"href\"].lstrip(\"#\"))\n",
    "\n",
    "        return authornums, authornames\n",
    "\n",
    "    def _get_bookswiki_info(self, soup):\n",
    "        '''\n",
    "        A helper function for __unite_authors_nums_books__. Extracts all the book titles and numbers from a \n",
    "        BeautifulSoup copy of a 'Browse by Author' page on the PG website. Also extracts author wikipedia \n",
    "        link information if it is available on the PG website. \n",
    "        inputs | soup:a BeautifulSoup object - containg a copy of the PG 'Browse by Author' page. \n",
    "        outputs | a tuple containing two lists. \n",
    "        1. The first list contains dictionaries. Each dictionary contains information about an author's books on PG. \n",
    "        this includes: book titles, corresponding PG books numbers, the author's role in each book, and the language \n",
    "        of each book. \n",
    "        2. The second list contains also contains dictionaries. Each dictionary contains information about an author's \n",
    "        wikipedia links on PG. An author's wiki dictionary may be empty, contain 1 link, or more than 1 link. \n",
    "        '''\n",
    "        books_info = []\n",
    "        wiki_info = []\n",
    "\n",
    "        # content under the 'ul' tags: books, links as one list organized by ul\n",
    "        authorsbooks_BSlist = soup.find_all('ul')\n",
    "        # for each ul, access the content: books, links; each book is a bs object\n",
    "\n",
    "        for author in authorsbooks_BSlist:\n",
    "            # there are two classes of attributes within each ul tag. the book information\n",
    "            # 1. title and book PG number is under the 'pgdbetext' class. \n",
    "            books_BSlist = author.find_all(class_='pgdbetext')\n",
    "\n",
    "            authorbooks_info = {}\n",
    "            for book in books_BSlist:\n",
    "                # the book numbers are stored in the href attribute. e.g. \"ebooks/19323\"\n",
    "                booknum = book.find('a')['href'].split(\"/\")[-1]\n",
    "                PG_booktitle = book.text\n",
    "\n",
    "                # storing the information regarding a single author's books in a dictionary\n",
    "                authorbooks_info[booknum]={\"PG_booktitle\":PG_booktitle}\n",
    "            \n",
    "            # appending the dictionary containing one author's books to a list\n",
    "            books_info.append(authorbooks_info)\n",
    "            \n",
    "            # 2. for the author is/are under the 'pgdbxlink' class. \n",
    "            wiki_BSlist = author.find_all(class_='pgdbxlink')\n",
    "\n",
    "            authorwiki_info = {}\n",
    "\n",
    "            for wiki in wiki_BSlist:\n",
    "                # 1. the wiki links are stored in the href attribute. \n",
    "                PG_wikilink = wiki.find('a')['href'] # get the whole link\n",
    "                PG_wikiname = PG_wikilink.split(\"/\")[-1] # get only the wikiname, to easily retrieve the page later \n",
    "                \n",
    "                # 2. because PG stores the link in URL-safe format (e.g. \"\\x\" is \"%\"), we will face issues with \n",
    "                # non-ASCII characters e.g. á whose URL-safe encoding cannot be passed into the wikipedia package. \n",
    "                # use urllib.requests.unquote to resolve this https://docs.python.org/2/library/urllib.html#utility-functions \n",
    "                PG_wikiname = urllib.request.unquote(PG_wikiname)\n",
    "                \n",
    "                # 3. get the language code for the wikipage\n",
    "                wikilang = re.findall(r'[^http://][a-z]+', PG_wikilink)[0] \n",
    "                # storing the information regarding a single author's wikipedia links in a dictionary\n",
    "                authorwiki_info[wikilang]={\"PG_wikilink\":PG_wikilink, \"PG_wikiname\":PG_wikiname}\n",
    "\n",
    "            # appending the dictionary containing one author's wikipedia links to a list\n",
    "            wiki_info.append(authorwiki_info)\n",
    "            \n",
    "        return books_info, wiki_info\n",
    "\n",
    "    \n",
    "    def _unite_authors_nums_books(self, authornums, authornames, books_info, wiki_info, min_books = 1, \n",
    "                                     languages = \"all\", roles = \"all\"):\n",
    "        '''\n",
    "        A helper function for get_library. \n",
    "        inputs | \n",
    "        1. authornums:list - list of author numbers obtained from a \"sort by author\" page on the PG website. \n",
    "        2. authornames:list - list of author names obtained from a \"sort by author\" page on the PG website. \n",
    "        3. books_info: list - a list containing dictionaries, each of which has information about one author's books \n",
    "        4. wiki_info: list - a list containing dictionaries, each of which has information about one author's wikipedia\n",
    "        page, as provided by the PG website. There may be none, one, or more wikilinks for an author. \n",
    "        5. min_books:int - the minimum number of books available for an author, which meets the languages \n",
    "        and roles parameters. default value is 1 (since an author listed on PG will have at least 1 book to his name).\n",
    "        6. languages:either a str \"all\", or a list containing the languages (in lowercase) to count towards the author's \n",
    "        min_books level. The list of languages available can be found here https://www.gutenberg.org/catalog/\n",
    "        default is \"all\". \n",
    "        7. roles: either a str \"all\", or a list containing the roles (in lowercase) that an author can have in a book. \n",
    "        These include: commentator, translator, contributor, photographer, illustrator, commentator, editor\n",
    "        default value is \"all\".\n",
    "        outputs | a dictionary containing PG numbers for authors who meet the min_books, languages and roles requirements, \n",
    "        as well as information each of these author's books. \n",
    "        '''\n",
    "        # we want to be sure that the authornums, authornames, books_info, and wiki_info are aligned before proceeding \n",
    "        # to merge them. \n",
    "        try:\n",
    "            assert len(authornums)==len(authornames) and len(authornums)==len(books_info) and len(authornums)==len(wiki_info)\n",
    "        except AssertionError as e:\n",
    "            e.args += (\"The length of authornums, authornames and books_info do not match.\",)\n",
    "            raise\n",
    "\n",
    "            \n",
    "        authorbooks_info = dict()\n",
    "        # if default parameters passed into the function. add all authors and their books to the corpus.  \n",
    "        if min_books == None and languages == \"all\" and roles == \"all\":\n",
    "            for i in range(len(authornums)):\n",
    "                authorbooks_info[authornums[i]]=\\\n",
    "                        {\"authorname\": authornames[i], \"books_info\": books_info[i], \"wiki_info\": wiki_info[i]}\n",
    "        else:\n",
    "            # place languages and roles input in sets, for use in .intersection below. \n",
    "            languages_set = set(languages)\n",
    "            roles_set = set(roles)\n",
    "            \n",
    "            for i in range(len(authornums)):\n",
    "                author_bookset = books_info[i]\n",
    "                __topop = []\n",
    "                for book in author_bookset: \n",
    "                    \n",
    "                    # using regex to find text in parentheses. book language e.g. (English) and author role \n",
    "                    # e.g. (as Author) are contained in parentheses. Some books, that are part of a series, \n",
    "                    # have (of N) in their titles too, where N is the number of books in that series. \n",
    "                    title_text_in_parentheses =\\\n",
    "                    re.findall(r'\\(([a-zA-Z]+\\s*[a-zA-Z]*[0-9]*)\\)', author_bookset[book][\"PG_booktitle\"])\n",
    "                    \n",
    "                    # lowercase the text in parentheses and putting into sets. \n",
    "                    __title_text_in_parentheses =\\\n",
    "                    set([i.lower() for i in title_text_in_parentheses])\n",
    "                    \n",
    "                    # if languages is set to \"all\" or if the intersection of __title_text_in_parentheses\n",
    "                    # and languages_set returns a non-empty set, pass to next check. Otherwise add this \n",
    "                    # book number to the list of books to pop from this author_bookset\n",
    "                    if languages == \"all\" or __title_text_in_parentheses.intersection(languages_set): pass\n",
    "                    else: \n",
    "                        __topop.append(book) \n",
    "                        continue \n",
    "                    # similar logic as above, this time for author role.\n",
    "                    if roles == \"all\" or __title_text_in_parentheses.intersection(roles_set): pass\n",
    "                    else: \n",
    "                        __topop.append(book) \n",
    "                        continue    \n",
    "                # pop the books that don't meet the language and role specifications. \n",
    "                for pop in __topop:\n",
    "                    books_info[i].pop(pop)\n",
    "                    \n",
    "                #check if number of books meeting the language and role requirements meet the min_book requirement \n",
    "                if len(books_info[i]) >= min_books:\n",
    "                    authorbooks_info[authornums[i]]=\\\n",
    "                            {\"authorname\": authornames[i], \"books_info\": books_info[i], \"wiki_info\": wiki_info[i]}\n",
    "                    \n",
    "        return authorbooks_info \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"There are {} authors in this corpus\".format(len(self.authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OSAvl1AGtpJU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1WRLJZBtpZK"
   },
   "source": [
    "### 2. A class to store subcorpora obtained from the Project Gutenberg website for each Author. \n",
    "\n",
    "1. The subcorpus is build with functions within the class that pre-processes each .txt file for filtered author books on the Project Gutenberg website. \n",
    "2. It also obtains the abstracts and literary movement tags for each author from Wikipedia and DBPedia respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m96uH1j8mzXl"
   },
   "outputs": [],
   "source": [
    "class Author:\n",
    "    \n",
    "    def __init__(self, authorbooks_info):\n",
    "        '''\n",
    "        initiates the Author object with the author's name. \n",
    "        input | str: author's name\n",
    "        '''\n",
    "        self.name = authorbooks_info[\"authorname\"]\n",
    "        self.wiki_info = authorbooks_info[\"wiki_info\"]\n",
    "        \n",
    "        # a dictionary with the keys as the book number and the value as the title of the book. \n",
    "        self.books = {}\n",
    "        \n",
    "        # a dictionary with the keys as the book number and the value as a list \n",
    "        # (containing strings that have been pre-processed by the segment_sentence method)\n",
    "        self.processed_subcorpus = {}        \n",
    "        \n",
    "        self.authorabstract = \"\" \n",
    "        self.literarymovements = []\n",
    "        \n",
    "    def populate_attributes(self):\n",
    "        '''\n",
    "        A convenience function to call _build_subcorpus, _get_authorabstract and  _get_literarymovement, \n",
    "        which will respectively populate the processed_subcorpus, authorabstract and literarymovements\n",
    "        attributes for this Author instance.  \n",
    "        input | nil\n",
    "        output | nil \n",
    "        '''\n",
    "        # _build_subcorpus\n",
    "        #_get_authorabstract\n",
    "        #_get_literarymovement\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def _build_subcorpus(self,):\n",
    "        '''\n",
    "        A helper function for .populate_attributes\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def _cleansegment_book(self, booknum, urlpath = \"https://www.gutenberg.org/files/{}/{}.txt\"):\n",
    "        '''\n",
    "        takes a booknum, navigates to the PG page with the .txt file for this book. uses urlopen to retrieve \n",
    "        the contents of this file. \n",
    "        once open, only retrieves lines before and \"START\" line include, do the same process with \"END\"\n",
    "        removes lines \n",
    "        '''    \n",
    "\n",
    "        target_url = urlpath.format(booknum,booknum)\n",
    "        \n",
    "        book_content = []\n",
    "        # open target_url with the urllib.request.urlopen() method,\n",
    "        # for each line in response, decodes with \"latin-1\" \n",
    "        # which is the expected encoding format PG uses for plain .txt book files. \n",
    "        \n",
    "        with urllib.request.urlopen(target_url) as response: \n",
    "            for line in response: \n",
    "                # urlopen reads as bytes, to ease processing, we decode to string.\n",
    "                # most PG .txt files are encoded in latin-1 format. \n",
    "                book_content.append(line.decode(\"latin-1\"))\n",
    "        \n",
    "        \n",
    "        start_index=0 #indice from the first part of the text\n",
    "        stop_index=0  #indice from the second part of the text  \n",
    "        \n",
    "        # Each PG book .txt file is bookended with metadata marked with \"* START\" and \"* END\" or minor variations. \n",
    "        # * START-tagged metadata appear in the first half of the .txt file, and vice-versa for * END.\n",
    "        # we split the file in two halves and run searches for * START and * END from front and back \n",
    "        # (for savings in search time)\n",
    "        for index_num in range(round(len(book_content)/2)):\n",
    "            # searching for the last * START in the first half of the file \n",
    "            if re.match(r'\\*+\\s*START ', book_content[index_num]):\n",
    "                start_index = index_num+1 \n",
    "                \n",
    "            # searching for the last * END from the back, in the last half of the file \n",
    "            if re.match(r'\\*+\\s*END ', book_content[-index_num]):\n",
    "                stop_index = -index_num-1 \n",
    "\n",
    "        # slicing the section of the text between the start_index and stop_index. \n",
    "        clean_book_content = book_content[start_index:stop_index]\n",
    "        \n",
    "        # join all the text without \"\\r\\n\" i.e. return carriage and newline \n",
    "        __clean_book_content = \" \".join([l.strip(\"\\r\\n\") for l in clean_book_content if l != \"\\r\\n\"])\n",
    "        \n",
    "        # splits the text into sentences using the TextBlob package, it in turns calls on the nltk package \n",
    "        textblob_sentsegs = [i.string for i in TextBlob(__clean_book_content).sentences]\n",
    "        \n",
    "        # strip to first and last 5 lines (as a buffer to avoid collecting overflow PG metadata)\n",
    "        textblob_sentsegs = textblob_sentsegs[5:-5]\n",
    "\n",
    "        return textblob_sentsegs\n",
    "\n",
    "    \n",
    "\n",
    "    def write_to_file(save_datapath):\n",
    "        '''\n",
    "        takes the list of sentences from a single book of an author (this is the value of the dictionary that is nested under an author's number in \n",
    "        self.processed_subcorpus) and writes it to a text file. \n",
    "\n",
    "        '''\n",
    "\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def _get_authorabstract(self, languages, author_wikiname):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. takes the author's wikiname passes it through the wikipedia package to\n",
    "        retrieve the abstract of the author's wiki page for each of the languages passes. \n",
    "        input | \n",
    "        1. languages: list - a list of languages (use language prefixes listed here \n",
    "        https://meta.wikimedia.org/wiki/List_of_Wikipedias)\n",
    "        2. author_wikiname: str - name of the author, obtained from the wikipedia links posted on the PG website\n",
    "        output | a list, containing the abstracts for an author in the order of the languages passed into the argument. \n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            # assert \n",
    "            assert len(self.wiki_info) > 0\n",
    "            \n",
    "            __abstracts = []\n",
    "            for lang in languages: \n",
    "                wikipedia.set_lang(lang)\n",
    "\n",
    "            try: \n",
    "                # without disambiguation \n",
    "                wikipage = wikipedia.page(title=self.name)\n",
    "                self.authorabstract = wikipage.summary()\n",
    "\n",
    "\n",
    "            except: \n",
    "                # use wikipedia disambiguation to \n",
    "                possible_pages = wikipedia.exceptions.DisambiguationError(title=self.name)\n",
    "\n",
    "                # check that the books collected for the author,\n",
    "        except AssertionError:\n",
    "            self.authorabstract = \"Project Gutenberg does not list any wikipedia pages for the author\". \n",
    "    \n",
    "    \n",
    "    def _get_literarymovement(self):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. takes an author's name, makes a DBpedia query \n",
    "        with the name using the SPARQLWrapper package, \n",
    "        returns the literary movements that the author is associated with. \n",
    "\n",
    "        input | \n",
    "        output | \n",
    "        '''\n",
    "\n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        sparql.setQuery(\"\"\"\n",
    "        SELECT ?writer ?name ?genre\n",
    "        WHERE {\n",
    "        ?writer rdf:type dbo:Writer ;\n",
    "        foaf:name ?name .\n",
    "        ?writer dbo:genre ?genre .\n",
    "        }\n",
    "        \"\"\")\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            genre=result[\"genre\"][\"value\"]\n",
    "\n",
    "        return \"pending\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UOe2Z-guIIG"
   },
   "source": [
    "### 3. pymongo implementation to store the corpus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAZsdVRoz0xA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# instantiate a MongoClient object. using the URI for the Mongo server. If it is locally hosted, \n",
    "# it is by default on the 27017 port. If using cloud, use the provided URI\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "corpusdb = client[\"corpus\"]\n",
    "authorcollection = corpusdb[\"author\"]\n",
    "\n",
    "# insert the documents into the collection \n",
    "for i in PGcorpus.authors: \n",
    "    authorcollection.insert_one(PGcorpus.authors[i])\n",
    "\n",
    "# some test code to check insertions \n",
    "print(authorcollection.find_one(), authorcollection.\n",
    "authorcollection.estimated_document_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9jRyG8qz1R1"
   },
   "source": [
    "### 4. Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hzx6BXsmMQMk"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  # instantiate a GutenbergCorpusBuilder \n",
    "  PGcorpus = GutenbergCorpusBuilder(corpusname=\"PG-eng-author-min2\")\n",
    "  # start collecting and filtering author and book details from the Project Gutenberg site\n",
    "  PGcorpus.get_library(min_books = 2, languages = [\"english\"], roles = [\"as author\"])\n",
    "  # read text files, select sentences, pre-process sentences, store to subcorpora\n",
    "#   PGcorpus.populate_corpus()\n",
    "  # write subcorpora to file \n",
    "#   PGCorpus.write_to_file()\n",
    "  \n",
    "  # import to mongoDB. export mongoDB database. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKrllplDkyqb"
   },
   "source": [
    "### 5. Test code - informal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJCo6Hhxkxn4"
   },
   "outputs": [],
   "source": [
    "# check that corpus contains only english books. it should return nothing. \n",
    "for i in PGcorpus.authors.keys(): \n",
    "    for i2 in PGcorpus.authors[i][\"books_info\"]:\n",
    "        if \"English\" not in PGcorpus.authors[i][\"books_info\"][i2][\"PG_booktitle\"]:\n",
    "            print(PGcorpus.authors[i][\"books_info\"][i2][\"PG_booktitle\"])\n",
    "\n",
    "# check that corpus contains only books where author role is as Author. it should return nothing.\n",
    "for i in PGcorpus.authors.keys(): \n",
    "    for i2 in PGcorpus.authors[i][\"books_info\"]:\n",
    "        if \"Author\" not in PGcorpus.authors[i][\"books_info\"][i2][\"PG_booktitle\"]:\n",
    "            print(PGcorpus.authors[i][\"books_info\"][i2][\"PG_booktitle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataScienceProject-Session1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
