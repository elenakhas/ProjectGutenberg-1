{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SXRp4LcnC4I"
   },
   "source": [
    "# Extracting and storing RDF information and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7wQupQrnC4J"
   },
   "outputs": [],
   "source": [
    "# # internal note: uncomment these rows to install packages... colaboratory says they \n",
    "# # are not present. we only get max 12 hour connection to the cloud each time. \n",
    "# # whenever we reconnect, we get a new instance of the virtual machine, which \n",
    "# # won't have these non-standard packages. \n",
    "# ! pip install wikipedia\n",
    "# ! pip install wptools\n",
    "# ! pip install SPARQLWrapper\n",
    "# ! pip install textblob\n",
    "# ! pip install nltk\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wptools, wikipedia\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from rdflib import Graph\n",
    "import csv, datetime, time, random, collections, string, re, urllib  \n",
    "from textblob import TextBlob\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIVWTPMRexID"
   },
   "source": [
    "### General overview\n",
    "\n",
    "1.  2 sets of classes - 1 to handle PG author-title mining, filtering and selection. The other to pre-process author\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObFltmPBnC4M"
   },
   "outputs": [],
   "source": [
    "# TO DO LIST: \n",
    "\n",
    "# 2. write a function to randomly select min_books number of books from each author's pool of books. Effectively, this only affects \n",
    "# authors with more than the min_books level specified. \n",
    "\n",
    "# 4. complete populate_corpus in GutenbergCorpusBuilder\n",
    "\n",
    "# 5. complete the database question - MongoDB (2 options - load a full GutenbergCorpusBuilder output, or author by author)\n",
    "\n",
    "# 6. review and add to documentation and comments for clarity and completeness \n",
    "\n",
    "# 7. draw a diagram of the database model \n",
    "\n",
    "\n",
    "\n",
    "## DONE \n",
    "# 1. Complete comments and docstring for _cleansegment_book \n",
    "\n",
    "# 3. complete _get_literarymovement, _get_authorabstract, _build_subcorpus, populate_attributes and write_to_file of the Author class (note that for _get_authorabstract\n",
    "# the GutenbergCorpusBuilder's _get_bookswiki_info method has been expanded to grab wikipedia pages on the PG website, where available. This can be passed into \n",
    "# the wikipedia package to easily get the pages. However, a small problem exists, some of PG's wikilinks uses non-unicode symbols \n",
    "# e.g. href=\"http://en.wikipedia.org/wiki/Fern%C3%A1n_Caballero\" passing \"Fern%C3%A1n_Caballero\" takes us to the wrong wikipedia page (solved - with urllib.requests.unquote)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUOWBinlnC4Z"
   },
   "source": [
    "### 1. A class to store a corpus obtained from the Project Gutenberg website. \n",
    "\n",
    "1. The corpus is build with functions within the class that filter the authors and books on the Project Gutenberg website. \n",
    "2. It also calls on the Author class, to process and generate information about sentences from an author's books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfplnCWJnC4Z"
   },
   "outputs": [],
   "source": [
    "class GutenbergCorpusBuilder: \n",
    "   \n",
    "    '''\n",
    "        initiates a GutenbergCorpusBuilder object which stores information about selected authors available on the\n",
    "        Project Gutenberg(PG) website.\n",
    "        Authors are stored based on their unique PG numerical code. For each author,\n",
    "        selected books and their respective PG URL are stored.\n",
    "        Inputs: corpusname - string representing name of the corpus being created.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, corpusname):\n",
    "       \n",
    "        self.corpusname = corpusname\n",
    "        self.corpusversion = \"v\"+ str(datetime.datetime.now().year) + str(datetime.datetime.now().month) +\\\n",
    "        str(datetime.datetime.now().day)\n",
    "        \n",
    "        # a dictionary containing dictionaries.\n",
    "        # The top level keys - unique numbers for authors on the PG website,\n",
    "        # the values -  dictionaries containing author information: keys - 'authorname', 'books_info'; 'wiki_info'; \n",
    "        # values - strings or embedded dictionaries:\n",
    "        # authorname: string with extracted name;\n",
    "        # books_info: dictionary: key - book ID, value - book title;\n",
    "        # wiki_info: dictionary: key - language; value - wiki link extracted from PG\n",
    "        #\n",
    "        # {author ID: {authorname:'name', books_info:{bookID: book title}, wiki_info: {language: wiki link}}}\n",
    "        self.authors = {}\n",
    "       \n",
    "        # a dictionary containing sets of sentences selected from each author's filtered books;\n",
    "        # the top level keys are the unique numbers for authors, the values are sets containing\n",
    "        # sentences from an author's book (as strings).\n",
    "        # {author ID: Author()}\n",
    "        self.corpus = {}\n",
    "       \n",
    "        \n",
    "    def populate_corpus(self, sent_num=50):\n",
    "        '''\n",
    "        for each author in self.authors, generates an Author class instance, populates all \n",
    "        attributes of the Author class, adds to self.corpus.\n",
    "        \n",
    "        '''\n",
    "        if len(self.authors) > 0:\n",
    "            for authornum in self.authors: # authornum is a key (an author's unique number)\n",
    "                if authornum not in self.corpus.keys():\n",
    "                    # instantiate an Author()\n",
    "                    __author = Author(self.authors[authornum][\"authorname\"], authornum, self.authors[authornum][\"wiki_info\"],self.authors[authornum][\"books_info\"])\n",
    "                    # run the populate_attributes() to extract and process the information for the author\n",
    "                    __author.populate_attributes(self.authors[authornum][\"wiki_info\"], self.authors[authornum][\"books_info\"], sent_num)\n",
    "                    # store to Author() to corpus \n",
    "                    self.corpus[authornum] = __author\n",
    "\n",
    "                    # append wiki abstract info and literary movement to author's dictionary in self.authors\n",
    "                    # intention is to easily transmit each author's basic information into mongodb\n",
    "                    self.authors[authornum][\"authorabstracts\"] = __author.authorabstracts\n",
    "                    self.authors[authornum][\"literarymovements\"] = __author.literarymovements\n",
    "                  \n",
    "        else: \n",
    "            print(\"The authors attribute is empty, please run get_library first or check the parameters passed into get_library.\")\n",
    "        pass \n",
    "        \n",
    "    \n",
    "    def get_library(self, min_books, languages = \"all\", roles = \"all\"):\n",
    "        '''\n",
    "        \n",
    "        Goes through the PG website's 'sort by author' pages. Extracts author and corresponding book \n",
    "        information that meet a number of selection criterion (see inputs). \n",
    "        Inputs | \n",
    "        1. min_books: int - the minimum number of books available for an author, which meets the languages \n",
    "        and roles parameters. Default value is 1. \n",
    "        5. languages: either a str \"all\", or a list containing the languages (in lowercase) to count towards \n",
    "        the author's min_books level. The list of languages available can be found here \n",
    "        https://www.gutenberg.org/catalog/. Default is \"all\". \n",
    "        6. roles: either a str \"all\", or a list containing the roles that an author can have in a book. \n",
    "        These include: Commentator, Translator, Contributor, Photographer, Illustrator, Editor.\n",
    "        Default value is \"all\".\n",
    "        \n",
    "        Outputs | saves the results to self.authors\n",
    "        \n",
    "        '''\n",
    "        charlist = []\n",
    "        charlist[:0] = \"a\" #[letter for letter in string.ascii_lowercase] + [\"other\"]\n",
    "\n",
    "        library = dict()\n",
    "        for char in charlist:\n",
    "            # Team comment: we select the authors and books via the \"Browse by Author\" lists instead of the \n",
    "            # \"Browse by Books\" list. Although the latter has a more predictable page structure \n",
    "            # (i.e. 1 book name, followed by 1 author name, recursively), the former includes \n",
    "            # information about the Author's role in the book. We believe that this could have\n",
    "            # a meaningful impact on the predictive capabilities for models on different tasks, \n",
    "            # especially at larger scale.\n",
    "            \n",
    "            link = 'https://www.gutenberg.org/browse/authors/'+ char\n",
    "            page = requests.get(link)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            one_letter = self._unite_authors_nums_books(self._get_authors_numsnames(soup)[0],\\\n",
    "                                                            self._get_authors_numsnames(soup)[1],\\\n",
    "                                                            self._get_bookswiki_info(soup)[0],\\\n",
    "                                                            self._get_bookswiki_info(soup)[1],\\\n",
    "                                                            min_books, languages, roles)\n",
    "            \n",
    "            library.update(one_letter)\n",
    "            print(\"{} authors from the '{}' alphabetical category have been added. \".format(len(one_letter),char))\n",
    "            \n",
    "            # Put the function to sleep for a randomised number of seconds (non-integer number between 2 and 8)\n",
    "            # to mimic human surfing patterns.\n",
    "            time.sleep(random.uniform(2,8))\n",
    "            \n",
    "        self.authors = library \n",
    "    \n",
    "    def _get_authors_numsnames(self, soup):\n",
    "        '''\n",
    "        A helper function for __unite_authors_nums_books__. Extracts all author names from a BeautifulSoup copy  \n",
    "        of a 'Browse by Author' page on the PG website. \n",
    "        Inputs | soup:a BeautifulSoup object - containg a copy of the PG 'Browse by Author' page. \n",
    "        Outputs | a tuple containing two lists: The first contains author's numbers on the page, the second contains\n",
    "        corresponding author's names on the page. \n",
    "        '''\n",
    "        authornames = []\n",
    "        # the author names are stored within the \"name\" attribute under each \"a\" class\n",
    "        # use regex wildcard so that find_all will catch and return all \"a names\" with values\n",
    "        authorname_BSlist = soup.find_all('a', {\"name\":re.compile(\"\\w*\")})\n",
    "\n",
    "        for authorname in authorname_BSlist:\n",
    "            # \\- and \\? to escape special characters. .rstrip to remove trailing whitespaces. \n",
    "            authornames.append(re.sub(r'[0-9,\\-\\?]*', '', authorname.text).rstrip())\n",
    "\n",
    "        authornums = []\n",
    "        # the author numbers are stored within the \"href\" attribute. Every line for a book \n",
    "        # on the page has a \"title\" attribute with the value \"Link to this author\". We will use\n",
    "        # this to shift to only the lines with the author's number. \n",
    "        authornums_BSlist = soup.find_all('a', {\"title\":\"Link to this author\"})\n",
    "\n",
    "        for authornum in authornums_BSlist:\n",
    "            authornums.append(authornum[\"href\"].lstrip(\"#\"))\n",
    "\n",
    "        return authornums, authornames\n",
    "\n",
    "    def _get_bookswiki_info(self, soup):\n",
    "        '''\n",
    "        A helper function for __unite_authors_nums_books__. Extracts all the book titles and numbers from a \n",
    "        BeautifulSoup copy of a 'Browse by Author' page on the PG website. Also extracts author wikipedia \n",
    "        link information if it is available on the PG website. \n",
    "        Inputs | soup:a BeautifulSoup object - containg a copy of the PG 'Browse by Author' page. \n",
    "        Outputs | a tuple containing two lists. \n",
    "          1. The first list contains dictionaries. Each dictionary contains information about an author's books on PG. \n",
    "          this includes: book titles, corresponding PG books numbers, the author's role in each book, and the language \n",
    "          of each book. \n",
    "          2. The second list contains dictionaries. Each dictionary contains information about an author's \n",
    "          wikipedia links on PG. An author's wiki dictionary may be empty, contain 1 link, or more than 1 link. \n",
    "        '''\n",
    "        books_info = []\n",
    "        wiki_info = []\n",
    "\n",
    "        # content under the 'ul' tags: books, links as one list organized by ul\n",
    "        authorsbooks_BSlist = soup.find_all('ul')\n",
    "        # for each ul, access the content: books, links; each book is a bs object\n",
    "\n",
    "        for author in authorsbooks_BSlist:\n",
    "            # there are two classes of attributes within each ul tag. The book information\n",
    "            # 1. title and book PG number is under the 'pgdbetext' class. \n",
    "            books_BSlist = author.find_all(class_='pgdbetext')\n",
    "\n",
    "            authorbooks_info = {}\n",
    "            for book in books_BSlist:\n",
    "                # the book numbers are stored in the href attribute. e.g. \"ebooks/19323\"\n",
    "                booknum = book.find('a')['href'].split(\"/\")[-1]\n",
    "                PG_booktitle = book.text\n",
    "\n",
    "                # storing the information regarding a single author's books in a dictionary\n",
    "                authorbooks_info[booknum]=PG_booktitle\n",
    "            \n",
    "            # appending the dictionary containing one author's books to a list\n",
    "            books_info.append(authorbooks_info)\n",
    "            \n",
    "            # 2. for the author is/are under the 'pgdbxlink' class. \n",
    "            wiki_BSlist = author.find_all(class_='pgdbxlink')\n",
    "\n",
    "            authorwiki_info = {}\n",
    "\n",
    "            for wiki in wiki_BSlist:\n",
    "                # 1. the wiki links are stored in the href attribute. \n",
    "                PG_wikilink = wiki.find('a')['href'] # get the whole link\n",
    "                \n",
    "                # 2. because PG stores the link in URL-safe format (e.g. \"\\x\" is \"%\"), we will face issues with \n",
    "                # non-ASCII characters e.g. á whose URL-safe encoding cannot be passed into the wikipedia package. \n",
    "                # use urllib.requests.unquote to resolve this https://docs.python.org/2/library/urllib.html#utility-functions \n",
    "                PG_wikilink = urllib.request.unquote(PG_wikilink)\n",
    "                \n",
    "                # 3. get the language code for the wikipage\n",
    "                wikilang = re.findall(r'[^http://][a-z]+', PG_wikilink)[0] \n",
    "                # storing the information regarding a single author's wikipedia links in a dictionary\n",
    "                authorwiki_info[wikilang] = PG_wikilink\n",
    "\n",
    "            # appending the dictionary containing one author's wikipedia links to a list\n",
    "            wiki_info.append(authorwiki_info)\n",
    "            \n",
    "        return books_info, wiki_info\n",
    "\n",
    "    \n",
    "    def _unite_authors_nums_books(self, authornums, authornames, books_info, wiki_info, min_books, \n",
    "                                     languages = \"all\", roles = \"all\"):\n",
    "        '''\n",
    "        A helper function for get_library. \n",
    "        inputs | \n",
    "        1. authornums:list - list of author numbers obtained from a \"sort by author\" page on the PG website. \n",
    "        2. authornames:list - list of author names obtained from a \"sort by author\" page on the PG website. \n",
    "        3. books_info: list - a list containing dictionaries, each of which has information about one author's books \n",
    "        4. wiki_info: list - a list containing dictionaries, each of which has information about one author's wikipedia\n",
    "        page, as provided by the PG website. There may be none, one, or more wikilinks for an author. \n",
    "        5. min_books:int - the minimum number of books available for an author, which meets the languages \n",
    "        and roles parameters. default value is 1 (since an author listed on PG will have at least 1 book to his name).\n",
    "        6. languages:either a str \"all\", or a list containing the languages (in lowercase) to count towards the author's \n",
    "        min_books level. The list of languages available can be found here https://www.gutenberg.org/catalog/\n",
    "        default is \"all\". \n",
    "        7. roles: either a str \"all\", or a list containing the roles (in lowercase) that an author can have in a book. \n",
    "        These include: commentator, translator, contributor, photographer, illustrator, commentator, editor\n",
    "        default value is \"all\".\n",
    "        outputs | a dictionary containing PG numbers for authors who meet the min_books, languages and roles requirements, \n",
    "        as well as information each of these author's books. \n",
    "        '''\n",
    "        # we want to be sure that the authornums, authornames, books_info, and wiki_info are aligned before proceeding \n",
    "        # to merge them. \n",
    "        try:\n",
    "            assert len(authornums)==len(authornames) and len(authornums)==len(books_info) and len(authornums)==len(wiki_info)\n",
    "        except AssertionError as e:\n",
    "            e.args += (\"The length of authornums, authornames and books_info do not match.\",)\n",
    "            raise\n",
    "\n",
    "            \n",
    "        authorbooks_info = dict()\n",
    "        # if default parameters passed into the function, add all authors and their books to the corpus.  \n",
    "        if min_books == None and languages == \"all\" and roles == \"all\":\n",
    "            for i in range(len(authornums)):\n",
    "                authorbooks_info[authornums[i]]=\\\n",
    "                        {\"authorname\": authornames[i], \"books_info\": books_info[i], \"wiki_info\": wiki_info[i]}\n",
    "        else:\n",
    "            # place languages and roles input in sets, for use in .intersection below. \n",
    "            languages_set = set(languages)\n",
    "            roles_set = set(roles)\n",
    "            \n",
    "            for i in range(len(authornums)):\n",
    "                author_bookset = books_info[i]\n",
    "                __topop = []\n",
    "                for book in author_bookset: \n",
    "                    \n",
    "                    # using regex to find text in parentheses. Book language e.g. (English) and author role \n",
    "                    # e.g. (as Author) are contained in parentheses. Some books which are part of a series, \n",
    "                    # have (of N) in their titles too, where N is the number of books in that series. \n",
    "                    title_text_in_parentheses =\\\n",
    "                    re.findall(r'\\(([a-zA-Z]+\\s*[a-zA-Z]*[0-9]*)\\)', author_bookset[book])\n",
    "                    \n",
    "                    # lowercase the text in parentheses and put it into sets. \n",
    "                    __title_text_in_parentheses =\\\n",
    "                    set([i.lower() for i in title_text_in_parentheses])\n",
    "                    \n",
    "                    # if languages is set to \"all\" or if the intersection of __title_text_in_parentheses\n",
    "                    # and languages_set returns a non-empty set, pass to the next check. Otherwise add this \n",
    "                    # book number to the list of books to pop from this author_bookset\n",
    "                    if languages == \"all\" or __title_text_in_parentheses.intersection(languages_set): pass\n",
    "                    else:\n",
    "                        __topop.append(book) \n",
    "                        continue \n",
    "                    # do the same for author's role as for language above\n",
    "                    if roles == \"all\" or __title_text_in_parentheses.intersection(roles_set): pass\n",
    "                    else:\n",
    "                        __topop.append(book) \n",
    "                        continue    \n",
    "                # pop the books that don't meet the language and role specifications. \n",
    "                for pop in __topop:\n",
    "                    books_info[i].pop(pop)\n",
    "                    \n",
    "                #check if number of books meeting the language and role requirements meet the min_book requirement \n",
    "                if len(books_info[i]) >= min_books:\n",
    "                    authorbooks_info[authornums[i]]=\\\n",
    "                            {\"authorname\": authornames[i], \"books_info\": books_info[i], \"wiki_info\": wiki_info[i]}\n",
    "                    \n",
    "        return authorbooks_info \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"There are {} authors in this corpus\".format(len(self.authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OSAvl1AGtpJU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1WRLJZBtpZK"
   },
   "source": [
    "### 2. A class to store subcorpora obtained from the Project Gutenberg website for each Author. \n",
    "\n",
    "1. The subcorpus is build with functions within the class that pre-processes each .txt file for filtered author books on the Project Gutenberg website. \n",
    "2. It also obtains the abstracts and literary movement tags for each author from Wikipedia and DBPedia respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m96uH1j8mzXl"
   },
   "outputs": [],
   "source": [
    "class Author:\n",
    "    \n",
    "    def __init__(self, authorname, authornum, authorwiki_info, authorbooks_info):\n",
    "        '''\n",
    "        initiates the Author object with the author's name. \n",
    "        Input | str: author's name\n",
    "        '''\n",
    "        self.name = authorname\n",
    "        self.number = authornum\n",
    "        \n",
    "        # a dictionary with the book numbers as keys and lists as values. Lists  \n",
    "        # contain strings that have been pre-processed by the segment_sentence method.\n",
    "        self.processed_subcorpus = {}        \n",
    "        \n",
    "        self.authorabstracts = {} \n",
    "        self.literarymovements = []\n",
    "        \n",
    "    def populate_attributes(self, author_wiki_info, authorbooks_info, sent_num):\n",
    "        '''\n",
    "        A convenience function to call _build_subcorpus, _get_authorabstract and  _get_literarymovement, \n",
    "        which will respectively populate the processed_subcorpus, authorabstract and literarymovements\n",
    "        attributes for this Author instance.  \n",
    "        input | \n",
    "        output |  \n",
    "        '''\n",
    "        self._build_subcorpus(authorbooks_info, sent_num)\n",
    "        self._get_authorabstract(author_wiki_info, sent_num)\n",
    "        self._get_literarymovement()\n",
    "        \n",
    "    \n",
    "    def _build_subcorpus(self, authorbooks_info, sent_num):\n",
    "        \n",
    "        '''\n",
    "        A helper function for .populate_attributes\n",
    "        '''\n",
    "        # results of _cleansegment_book\n",
    "        # run _select_sentences on results\n",
    "        # store to processed_subcorpus under booknumber \n",
    "        # write result to a csv file\n",
    "        for booknum in authorbooks_info: \n",
    "            full_sentence = self._cleansegment_book(booknum, urlpath = \"https://www.gutenberg.org/files/{}/{}.txt\")\n",
    "\n",
    "            s = 0\n",
    "            sentence_extracted = []\n",
    "            for sent in full_sentence:\n",
    "                while s <= sent_num:\n",
    "                    sentence_extracted.append(sent)\n",
    "\n",
    "                if(len(sentence_extracted) < sent_num):\n",
    "                    number_sentence = sent_num - len(sentence)\n",
    "\n",
    "                path = path + '/' \n",
    "                '''\n",
    "                do not need to make a folder \n",
    "                like num_author+book\n",
    "                '''\n",
    "                #try:\n",
    "                    #os.mkdir(path)\n",
    "\n",
    "                #except exception_type as exception_returned:\n",
    "                    #print('This is the error: ', exception_returned)\n",
    "\n",
    "                #else:\n",
    "                    #print('The folder is created')\n",
    "\n",
    "                '''\n",
    "                write the selected sentence, list of string, into a csv file for the next step\n",
    "                keep the number as the author name and the book number\n",
    "                '''\n",
    "\n",
    "            with open(self.authornum+\"_\"+booknum+'.csv', 'w') as csv_file:\n",
    "                writed_file = csv.writer(csv_file, dialect = 'excel')\n",
    "                writed_file.writerow(sentence_extracted)\n",
    "\n",
    "        return \"done\"\n",
    "        \n",
    "     \n",
    "      \n",
    "    def _cleansegment_book(self, booknum, urlpath = \"https://www.gutenberg.org/files/{}/{}.txt\"):\n",
    "        '''\n",
    "        takes a booknum, navigates to the PG page with the .txt file for this book. uses urlopen to retrieve \n",
    "        the contents of this file. \n",
    "        once open, only retrieves lines before and \"START\" line including, does the same process with \"END\"\n",
    "        removes lines\n",
    "        \n",
    "        '''    \n",
    "\n",
    "        target_url = urlpath.format(booknum,booknum)\n",
    "        \n",
    "        book_content = []\n",
    "        # open target_url with the urllib.request.urlopen() method,\n",
    "        # for each line in response, decodes with \"latin-1\", which is the expected encoding format PG uses for plain .txt book files. \n",
    "        \n",
    "        #try except add\n",
    "        \n",
    "        with urllib.request.urlopen(target_url) as response: \n",
    "            for line in response: \n",
    "                # urlopen reads as bytes, to ease processing, we decode to string.\n",
    "                # most PG .txt files are encoded in latin-1 format. \n",
    "                book_content.append(line.decode(\"latin-1\"))\n",
    "        \n",
    "        \n",
    "        start_index=0 #index from the first part of the text\n",
    "        stop_index=0  #index from the second part of the text  \n",
    "        \n",
    "        # Each PG book .txt file is ended with metadata marked with \"* START\" and \"* END\" or minor variations. \n",
    "        # * START-tagged metadata appear in the first half of the .txt file, and vice-versa for * END.\n",
    "        # we split the file in two halves and run searches for * START and * END from the front and the back \n",
    "        # (for savings in search time)\n",
    "        for index_num in range(round(len(book_content)/2)):\n",
    "            # searching for the last * START in the first half of the file \n",
    "            if re.match(r'\\*+\\s*START ', book_content[index_num]):\n",
    "                start_index = index_num+1 \n",
    "                \n",
    "            # searching for the last * END from the back, in the last half of the file \n",
    "            if re.match(r'\\*+\\s*END ', book_content[-index_num]):\n",
    "                stop_index = -index_num-1 \n",
    "\n",
    "        # slicing the section of the text between the start_index and stop_index. \n",
    "        clean_book_content = book_content[start_index:stop_index]\n",
    "        \n",
    "        # join all the text without \"\\r\\n\" i.e. return carriage and newline \n",
    "        __clean_book_content = \" \".join([l.strip(\"\\r\\n\") for l in clean_book_content if l != \"\\r\\n\"])\n",
    "        \n",
    "        # splits the text into sentences using the TextBlob package, it in turn calls on the nltk package \n",
    "        #textblob_sentsegs = [i.string for i in TextBlob(__clean_book_content).sentences]\n",
    "        \n",
    "        # strip to first and last 5 lines (as a buffer to avoid collecting overflow PG metadata)\n",
    "        #textblob_sentsegs = textblob_sentsegs[5:-5]\n",
    "        sentence = sent_tokenize(__clean_book_content)\n",
    "\n",
    "\n",
    "        return sentence\n",
    "\n",
    "\n",
    "    \n",
    "    def _get_authorabstract(self, author_wiki_info):\n",
    "      \n",
    "        '''\n",
    "        A helper function for .populate_attributes. \n",
    "        input | languages: list - a list of languages (use language prefixes here https://meta.wikimedia.org/wiki/List_of_Wikipedias)\n",
    "        output | a list, containing the abstracts for an author in the order of the languages passed into the argument. \n",
    "\n",
    "        '''\n",
    "        __abstracts = []\n",
    "\n",
    "\n",
    "        for lang in author_wiki_info: \n",
    "        # set the language \n",
    "            wikipedia.set_lang(lang)\n",
    "            wiki_name = author_wiki_info[lang].split(\"/\")[-1]\n",
    "\n",
    "        try: # without disambiguation \n",
    "\n",
    "            wikipage = wikipedia.page(title=wiki_name)\n",
    "            __abstracts[lang] = wikipage.summary()\n",
    "\n",
    "\n",
    "        except PageError: \n",
    "            print(\"There is a PageError resulting with this wikiname: {}\".format(wiki_name) )\n",
    "            pass \n",
    "        except DisambiguationError: \n",
    "            print(\"There is a DisambiguationError resulting with this wikiname: {}\".format(wiki_name) )\n",
    "            pass \n",
    "\n",
    "        self.authorabstracts = __abstracts    \n",
    "    \n",
    "      \n",
    "    def _get_literarymovement(self):\n",
    "      \n",
    "        '''\n",
    "        A helper function for .populate_attributes. takes an author's name, makes a DBpedia query \n",
    "        with the name using the SPARQLWrapper package, \n",
    "        returns the literary movements that the author is associated with. \n",
    "      \n",
    "        input | \n",
    "        output |\n",
    "        '''\n",
    "      \n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        authorname = \"\\'\" + authorname + \"\\'\" + '@en'\n",
    "        qr = '''SELECT ?text\n",
    "            WHERE {\n",
    "            ?writer rdf:type dbo:Writer ;\n",
    "            foaf:name %s .\n",
    "            ?writer dbo:genre ?genre .\n",
    "            ?genre rdfs:label ?text\n",
    "            FILTER (lang(?text) = \"en\")\n",
    "        }''' % self.authorname\n",
    "        print(qr)\n",
    "        sparql.setQuery(qr)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "        # print(results)\n",
    "        \n",
    "        genres = []\n",
    "        for i in range (len(results['results']['bindings'])):\n",
    "            genre = results['results']['bindings'][i]['text']['value']\n",
    "            genres.append(genre)\n",
    "        self.literarymovements = genres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9jRyG8qz1R1"
   },
   "source": [
    "### 3. Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hzx6BXsmMQMk"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  # instantiate a GutenbergCorpusBuilder \n",
    "  PGcorpus = GutenbergCorpusBuilder(corpusname=\"PG-eng-author-min2\")\n",
    "  # start collecting and filtering author and book details from the Project Gutenberg site\n",
    "  PGcorpus.get_library(min_books = 2, languages = [\"english\"], roles = [\"as author\"])\n",
    "  # read text files, select sentences, pre-process sentences, store to subcorpora\n",
    "#   PGcorpus.populate_corpus()\n",
    "  # write subcorpora to file \n",
    "#   PGCorpus.write_to_file()\n",
    "  \n",
    "  # import to mongoDB. export mongoDB database. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKrllplDkyqb"
   },
   "source": [
    "### 4. Test code - informal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJCo6Hhxkxn4"
   },
   "outputs": [],
   "source": [
    "# check that corpus contains only english books. it should return nothing. \n",
    "for i in PGcorpus.authors.keys(): \n",
    "    for i2 in PGcorpus.authors[i][\"books_info\"]:\n",
    "        if \"English\" not in PGcorpus.authors[i][\"books_info\"][i2][\"PG_booktitle\"]:\n",
    "            print(PGcorpus.authors[i][\"books_info\"][i2][\"PG_booktitle\"])\n",
    "\n",
    "# check that corpus contains only books where author role is as Author. it should return nothing.\n",
    "for i in PGcorpus.authors.keys(): \n",
    "    for i2 in PGcorpus.authors[i][\"books_info\"]:\n",
    "        if \"Author\" not in PGcorpus.authors[i][\"books_info\"][i2][\"PG_booktitle\"]:\n",
    "            print(PGcorpus.authors[i][\"books_info\"][i2][\"PG_booktitle\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UOe2Z-guIIG"
   },
   "source": [
    "### 5. pymongo implementation to store the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAZsdVRoz0xA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# instantiate a MongoClient object. using the URI for the Mongo server. If it is locally hosted, \n",
    "# it is by default on the 27017 port. If using cloud, use the provided URI\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "corpusdb = client[\"corpus\"]\n",
    "authorcollection = corpusdb[\"author\"]\n",
    "\n",
    "# insert the documents into the collection \n",
    "for i in PGcorpus.authors: \n",
    "    authorcollection.insert_one(PGcorpus.authors[i])\n",
    "\n",
    "# some test code to check insertions \n",
    "print(authorcollection.find_one(), authorcollection.\n",
    "authorcollection.estimated_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataScienceProject-Session1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
