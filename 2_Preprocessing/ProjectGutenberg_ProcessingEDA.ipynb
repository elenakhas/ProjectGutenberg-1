{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing text, extracting and visualising descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO:\n",
    "# 1. remove punctuation and -PRON- for lemmas\n",
    "# 2. review how tags are passed into process_one_author and support functions\n",
    "# 2. a wrapper function to process each author's abstract too / use @decorators with the retrieve_abstract\n",
    "# 3. a general function to transform the outputs of process_one_author into a single pandas dataframe .makedataframe\n",
    "# 4. consider dropping sentences of less than n tokens (there are 0 token sentences, after processing)\n",
    "# 5. generalised visualisations function\n",
    "# 6. write the README file regarding general overivew, technicalities, packages and usage\n",
    "\n",
    "\n",
    "### Important note:\n",
    "# 1. the current implementation is that each time process_one_author is run, it appends to the file. so if you run it \n",
    "# you will have twice the same information in the file. \n",
    "# 2. the files are currently written with \"\\t\" between elements for the same sentence, except named entites where \n",
    "# there is a sub-cat of info. these are separated by \"\\t\\t\".\n",
    "# 3. each line (for each sentence) is separated by a \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en2 to\n",
      "[nltk_data]     /Users/k1000mbp/nltk_data...\n",
      "[nltk_data]   Package benepar_en2 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, re, glob, pickle, string, collections\n",
    "from os import path\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns \n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from pg_dataextraction import GutenbergCorpusBuilder, Author\n",
    "import utils_loaddataframe, utils_tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authorabstracts</th>\n",
       "      <th>authorname</th>\n",
       "      <th>literarymovements</th>\n",
       "      <th>wiki_info</th>\n",
       "      <th>booknum</th>\n",
       "      <th>selected_sents</th>\n",
       "      <th>authornum</th>\n",
       "      <th>filename</th>\n",
       "      <th>booktitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'en': 'Christian Nephi Anderson (January 22, ...</td>\n",
       "      <td>Anderson Nephi</td>\n",
       "      <td>[lds fiction]</td>\n",
       "      <td>{'en': 'http://en.wikipedia.org/wiki/Nephi_And...</td>\n",
       "      <td>16534</td>\n",
       "      <td>[When did Joseph visit Jackson county the seco...</td>\n",
       "      <td>a4501</td>\n",
       "      <td>./data/booksample_txt/a4501_16534.txt</td>\n",
       "      <td>A Young Folks' History of the Church of Jesus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'en': 'Christian Nephi Anderson (January 22, ...</td>\n",
       "      <td>Anderson Nephi</td>\n",
       "      <td>[lds fiction]</td>\n",
       "      <td>{'en': 'http://en.wikipedia.org/wiki/Nephi_And...</td>\n",
       "      <td>17249</td>\n",
       "      <td>[It has been revealed to and tried by men in v...</td>\n",
       "      <td>a4501</td>\n",
       "      <td>./data/booksample_txt/a4501_17249.txt</td>\n",
       "      <td>Added Upon\\rA Story (English) (as Author)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     authorabstracts      authorname  \\\n",
       "0  {'en': 'Christian Nephi Anderson (January 22, ...  Anderson Nephi   \n",
       "1  {'en': 'Christian Nephi Anderson (January 22, ...  Anderson Nephi   \n",
       "\n",
       "  literarymovements                                          wiki_info  \\\n",
       "0     [lds fiction]  {'en': 'http://en.wikipedia.org/wiki/Nephi_And...   \n",
       "1     [lds fiction]  {'en': 'http://en.wikipedia.org/wiki/Nephi_And...   \n",
       "\n",
       "   booknum                                     selected_sents authornum  \\\n",
       "0    16534  [When did Joseph visit Jackson county the seco...     a4501   \n",
       "1    17249  [It has been revealed to and tried by men in v...     a4501   \n",
       "\n",
       "                                filename  \\\n",
       "0  ./data/booksample_txt/a4501_16534.txt   \n",
       "1  ./data/booksample_txt/a4501_17249.txt   \n",
       "\n",
       "                                           booktitle  \n",
       "0  A Young Folks' History of the Church of Jesus ...  \n",
       "1          Added Upon\\rA Story (English) (as Author)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_authorbook_df = utils_loaddataframe.loaddataframe()\n",
    "corpus_authorbook_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a. Dataframes to store the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authornum</th>\n",
       "      <th>literarymovements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a4501</td>\n",
       "      <td>[lds fiction]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  authornum literarymovements\n",
       "0     a4501     [lds fiction]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use add the generated features from the author's books and abstracts to these dataframes\n",
    "\n",
    "author_ab_feat_df = corpus_authorbook_df[[\"authornum\", \"literarymovements\"]].copy()\n",
    "author_bk_feat_df = corpus_authorbook_df[[\"authornum\", \"literarymovements\"]].copy()\n",
    "author_bk_feat_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6MXr7wBJ1ZHQ"
   },
   "source": [
    "### 1a. Preprocessing for one author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a df containing only authornum and their wiki abstracts (in dicts). remove duplicates\n",
    "_ = corpus_authorbook_df[\"authornum\"].copy().drop_duplicates().index\n",
    "all_abstracts_df = corpus_authorbook_df[[\"authornum\",\"authorabstracts\"]].loc[_]\n",
    "# set the author num as the index\n",
    "all_abstracts_df.set_index(\"authornum\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4501 = utils_tokeniser.process_one_author(\"a4501\", functions=[utils_tokeniser.get_sentence, utils_tokeniser.get_tokens,\n",
    "                                               utils_tokeniser.get_lemmas,utils_tokeniser.get_postags,\n",
    "                                               utils_tokeniser.get_namedentities,utils_tokeniser.get_parsetags,\n",
    "                                               utils_tokeniser.get_concreteness])\n",
    "\n",
    "a206 = utils_tokeniser.process_one_author(\"a206\", functions=[utils_tokeniser.get_sentence, utils_tokeniser.get_tokens,\n",
    "                                               utils_tokeniser.get_lemmas,utils_tokeniser.get_postags,\n",
    "                                               utils_tokeniser.get_namedentities,utils_tokeniser.get_parsetags,\n",
    "                                               utils_tokeniser.get_concreteness])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBxWHm1r6HTk"
   },
   "source": [
    "### 2. Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2a. Vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in a206[\"tokens\"]:\n",
    "    vocab.update(sent)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2b. Average sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list()\n",
    "for sent in a206[\"tokens\"]:\n",
    "    words.append(sent)\n",
    "len(vocab)/len(a206[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max length of sentences\n",
    "words = list()\n",
    "for sent in a206[\"tokens\"]:\n",
    "    words.append(len(sent))\n",
    "max(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max length of sentences\n",
    "words = list()\n",
    "for sent in a206[\"tokens\"]:\n",
    "    words.append(len(sent))\n",
    "min(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2c. POS distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2d. Most frequent named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max length of sentences\n",
    "NERs = list()\n",
    "for sent in a206[\"namedentities\"]:\n",
    "    for type_ in sent:\n",
    "        NERs.extend(sent[type_])\n",
    "collections.Counter(NERs).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhz4HMBl6NW8"
   },
   "source": [
    "##### 2e. Min, max, average NPs and VPs per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QN-4NJciFcAK"
   },
   "source": [
    "**Visualizations**\n",
    "\n",
    "\n",
    "\n",
    "* Author and the number of distinct nouns/ adj / verbs, etc. Prepare dummy code for different types of diagrams\n",
    "* POS distribution, at author and literary movement.-- horizontal barplots\n",
    "* Word frequency across corpus\n",
    "* Average tokens per sentence, for each literary movement/author.\n",
    "* Word cloud per movement? for top 10 movements\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_visuals(author_df): \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # set up 1x2 plt plot \n",
    "    \n",
    "    _vocab_boxplot\n",
    "    _sentsize_boxplot\n",
    "    \n",
    "    # set up 1x1 plt plot that shows all the pos_tags \n",
    "    _posdistributions()\n",
    "    \n",
    "    \n",
    "    _makecloud()\n",
    "    \n",
    "    # print to file? plt.savefig with authornum\n",
    "    \n",
    "    \n",
    "    \n",
    "def _vocab_boxplot():\n",
    "    \n",
    "    pass\n",
    "    \n",
    "def _sentsize_boxplot(): \n",
    "    \n",
    "    pass \n",
    "def _posdistributions():\n",
    "    '''\n",
    "    '''\n",
    "    pass \n",
    "def _makecloud():\n",
    "    '''\n",
    "    '''\n",
    "    # Start with one review:\n",
    "    text = \"hello world, the world is round and square and square and square and square\"\n",
    "    # Create and generate a word cloud image:\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "\n",
    "    # Display the generated image:\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
