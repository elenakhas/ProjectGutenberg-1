{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing text, extracting and visualising descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/k1000mbp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/k1000mbp/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/k1000mbp/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/k1000mbp/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n"
     ]
    }
   ],
   "source": [
    "import os, re, glob, pickle, string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS #312 stopwords\n",
    "\n",
    "java_path = r'/usr/lib/jvm/java-8-oracle/jre/bin/java'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "scp = StanfordParser(path_to_jar='./supportdata/CoreNLP/stanford-corenlp-3.9.2.jar',\n",
    "           path_to_models_jar='./supportdata/CoreNLP/stanford-english-corenlp-2018-10-05-models.jar')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn \n",
    "\n",
    "from pg_dataextraction import GutenbergCorpusBuilder, Author\n",
    "import utils_tokeniser_vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a convenience function to load a previously pickled GutenbergCorpusBuilder object\n",
    "def pickleloader(filename):\n",
    "    # # open the file for writing\n",
    "    fileObject = open(filename,'rb')\n",
    "    \n",
    "    return pickle.load(fileObject)  \n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()\n",
    "\n",
    "PGcorpus = pickleloader(\"PG-eng-author-min3v2019424.pickle\")\n",
    "# run/re-run cells containing GutenbergCorpusBuilder and Author class before loading the pickle file\n",
    "# to provide the unpickler attribute structures of the class for unpickling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authorabstracts</th>\n",
       "      <th>authorname</th>\n",
       "      <th>literarymovements</th>\n",
       "      <th>wiki_info</th>\n",
       "      <th>booknum</th>\n",
       "      <th>selected_sents</th>\n",
       "      <th>authornum</th>\n",
       "      <th>filename</th>\n",
       "      <th>booktitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'en': 'Christian Nephi Anderson (January 22, ...</td>\n",
       "      <td>Anderson Nephi</td>\n",
       "      <td>[lds fiction]</td>\n",
       "      <td>{'en': 'http://en.wikipedia.org/wiki/Nephi_And...</td>\n",
       "      <td>16534</td>\n",
       "      <td>[When did Joseph visit Jackson county the seco...</td>\n",
       "      <td>a4501</td>\n",
       "      <td>./data/booksample_txt/a4501_16534.txt</td>\n",
       "      <td>A Young Folks' History of the Church of Jesus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'en': 'Christian Nephi Anderson (January 22, ...</td>\n",
       "      <td>Anderson Nephi</td>\n",
       "      <td>[lds fiction]</td>\n",
       "      <td>{'en': 'http://en.wikipedia.org/wiki/Nephi_And...</td>\n",
       "      <td>17249</td>\n",
       "      <td>[It has been revealed to and tried by men in v...</td>\n",
       "      <td>a4501</td>\n",
       "      <td>./data/booksample_txt/a4501_17249.txt</td>\n",
       "      <td>Added Upon\\rA Story (English) (as Author)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     authorabstracts      authorname  \\\n",
       "0  {'en': 'Christian Nephi Anderson (January 22, ...  Anderson Nephi   \n",
       "1  {'en': 'Christian Nephi Anderson (January 22, ...  Anderson Nephi   \n",
       "\n",
       "  literarymovements                                          wiki_info  \\\n",
       "0     [lds fiction]  {'en': 'http://en.wikipedia.org/wiki/Nephi_And...   \n",
       "1     [lds fiction]  {'en': 'http://en.wikipedia.org/wiki/Nephi_And...   \n",
       "\n",
       "   booknum                                     selected_sents authornum  \\\n",
       "0    16534  [When did Joseph visit Jackson county the seco...     a4501   \n",
       "1    17249  [It has been revealed to and tried by men in v...     a4501   \n",
       "\n",
       "                                filename  \\\n",
       "0  ./data/booksample_txt/a4501_16534.txt   \n",
       "1  ./data/booksample_txt/a4501_17249.txt   \n",
       "\n",
       "                                           booktitle  \n",
       "0  A Young Folks' History of the Church of Jesus ...  \n",
       "1          Added Upon\\rA Story (English) (as Author)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the json mongo exports for the authors and books info into pandas dataframes\n",
    "authors_df = pd.read_json(\"./data/mongo_dumps/jsondump_authors_mongo.json\")\n",
    "books_df = pd.read_json(\"./data/mongo_dumps/jsondump_books_mongo.json\")\n",
    "\n",
    "# the books collection (imported into books_df) contains the eventual set of authors and books selected for the \n",
    "# corpus we want to join both dataframes on the set of authornums in books_df. \n",
    "\n",
    "# 1. make a copy of authors_df so we don't work on the original data\n",
    "corpus_authors_df = authors_df.copy()\n",
    "# 2. we set the index for copy of the authors_df to the values in \"authornum\"\n",
    "corpus_authors_df = corpus_authors_df.set_index(corpus_authors_df[\"authornum\"])\n",
    "# 3. with the index set, we  can slice the dataframe to get only the authornums present in books_df\n",
    "corpus_authors_df = corpus_authors_df.loc[list(books_df[\"authornum\"].copy().unique())]\n",
    "# 4. we also set the index for books df to authornums. we need this for the join below\n",
    "books_df = books_df.set_index(books_df[\"authornum\"])\n",
    "\n",
    "# 5. use pd.concat for both dfs. use inner join. place the smaller df on the left. (we know all authornums in \n",
    "# books_df are present in corpus_authors_df)\n",
    "corpus_authorbook_df = pd.concat([corpus_authors_df.copy(), books_df], axis=1, join=\"inner\")\n",
    "\n",
    "# 7. get the book txt file names \n",
    "filenames = glob.glob(\"./data/booksample_txt/*\")\n",
    "files=[]\n",
    "for filename in filenames:\n",
    "    _ = filename.rstrip(\".txt\")\n",
    "    _ = _.split(\"/\")[-1]\n",
    "    authornum = _.split(\"_\")[0]\n",
    "    booknum = _.split(\"_\")[1]\n",
    "    file_dict ={\"authornum\":authornum, \"booknum\":booknum,\"filename\":filename }\n",
    "    files.append(file_dict)\n",
    "files_df = pd.DataFrame(files)\n",
    "\n",
    "files_df.set_index(files_df[\"booknum\"])\n",
    "files_df.head(2)\n",
    "\n",
    "\n",
    "# 8. since both dfs are of the same height use pd.merge for both dfs. \n",
    "# use inner join by default. but before that, corpus_author_df has booknum in\n",
    "# int64, files_df has booknum as strings (from the split from the filename)\n",
    "files_df[\"booknum\"]=files_df[\"booknum\"].astype(\"int64\")\n",
    "corpus_authorbook_df = pd.merge(corpus_authorbook_df.copy(),files_df, on=\"booknum\")\n",
    "\n",
    "\n",
    "# 9. add the book titles to the dataframe\n",
    "titles = []\n",
    "for row in corpus_authorbook_df.index:\n",
    "    for booknum in corpus_authorbook_df[\"books_info\"][row]: \n",
    "        if str(corpus_authorbook_df.loc[row,\"booknum\"]) == booknum:\n",
    "            titles.append(corpus_authorbook_df.loc[row,\"books_info\"][booknum])\n",
    "corpus_authorbook_df.loc[:,\"booktitle\"] = titles\n",
    "\n",
    "# 10 drop the books_info and other columns that are not necessary\n",
    "corpus_authorbook_df.drop(columns=[\"books_info\", \"_id\", \"authornum_x\"], inplace=True)\n",
    "corpus_authorbook_df.rename(columns={'authornum_y':'authornum'}, inplace = True)\n",
    "corpus_authorbook_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1a. Concreteness score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_df = pd.read_excel(\"./supportdata/ConcretenessRatings/Concreteness_ratings_Brysbaert_et_al_BRM.xlsx\")\n",
    "# there are \"words\" in the corpus that are actually bi-grams. e.g. baking soda \n",
    "concrete_df[concrete_df[\"Bigram\"]==1].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_df[concrete_df[\"Bigram\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Visualisation and descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
