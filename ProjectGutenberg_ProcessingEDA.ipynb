{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing text, extracting and visualising descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy, nltk\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO List \n",
    "# 1. tidy up preprocessing (get POS, ngram)\n",
    "# 2. write a function to get concretisation score for each sentence/book. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_index(tokenslist):\n",
    "    '''\n",
    "    given a list of tokens, generate two indices (index2token, token2index) \n",
    "    input | tokenslist: list - \n",
    "    output | two dictionaries containing index2token, token2index respectively. \n",
    "    '''\n",
    "    token_to_index = {}\n",
    "    index_to_token = {}\n",
    "    \n",
    "    _tokens_unique = set([i.lower() for i in tokenslist])\n",
    "    _tokens_unique = list(__tokens_unique) # return it to a set so that it is iterable\n",
    "    # sort the dictionary by alphabet for easy searching\n",
    "    # \n",
    "    for idx_number in range(len(_tokens_unique)):\n",
    "        \n",
    "        index_to_token[idx_number] =  _tokens_unique[idx_number]\n",
    "        token_to_index[_tokens_unique[idx_number]] = idx_number\n",
    "        \n",
    "    return index_to_token, token_to_index\n",
    "\n",
    "def generate_emptyarr(num_sentences, max_sentencelength, vocab_size):\n",
    "    _array = np.zeros([num_sentences, max_sentencelength, vocab_size], dtype=int)\n",
    "    return _array \n",
    "\n",
    "array = generate_emptyarr(num_sentences, max_sentencelength, vocab_size)\n",
    "\n",
    "\n",
    "def generate_array(sentences, array, token_to_index): \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    for sent_num in range(len(sentences)):\n",
    "        # send the sentence through spacy\n",
    "        _ = nlp(sentences[sent_num])\n",
    "        # filter out the punctuation, tokens retained are stored in str form (.text attribute on spacy tokens)\n",
    "        _ = [token.text for token in __ if token.pos_ != \"PUNCT\"]\n",
    "        # get the matrix for the sentence\n",
    "        matrix = array[sent_num]\n",
    "\n",
    "        for word_num in range(len(__)):\n",
    "            # get the vector for the position of the word in the sentence\n",
    "            vector = matrix[word_num] \n",
    "            # get the str form of the word. lowercase it to match the entries in the vocabulary dictionary                  \n",
    "            word = _[word_num].lower()\n",
    "            # find its index in the tok_to_index dictionary\n",
    "            index = token_to_index[word]\n",
    "            # go to that index on the vector and update it to 1\n",
    "            vector[index] = 1\n",
    "        \n",
    "    return  array\n",
    "        \n",
    "def recover_text(results_array, index_to_token):\n",
    "    # empty list to store recovered sentences \n",
    "    text = []\n",
    "    \n",
    "    # iterate through every sentence (1st dimension) of the array \n",
    "    for sentence in array: \n",
    "        # empty list to store the words recovered within this sentence \n",
    "        words = []\n",
    "        for word in sentence: \n",
    "            # check using if-continue. only continue if the sum of the vector for the word is > 0 [i.e \n",
    "            # stop the iteration the moment an empty vector comes up. ]\n",
    "            if sum(word) > 0:\n",
    "                index_of_word = np.argmax(word)\n",
    "                words.append(index_to_token[index_of_word])\n",
    "                continue \n",
    "        text.append(words)\n",
    "        \n",
    "    return text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word_pos_tuple):\n",
    "    \"\"\"\n",
    "    Helper function for text_preprocessor. Takes a tuple of (token, pos_tag) generated from running a tokenised \n",
    "    sentence through nltk.word_tokenize, and maps POS tag to the first character that nltk wordnetlemmatizer's \n",
    "    .lemmatize() method accepts\n",
    "    source: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#wordnetlemmatizerwithappropriatepostag \n",
    "    \"\"\"\n",
    "    tag = word_pos_tuple[1][0]\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag)\n",
    "\n",
    "def text_preprocessor(text):\n",
    "    '''\n",
    "    Takes a text (a sentence, or a document) and preprocesses for the purposes of generating machine learning data from the \n",
    "    input. The preprocessing includes: (a) tokenisation, (b) removal of punctuation, (c) lemmatisation and lowercasing. \n",
    "    Returns a list of tokens from the input text. \n",
    "    '''\n",
    "    _processed = []\n",
    "    \n",
    "    # tokenize the string\n",
    "    _tokens = word_tokenize(text)\n",
    "    # use nltk's pos_tag function to get the pos_tag for the string of tokens. \n",
    "    _tokens_postags = pos_tag(_tokens)\n",
    "\n",
    "    \n",
    "    for token_postag in _tokens_postags:  \n",
    "        if token_postag[1] not in string.punctuation:\n",
    "        # use get_wordnet_pos helper function to get the equivalent WordNetLemmatiser pos-tag\n",
    "            wn_pos = get_wordnet_pos(token_postag)\n",
    "            # WordNetLemmatiser only has tags for a, n, v, r. if-else to handle this. \n",
    "            if wn_pos != None: \n",
    "                _lemma = wnl.lemmatize(token_postag[0], wn_pos).lower()\n",
    "            else:\n",
    "                _lemma = token_postag[0].lower()\n",
    "            _processed.append(_lemma)\n",
    "    \n",
    "    return __processed\n",
    "\n",
    "def compute_vocabulary(text_lists):\n",
    "    '''\n",
    "    takes a list containing texts, (i) processes it and (ii) returns a pandas dataframe with the count of each word. \n",
    "    The processing involves: (a) tokenisation, (b) removal of punctuation, (c) lemmatisation and lowercasing. \n",
    "    '''\n",
    "    _vocab_df = pd.DataFrame()\n",
    "    counter = 0\n",
    "    for text in text_lists: \n",
    "        _processed_text = text_preprocessor(text)\n",
    "        _counter = collections.Counter(_processed_text)\n",
    "        _text_df = pd.DataFrame([list(_counter.values())], columns = list(_counter.keys()))\n",
    "        \n",
    "        _vocab_df = pd.concat([_vocab_df, _text_df], sort=False, ignore_index=True)\n",
    "        counter +=1\n",
    "        if counter%250 ==0:\n",
    "            print (counter)\n",
    "    \n",
    "    return _vocab_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter short sentences. \n",
    "def filter_df_shortsent(dataframe, min_tokens):\n",
    "    _tokeep_index = [    ]\n",
    "    [_tokeep_index.append(i) for i in dataframe.index if dataframe.loc[i].sum() >= min_tokens]\n",
    "    return dataframe.loc[_tokeep_index]\n",
    "\n",
    "# function to compute the frequency of all words in the dataset\n",
    "def compute_vocab_freq(dataframe, top_n = 10, ascending=False):\n",
    "    return dataframe.describe().loc['count'].sort_values(ascending=ascending)[0:top_n]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write a function that extracts the features, i.e. take all the books as input and return them as a list of lists (e.g. a list of lists of tokens)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_unigrams(texts_list):\n",
    "    '''\n",
    "    Takes a flat list (each containing strings - corresponding to a sentence or document), tokenises each sentence \n",
    "    using NLTK's word_tokenize and stores the results in a list. Does this for each sentence and then returns a \n",
    "    list of lists. \n",
    "    '''\n",
    "    _tokenised_text_list = []\n",
    "    for text in texts_list: \n",
    "        _tokens = word_tokenize(text)\n",
    "        _tokenised_text_list.append(_tokens)\n",
    "\n",
    "    return _tokenised_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise_unigrams(texts_list):\n",
    "    '''\n",
    "    Takes a text (a sentence, or a document) and preprocesses for the purposes of generating machine learning data from the \n",
    "    input. The preprocessing includes: (a) tokenisation, (b) removal of punctuation, (c) lemmatisation and lowercasing. \n",
    "    Returns a list of tokens from the input text. Calls on the text_preprocessor function. \n",
    "    '''\n",
    "    _lemmatised_text_list = []\n",
    "    \n",
    "    for text in texts_list: \n",
    "        _tokens = text_preprocessor(text)\n",
    "        _lemmatised_text_list.append(_tokens)\n",
    "    \n",
    "    return _lemmatised_text_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams (processed_texts_lists, n_gram=2, add_padding=False):\n",
    "    '''\n",
    "    Takes a list of lists (the lists contain pre-processed tokens) and produces n-gram tokens. Outputs a list of lists (in\n",
    "    the same structure as the input). \n",
    "    '''\n",
    "    # empty list to store the generated n-grams \n",
    "    ngrams_list = [] \n",
    "    \n",
    "    \n",
    "    # a for-loop just to iterate the number of times equal to the num of tokens in list \n",
    "    for processed_list in processed_texts_lists:\n",
    "        _list = []\n",
    "        counter = 0 \n",
    "        for token in processed_list:\n",
    "            # try-except to handle IndexErrors \n",
    "            try:\n",
    "                ngram = \" \".join([processed_list[0+i] for i in range(counter,counter+n_gram)])\n",
    "\n",
    "            except IndexError: \n",
    "                if add_padding==False: \n",
    "                    break\n",
    "                elif add_padding==True: \n",
    "                    # grab the remaining words that have not had n-grams generated from each of their positions\n",
    "                    remaining_words = [processed_list[-1-i] for i in range(len(processed_list)-counter)]\n",
    "                    # reverse the list since it was adding from the end of the previous \n",
    "                    remaining_words.reverse()\n",
    "                    # end \"<END>\" tokens to pad the remaining spaces in the n-gram\n",
    "                    ngram = \" \".join(remaining_words + [\"<END>\"]*(n_gram-len(remaining_words)))\n",
    "\n",
    "            _list.append(ngram)\n",
    "            counter+=1\n",
    "        ngrams_list.append(_list)\n",
    "    return ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the year',\n",
       "  'year shall',\n",
       "  'shall run',\n",
       "  'run like',\n",
       "  'like rabbit',\n",
       "  'rabbit rabbit',\n",
       "  'rabbit rabbit'],\n",
       " ['the year', 'year shall', 'shall run', 'run like', 'like chicken']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_ngrams(lemmatise_unigrams([test1,test2]),n_gram=2, add_padding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write a function that takes the list and return a list of dictionnaries {attribute:value} where you might have different options to compute the value (here, only raw frequency or relative frequency. That could be also done in an additional function i.e. normalizer).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency (data_lists , add_normalisation = None):\n",
    "    '''\n",
    "    takes a list of lists (each comprising processed inputs for a particular sentence or document). \n",
    "    \n",
    "    input | data:list - a list of lists. Each list inside contains either tokens, lemmas, bigrams, or trigrams. \n",
    "    \n",
    "    add_normalisation = \"tf_max\" implements max tf normalisation \n",
    "    (see https://nlp.stanford.edu/IR-book/html/htmledition/maximum-tf-normalization-1.html)\n",
    "    add_normalisation = \"relative_frequency\"\n",
    "    '''\n",
    "    dictcount_lists = []\n",
    "    \n",
    "    def dictcount_maker(row):\n",
    "        '''\n",
    "        helper function to take a list of tokens (that comprise/make up a sentence) and generates a dictionary with\n",
    "        the count of each token. \n",
    "        '''\n",
    "        dictcount = {}\n",
    "        for token in row:\n",
    "            try: \n",
    "                dictcount[token] += 1\n",
    "            except: \n",
    "                dictcount[token] = 1\n",
    "        return dictcount\n",
    "    \n",
    "    if add_normalisation == None: \n",
    "        for row in data_lists:\n",
    "            dictcount = dictcount_maker(row)\n",
    "            dictcount_lists.append(dictcount)\n",
    "\n",
    "            \n",
    "    elif add_normalisation == \"tf_max\":\n",
    "        for row in data_lists:\n",
    "            dictcount = dictcount_maker(row)\n",
    "            \n",
    "            # get the max count within the dictionary  \n",
    "            maxcount=max(dictcount.values())\n",
    "            \n",
    "            # dictionary comprehension to divide each count in the dictionary by the max count \n",
    "            # and apply a weight and \"bias\" to get the normalised frequency. \n",
    "            dictcount = {key:0.4+(1-0.4)*(count/maxcount) for key,count in dictcount.items()}\n",
    "            \n",
    "            dictcount_lists.append(dictcount)\n",
    "\n",
    "            \n",
    "    elif add_normalisation ==  \"relative_frequency\":\n",
    "        for row in data_lists:\n",
    "            dictcount = dictcount_maker(row)\n",
    "            \n",
    "            # get the sum of all frequency counts for each token. \n",
    "            totalcount=sum(dictcount.values())\n",
    "            \n",
    "            # dictionary comprehension to divide each count in the dictionary by the total count \n",
    "            # to get the relative frequency. \n",
    "            dictcount = {key:count/totalcount for key,count in dictcount.items()}\n",
    "            \n",
    "            dictcount_lists.append(dictcount)\n",
    "    \n",
    "    else: \n",
    "        print(\"The add_normalisation parameter chosen is not recognised. Please check.\")\n",
    "    \n",
    "    return dictcount_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: compare tf_max and tfidf implementation in sklearn to understand difference in assumptions and subsequent impact. \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'the': 0.125,\n",
       "  'year': 0.125,\n",
       "  'shall': 0.125,\n",
       "  'run': 0.125,\n",
       "  'like': 0.125,\n",
       "  'rabbit': 0.375},\n",
       " {'the': 0.16666666666666666,\n",
       "  'year': 0.16666666666666666,\n",
       "  'shall': 0.16666666666666666,\n",
       "  'run': 0.16666666666666666,\n",
       "  'like': 0.16666666666666666,\n",
       "  'chicken': 0.16666666666666666},\n",
       " {'i': 0.125,\n",
       "  'saw': 0.125,\n",
       "  'her': 0.125,\n",
       "  'sell': 0.125,\n",
       "  'seashell': 0.125,\n",
       "  'on': 0.125,\n",
       "  'a': 0.125,\n",
       "  'seashore': 0.125},\n",
       " {'i': 0.08333333333333333,\n",
       "  'shall': 0.08333333333333333,\n",
       "  'see': 0.16666666666666666,\n",
       "  'her': 0.08333333333333333,\n",
       "  'sell': 0.08333333333333333,\n",
       "  'seashell': 0.08333333333333333,\n",
       "  'on': 0.08333333333333333,\n",
       "  'a': 0.08333333333333333,\n",
       "  'seashore': 0.08333333333333333,\n",
       "  'let': 0.08333333333333333,\n",
       "  \"'s\": 0.08333333333333333}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_frequency(make_ngrams(lemmatise_unigrams([test1,test2,test3,test4]),n_gram=1),add_normalisation = \"relative_frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write the vectorizer: a function that compute the entire vocabulary, and return a list of Numpy Arrays of the size the vocabulary size and the value corresponding to the feature values.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectoriser(dictcount_lists):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # pseudocode: \n",
    "    # 1. iterate through the list of dictcounts. get all keys and add to a set (call this vocab). set because it is sorted \n",
    "    # and no repeated values\n",
    "    # 2. iterate through all the list of distcounts again. create an empty np array of the same size as the vocab. get index \n",
    "    # of each key in the distcount in the vocab. add values of distcount to the respective entry in the nparray. \n",
    "\n",
    "    # question: what is the impact of the choice tf_max and relative_frequency? i.e. values in the np are no longer zeros and\n",
    "    # and counts, but zeros and frequencies (at sentence/document level)\n",
    "    \n",
    "    vocabulary_set = set()\n",
    "    for distcount in dictcount_lists:\n",
    "        sentence_tokens = set(distcount.keys())\n",
    "        vocabulary_set.update(sentence_tokens)\n",
    "    \n",
    "    # convert the vocab into a list so that we can use its index \n",
    "    vocabulary_list = list(vocabulary_set)\n",
    "    \n",
    "    # \n",
    "    vectorised_arrays = []\n",
    "    for distcount in dictcount_lists:\n",
    "        _array = np.zeros(len(vocabulary_list))\n",
    "        for token in distcount: \n",
    "            index_in_vocab = vocabulary_list.index(token)\n",
    "            _array[index_in_vocab] = distcount[token]\n",
    "            \n",
    "        vectorised_arrays.append(_array)\n",
    "        \n",
    "        \n",
    "    return vectorised_arrays, vocabulary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0.14285714, 0.14285714, 0.14285714, 0.14285714, 0.28571429,\n",
       "         0.14285714, 0.        ]), array([0.2, 0. , 0.2, 0.2, 0. , 0.2, 0.2])],\n",
       " ['year shall',\n",
       "  'like rabbit',\n",
       "  'run like',\n",
       "  'shall run',\n",
       "  'rabbit rabbit',\n",
       "  'the year',\n",
       "  'like chicken'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = compute_frequency(make_ngrams(lemmatise_unigrams([test1,test2]),n_gram=2, add_padding=False),add_normalisation = \"relative_frequency\")\n",
    "vectoriser(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Filtering:__\n",
    "\n",
    "Write a function that filters out function words\n",
    "\n",
    "Write a function that filters out words appearing less than X times in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterfunction(vectorised_arrays, vocabulary_list, datamin_freq = 10):\n",
    "    '''\n",
    "    takes a list of vectorised arrays as well as its associated vocabulary list. checks the global count/frequency \n",
    "    (normalised or not), removes from all arrays the columns where the global count/frequency (normalised or not)\n",
    "    is below the provided value. removes the same columns from the vocabulary list returns (i) a new list of arrays;\n",
    "    and (ii) a new vocabulary list.\n",
    "    '''\n",
    "    # pseudocode\n",
    "    # easiest way: place all vectorised_arrays into a pandas and sum values of each column. then filter out \n",
    "    # manual way: concat all np.arrays. slice by column. sum it out and delete if < certain value. remember to remove \n",
    "    # the corresponding word in the vocab list. \n",
    "    \n",
    "    # since each np array is the same size and columns are all aligned (i.e. indexed to the vocab list), we can just \n",
    "    # sum all the np arrays, this will generate a 1D array with the sum on each of the columns\n",
    "    total_freqs = sum(vectorised_arrays)\n",
    "    \n",
    "    # identify the columns for words that have counts less than datamin_freq\n",
    "    to_delete = []\n",
    "    for col, val in enumerate(total_freqs):\n",
    "        if val < datamin_freq:\n",
    "            to_delete.append(col)\n",
    "    \n",
    "    # remove the columns from each of the vectorised arrays \n",
    "    new_vectorised_arrays = []\n",
    "    for vectorised_array in vectorised_arrays:\n",
    "        new_vectorised_arrays.append(np.delete(vectorised_array, to_delete))\n",
    "    \n",
    "    # remove the columns from vocabulary list \n",
    "    new_vocabulary_list = np.delete(vocabulary_list, to_delete)\n",
    "    \n",
    "    return new_vectorised_arrays, new_vocabulary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 7, 9, 12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([0.   , 0.   , 0.125, 0.125, 0.   , 0.   , 0.125, 0.125, 0.125,\n",
       "         0.   , 0.   , 0.   , 0.375]),\n",
       "  array([0.        , 0.        , 0.16666667, 0.16666667, 0.        ,\n",
       "         0.        , 0.16666667, 0.16666667, 0.16666667, 0.        ,\n",
       "         0.        , 0.        , 0.        ]),\n",
       "  array([0.125, 0.125, 0.   , 0.   , 0.125, 0.125, 0.   , 0.   , 0.   ,\n",
       "         0.125, 0.125, 0.125, 0.   ]),\n",
       "  array([0.08333333, 0.08333333, 0.        , 0.        , 0.08333333,\n",
       "         0.08333333, 0.08333333, 0.        , 0.        , 0.08333333,\n",
       "         0.08333333, 0.08333333, 0.        ])],\n",
       " array(['her', 'a', 'run', 'year', 'on', 'i', 'shall', 'like', 'the',\n",
       "        'seashell', 'sell', 'seashore', 'rabbit'], dtype='<U8'))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lem_freq = compute_frequency(make_ngrams(lemmatise_unigrams([test1,test2,test3,test4]),n_gram=1),add_normalisation = \"relative_frequency\")\n",
    "test_vect = vectoriser(test_lem_freq)\n",
    "filterfunction(test_vect[0], test_vect[1],datamin_freq=0.2)\n",
    "\n",
    "# there are np arrays with 0 values still because the global freq for that word is more than the datamin_freq set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cross-features:__\n",
    "\n",
    "Write a function that returns the cartesian product of the features, the values being the multiplication of the values of each feature, i.e.:\n",
    "<f0:v0, f1:v1, f2:2, f3:v3> \n",
    "> Becomes (we also keep the original features):\n",
    "<f0_f1:v0*v1, f0_f2:v0*v2, f0_f3:v0*v3, f1_f2:v1*v2, f1_f3:v1*v3, f2_f3:v02*v3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair-wise interaction only:\n",
    "def generate_interaction(vectorised_array, vocabulary_list): \n",
    "    '''\n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    interaction_vals = []\n",
    "    interaction_vocab = []\n",
    "    for index in range(len(vectorised_array)):\n",
    "        val1 = vectorised_array[index]\n",
    "        val1_name = vocabulary_list[index]\n",
    "        vals2 = np.delete(vectorised_array, index)\n",
    "        vals2_names = vocabulary_list.copy()\n",
    "        del vals2_names[index]\n",
    "        \n",
    "        interaction_names = [val1_name+\"_\"+vals2_names[i] for i in range(len(vals2_names))]\n",
    "        interactions = [val1*vals2[i] for i in range(len(vals2))]\n",
    "        \n",
    "        interaction_vocab.extend(interaction_names)\n",
    "        interaction_vals.extend(interactions)\n",
    "        \n",
    "    return interaction_vals, interaction_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Visualisation and descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
