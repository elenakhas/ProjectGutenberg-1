{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "import utils_vectoriser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create training and evaluation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do train-test this at each author level, before merging and exporting as SVMlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authornum</th>\n",
       "      <th>concreteness</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>ne_dates</th>\n",
       "      <th>ne_persons</th>\n",
       "      <th>ne_places</th>\n",
       "      <th>parse_NP</th>\n",
       "      <th>parse_S</th>\n",
       "      <th>parse_VP</th>\n",
       "      <th>pos_ADJ</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_PUNCT</th>\n",
       "      <th>pos_SCONJ</th>\n",
       "      <th>pos_SPACE</th>\n",
       "      <th>pos_SYM</th>\n",
       "      <th>pos_VERB</th>\n",
       "      <th>pos_X</th>\n",
       "      <th>poswordpairs</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a95</td>\n",
       "      <td>0.25</td>\n",
       "      <td>[-PRON-, be, pleased, with, the, word, .]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[(PRON, I), (VERB, am), (ADJ, pleased), (ADP, ...</td>\n",
       "      <td>I am pleased with the word.</td>\n",
       "      <td>[i, am, pleased, with, the, word]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a95</td>\n",
       "      <td>2.25</td>\n",
       "      <td>[[, 1, ], this, be, immediately, follow, by, a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[(PUNCT, [), (PUNCT, 1), (PUNCT, ]), (DET, Thi...</td>\n",
       "      <td>[1] This was immediately followed by an interv...</td>\n",
       "      <td>[1, this, was, immediately, followed, by, an, ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  authornum concreteness                                             lemmas  \\\n",
       "0       a95         0.25          [-PRON-, be, pleased, with, the, word, .]   \n",
       "1       a95         2.25  [[, 1, ], this, be, immediately, follow, by, a...   \n",
       "\n",
       "  ne_dates ne_persons ne_places  parse_NP  parse_S  parse_VP  pos_ADJ  \\\n",
       "0       []         []        []         2        1         1        1   \n",
       "1       []         []        []         7        4         3        1   \n",
       "\n",
       "      ...       pos_PUNCT  pos_SCONJ  pos_SPACE  pos_SYM  pos_VERB  pos_X  \\\n",
       "0     ...               1          0          0        0         1      0   \n",
       "1     ...               5          0          0        0         3      0   \n",
       "\n",
       "                                        poswordpairs  \\\n",
       "0  [(PRON, I), (VERB, am), (ADJ, pleased), (ADP, ...   \n",
       "1  [(PUNCT, [), (PUNCT, 1), (PUNCT, ]), (DET, Thi...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0                        I am pleased with the word.   \n",
       "1  [1] This was immediately followed by an interv...   \n",
       "\n",
       "                                              tokens  sent_length  \n",
       "0                  [i, am, pleased, with, the, word]            6  \n",
       "1  [1, this, was, immediately, followed, by, an, ...           23  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_pickle(\"../2_Preprocessing/processeddata/df_pickles/a95_df.pickle\")\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = utils_vectoriser._compute_frequency(test[\"lemmas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = utils_vectoriser._vectoriser(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1882"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data set. every single author_df, merged.  \n",
    "# extract the features \n",
    "# tokens with TFID normalisation on the raw counts  - run the tokeniser and then add to the dataframe \n",
    "# NER with TFID normalisation on the raw counts \n",
    "# check correlation between features and \n",
    "\n",
    "# saving to SVM \n",
    "# train_tokens, test_tokens\n",
    "# train_ner, test_ner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to svmlight \n",
    "sklearn.datasets.dump_svmlight_file()\n",
    "\n",
    "def _save_to_file( X, y, file):\n",
    "    '''\n",
    "    Save X and y into file, using the svmlight format.\n",
    "    X: sparse matrix (samples)\n",
    "    y: numpy array (labels)\n",
    "    file: path \n",
    "    '''\n",
    "    dump_svmlight_file( X, y, file )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multilabel Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_multilabel_classification(n_classes=5, n_labels=5,\n",
    "                                      allow_unlabeled=True,\n",
    "                                      random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to get the labels encoded like this for multilabel classification\n",
    "# step 1, find out how many unique labels we have in the corpus\n",
    "# have a look at the distribution of the label. are there significant outliers. if few, consider removing them\n",
    "# else, we may consider augmenting datapoints with SMOTE \n",
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn import datasets\n",
    ">>> from sklearn.multiclass import OneVsRestClassifier\n",
    ">>> from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.\n",
    ">>> iris = datasets.load_iris()\n",
    ">>> X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clustering with Graph propagation extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i think we can try this. since our data set is not very big. we could use our initial clusters as a feature and retrain \n",
    "# the classifier \n",
    "\n",
    "# https://arxiv.org/pdf/1709.05634.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
