{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import collections, re, string, glob, collections, pickle, os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import dump_svmlight_file, load_svmlight_file\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn import metrics\n",
    "from scipy.sparse import hstack\n",
    "from imblearn import over_sampling\n",
    "\n",
    "import utils_vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO\n",
    "# 4. lemma subset by POS tags - probably NMF\n",
    "# 3. imbalearn upsampling \n",
    "# 4. check \n",
    "\n",
    "# Probably less useful\n",
    "# 5. tuning TFIDF levels \n",
    "# 1. ngrams - since we are working at sentence-based level (instead of doc-based)\n",
    "\n",
    "## Bells and whistles\n",
    "# 1. pipeline to save models, featuresets and results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial results\n",
    "|Run   \t|Classifier   \t|Featureset   \t|train acc   \t|test acc   \t|alpha/C   \t|\n",
    "|---\t|---\t|---\t|---\t|---\t|---\t|\n",
    "|1   \t|M_NB   \t|subj, lemmas, parse_VP, book_ners   \t|98.67   \t|28.71   \t|0   \t|\n",
    "|2   \t|LogReg, OVR, L2   \t|subj, lemmas, parse_VP, book_ners   \t|0.84   \t|0.38   \t|1   \t|\n",
    "|3   \t|LogReg, OVR, L2, with TSVD5000   \t|subj, lemmas, parse_VP, book_ners   \t|0.77   \t|0.36   \t|1   \t|\n",
    "|4   \t|LogReg, multin, L1   \t|subj, lemmas, parse_VP, book_ners   \t|very poor   \t|very poor   \t|0.5   \t|\n",
    "|4   \t|LogReg, OVR, L1, with MaxAbsScaler   \t|subj, lemmas, parse_VP, book_ners   \t|0.99  \t|0.39   \t|1   \t|\n",
    "|5   \t|LogRegCV, OVR, L2   \t|subj, lemmas, parse_VP, book_ners   \t|   \t|   \t|   \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross_val_score on the train set using Logistic Regression is not good at all (feature set is with entire set of lemmas. Propose to add the subset of the lemmas based on their POS tags. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create training and evaluation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Corollary: Computing a subjectivity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have access to a set of [subjectivity classification for about 8,200 English words](https://gitlab.inria.fr/mastertal/UE803/blob/master/data/l10_data/subjectivity_clues_hltemnlp05/subjclueslen1-HLTEMNLP05.README). We hypothesize that an average subjectivity score can help to further distinguish between authors. \n",
    "Our datapoints are however single sentences from at least 3 books of an author, and it is not sensible to compute the score for every sentence as there can be significant variability for such a score between an author's ouevre. Instead we compute the score based on the entire set of the author's sentences admitted into our corpus. \n",
    "\n",
    "We hypothesize that ideally, it will be better to compute the score based on each set of sentences from a single book of one author. This could provide a better predictor of an author's identity, by capturing a certain understanding about the subjectivity, and variability of the subjectivity, of his/her body of work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../2_Preprocessing/supportdata/SubjectivityClues/data_l10_data_subjectivity_clues_hltemnlp05_subjclueslen1-HLTEMNLP05.tff\", \n",
    "          \"r\") as file:\n",
    "    subjectivityclues_all = file.read()\n",
    "    \n",
    "subj_scores = [1 if i == \"type=weaksubj\" else 2 for i in re.findall(r\"type=[a-z]+\", subjectivityclues_all)]\n",
    "# encoding 1 and 2, since the classes are ordinal - \"weaksubj\" and \"strongsubj\"\n",
    "subj_words = [i.lstrip(\"word1=\") for i in re.findall(r\"word1=[a-z]+\", subjectivityclues_all)]\n",
    "subj_pos = [i.lstrip(\"pos1=\") for i in re.findall(r\"pos1=[a-z]+\", subjectivityclues_all)]\n",
    "# side note on pos and stemming: the dataset does not use the UD POS tags, so we cannot map the tokens more precisely\n",
    "# based on the POS tag we have for the corpus tokens. also, the Wilson et al 2005 data include whether the words were \n",
    "# stemmed (i.e. usable on our lemmas sets), but it numbers only 1620, out of 8222 words, or less than 20%. so we \n",
    "# should compute our subjectivity score using our tokens set instead of lemmas\n",
    "\n",
    "subj_trip_info = [i for i in zip(subj_scores, subj_pos)]\n",
    "__dictcombined = {subj_words[index]: subj_trip_info[index] for index in range(len(subj_words))}\n",
    "# there are 5 words that have zero length and it is throwing an error in the filtering.\n",
    "# also pointless to have a score for an empty string in the computation of the subjectivity score. \n",
    "# it might have come about at the re.findall level, but the empty word issue still persists\n",
    "# after adding A-Z and 1-9 to the search pattern. \n",
    "__dictcombined_filter = {i: __dictcombined[i] for i in __dictcombined if len(i) > 0}\n",
    "\n",
    "# sort into a dictionary to speed search time \n",
    "subj_dict = {}\n",
    "for letter in string.ascii_lowercase: \n",
    "    subj_dict[letter] = {key:__dictcombined_filter[key] for key in __dictcombined_filter if key[0] == letter}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectivity(spacysentdoc, subj_dict):\n",
    "    '''given a spacysentdoc, or a np array of strings (tokens of a sentence), returns the subjectivity\n",
    "    score for the sentence. The score is the sum of the score for tokens that can be \n",
    "    found in the http://crr.ugent.be/archives/1330 dataset. \n",
    "    Inputs: spacysentdoc\n",
    "    Outputs: a tuple containing the function name and the result. result is an integer rounded to 3 dp \n",
    "    and converted to a string\n",
    "    ''' \n",
    "    if type(spacysentdoc) == np.ndarray: \n",
    "        s_score = 0\n",
    "        for sentence in spacysentdoc: \n",
    "            for token in sentence:\n",
    "                try:\n",
    "                    # using first letter of token, go to sub-dict of subj_dict\n",
    "                    # if the token is one of the values of the sub-dict\n",
    "                    # retrive the subjectivity score\n",
    "                    if token in subj_dict[token[0]].keys():\n",
    "                        s_score += int(subj_dict[token[0]][token][0])\n",
    "                except: \n",
    "                    pass\n",
    "        # the score is averaged across the number of tokens across the entire corpus/doc\n",
    "        # it gives an indication of the usage of subjectivity words in writing style\n",
    "        return \"subjectivity\", round(s_score/len(spacysentdoc), 3)\n",
    "    else: \n",
    "        # condition for code extension to cover spacysentdoc (for alignment to insert into utils_tokeniser.py)\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Importing pickled DFs, train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__General approach__\n",
    "\n",
    "1. iteratively load author pickled dfs, filter out sentences that are less than 10 tokens long (between the 25th and 50th percentile for sentence length across the entire corpus of all authors)\n",
    "    - (since our dataset is built on one sentence per author, we drop sentences to avoid the possibility of an extremely sparse matrix. the short sentences would provide limited linguistics indicators (expressed through word freqency counts, ) to distinguish between authors. \n",
    "    - plus we already have sent_length and a high-level overview of a sentence's constituency structure as features\n",
    "    - add author wiki abstract NERs\n",
    "2. cut into train test split on each load (to ensure each author is evenly represented in the train and test sets)\n",
    "3. concat train at end, concat test at end\n",
    "4. fit tfidf on train (so as not to leak test data into train), transform on train and test\n",
    "5. cut into sets of features\n",
    "6. export to svm light "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data integrity checkpoint__\n",
    "\n",
    "We could not complete preprocessing for 3 authors admitted into the corpus by Phase 1. An issue with the contents of their sentences is causing the constituency parser to break. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Preliminaries #####\n",
    "# 0. grab all the pickled df files\n",
    "df_filenames = glob.glob(\"../2_Preprocessing/processeddata/df_pickles/*.pickle\")\n",
    "len(df_filenames)\n",
    "\n",
    "# 1a. lists for collecting all the df objects to be generated for pd.concat - speediest approach \n",
    "# https://stackoverflow.com/questions/10715965/add-one-row-to-pandas-dataframe/24913075#24913075\n",
    "dfs_list_train = [] \n",
    "dfs_list_test = []\n",
    "\n",
    "\n",
    "# 1b. store author-label mapping\n",
    "label2auth_map = {}\n",
    "auth2label_map = {}\n",
    "\n",
    "\n",
    "##### Processing #####\n",
    "# 2. generate the master DF\n",
    "label_counter = 1\n",
    "for filename in df_filenames: \n",
    "    __ = pd.read_pickle(filename)\n",
    "     \n",
    "    authornum = __[\"authornum\"][0]                             # get the authornum for this df   \n",
    "                                \n",
    "    label2auth_map[label_counter] = authornum                  # switch the authornum to int, store the mapping\n",
    "    auth2label_map[authornum] = label_counter\n",
    "    __[\"authornum\"] = label_counter\n",
    "    \n",
    "    __ = __[__[\"sent_length\"]>10]                               # filter out short sentences\n",
    "    __[\"concreteness\"] = __[\"concreteness\"].astype(float)       # we set the concreteness score as a string \n",
    "                                                                # to run the process_one_author functions easily\n",
    "                                                                # switch back to int here\n",
    "    # get the subjectivity score for the author. \n",
    "    s_score = get_subjectivity(__[\"tokens\"].values,subj_dict)[1]\n",
    "    # add as a new column to this df\n",
    "    __[\"subjectivity\"]=s_score\n",
    "    \n",
    "\n",
    "    # train test split \n",
    "    __train, __test = train_test_split(__, test_size=0.20)\n",
    "    dfs_list_train.append(__train)\n",
    "    dfs_list_test.append(__test)\n",
    "    \n",
    "    label_counter += 1\n",
    "    \n",
    "allauthor_df_train = pd.concat(dfs_list_train, axis=0)\n",
    "allauthor_df_test = pd.concat(dfs_list_test, axis=0)\n",
    "allauthor_df_train.reset_index(drop= True, inplace=True)\n",
    "allauthor_df_test.reset_index(drop= True, inplace=True)\n",
    "\n",
    "print(\"train size: {}, test size: {}\".format(allauthor_df_train.shape, allauthor_df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview stats\n",
    "allauthor_df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check object types - ints/floats, categoricals etc\n",
    "allauthor_df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 2)\n",
    "# let's take a look at the population distribution, to see if we need to finetune the corpus or augment datapoints\n",
    "sns.distplot(allauthor_df_train.groupby(\"authornum\")[\"authornum\"].count())\n",
    "plt.show()\n",
    "# distribution of mean subjectivity score for each author's sentences  \n",
    "sns.distplot(allauthor_df_train.groupby([\"authornum\",\"subjectivity\"])[\"subjectivity\"].mean(), bins=20)\n",
    "plt.show()\n",
    "# distribution of mean concreteness score for each author's sentences  \n",
    "sns.distplot(allauthor_df_train.groupby([\"authornum\",\"concreteness\"])[\"concreteness\"].mean(), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1d. Vectorising lemmas and NERs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our custom vectoriser in utils_vectoriser.py is not appropriate for this task. It implements [tf-max]( https://nlp.stanford.edu/IR-book/html/htmledition/maximum-tf-normalization-1.html) which is useful when working with a set that has document level data. Our current tf-max code set-up is not useful for our featureset structure here (we have no way to normalise over the docset (a book of an author's)). Instead, we use sklearn's TfidTransformer with DictVectoriser, since we already have a set of tokens and lemmas from our earlier custom preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__features = {\"lemmas\": \n",
    "                  {\"train\": allauthor_df_train[\"lemmas\"],\n",
    "                    \"test\": allauthor_df_test[\"lemmas\"],},\n",
    "              \"namedentities\": \n",
    "                  {\"train\": allauthor_df_train['ne_dates']+allauthor_df_train['ne_places'],\n",
    "                   \"test\": allauthor_df_test['ne_dates']+allauthor_df_test['ne_places'],}, }\n",
    "                # add the ne_dates and ne_places columns for the vectoriser to manage sparsity. leave out \n",
    "                # persons for now, it is possible person names can significantly identify authors (except \n",
    "                # perhaps authors) \n",
    "                          \n",
    "for feat in __features:\n",
    "    # train data - generate DictCounts\n",
    "    token_Counters_train = []\n",
    "    for row in __features[feat][\"train\"].index:\n",
    "        token_Counters_train.append(collections.Counter(__features[feat][\"train\"][row]))\n",
    "    \n",
    "    # test data - generate DictCounts\n",
    "    token_Counters_test = []\n",
    "    for row in __features[feat][\"test\"].index:\n",
    "        token_Counters_test.append(collections.Counter(__features[feat][\"test\"][row]))\n",
    "    \n",
    "    # transforming test set based on the train set fit \n",
    "    # pass the DictCounts into DictVectorizer() \n",
    "    Dvectoriser = DictVectorizer()\n",
    "    Dvect_result_train = Dvectoriser.fit_transform(token_Counters_train)\n",
    "    Dvect_result_test = Dvectoriser.transform(token_Counters_test)\n",
    "\n",
    "    Ttransformer = TfidfTransformer()\n",
    "    Ttrans_results_train = Ttransformer.fit_transform(Dvect_result_train)\n",
    "    Ttrans_results_test = Ttransformer.transform(Dvect_result_test)\n",
    "    \n",
    "    # write to SVM\n",
    "    dump_svmlight_file(Ttrans_results_train, allauthor_df_train[\"authornum\"], \"data/train_\"+feat+\".svmlight\")\n",
    "    dump_svmlight_file(Ttrans_results_test, allauthor_df_test[\"authornum\"], \"data/test_\"+feat+\".svmlight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__other_features=['concreteness', 'subjectivity', \"parse_VP\"]\n",
    "for feat in __other_features:\n",
    "    # .reshapes since we are working with single feature sets each time \n",
    "    dump_svmlight_file(allauthor_df_train[feat].values.reshape(-1,1),\n",
    "                       allauthor_df_train[\"authornum\"], \"data/train_\"+feat+\".svmlight\")\n",
    "    dump_svmlight_file(allauthor_df_test[feat].values.reshape(-1,1), \n",
    "                       allauthor_df_test[\"authornum\"], \"data/test_\"+feat+\".svmlight\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allauthor_df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Large multiclass classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path = \"./data\", features='lemmas'):\n",
    "    '''\n",
    "    '''\n",
    "    X_train, y_train = load_svmlight_file( data_path+'/train_'+features+\".svmlight\" ) \n",
    "    X_test, y_test = load_svmlight_file( data_path+'/test_'+features+\".svmlight\", n_features=X_train.shape[1] ) \n",
    "    # shape of test shouldn't matter here since we enforced it at the vectorising step above\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack( A, B ):\n",
    "    return hstack( [A,B] )\n",
    "\n",
    "X_tr_lemmas, y_train, X_te_lemmas, y_test = load_data( \"./data\", features='lemmas')\n",
    "X_tr_concreteness, y_train, X_te_concreteness, y_test = load_data( \"./data\", features='concreteness')\n",
    "X_tr_namedentities, y_train, X_te_namedentities, y_test = load_data( \"./data\", features='namedentities')\n",
    "X_tr_parse_VP, y_train, X_te_parse_VP, y_test = load_data( \"./data\", features='parse_VP')\n",
    "X_tr_subjectivity, y_train, X_te_subjectivity, y_test = load_data( \"./data\", features='subjectivity')\n",
    "\n",
    "X_train = hstack( [X_tr_subjectivity, X_tr_lemmas,X_tr_namedentities, X_tr_parse_VP] )\n",
    "X_test = hstack( [X_te_subjectivity, X_te_lemmas, X_te_namedentities, X_te_parse_VP] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13837, 26502)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3518, 26502)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/k1000mbp/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1a0186d8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The distribution of lables in the train set is a little uneven - we could try SMOTE upsampling (since we don't have \n",
    "# a very large dataset, so downsampling could worsen the learning)\n",
    "sns.distplot(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A. Dimensionality reduction to explore the characteristics of the data.__\n",
    "\n",
    "We have a very sparse matrix of about 25,000 features. This is confusing the learning for the classifier, let's reduce dimensions to 5000, instead of 25,000, to see it's impact.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale with MaxAbsScaler, to retain sparsity strcuture. Hopefully it will speed up computation. Would allow us to include \n",
    "# concreteness score (which has negative values) in the featureset, but need to investigate (i) sensibility of our \n",
    "# earlier centering of the concreteness score about -2.5 and 2.5. And if so, does MaxAbsScale entirely negate that? \n",
    "\n",
    "mas = MaxAbsScaler()\n",
    "mas.fit(X_train)\n",
    "X_train_SS = mas.transform(X_train)\n",
    "X_test_SS = mas.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_components in [5,1000, 5000]:\n",
    "    TSVD_model = TruncatedSVD(n_components=n_components, random_state=1,)\n",
    "    X_train_TSVD_model = TSVD_model.fit(X_train)\n",
    "    print(\"The explained variance for PCA with {} is {}\".format(n_components, sum(TSVD_model.explained_variance_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_TSVD=X_train_TSVD_model.transform(X_train)\n",
    "X_test_TSVD=X_train_TSVD_model.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A1. Results for [X_tr_subjectivity, X_tr_lemmas,X_tr_namedentities, X_tr_parse_VP]__\n",
    "\n",
    "The explained variance for TruncatedSVD with 5 is 17.62470506672777\n",
    "\n",
    "The explained variance for TruncatedSVD with 1000 is 18.178797730372374\n",
    "\n",
    "The explained variance for TruncatedSVD with 5000 is 18.585553228841114\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B. Attempt at NMF to ensure non-negative values (which PCA allows)__\n",
    "\n",
    "Post-PCA, our train set contains negative values and MultinomialNB does not accept that (although, fine for LogReg since it's a linear model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5000\n",
    "nmf_model = NMF(n_components=n_components, init=\"nndsvdar\", random_state=1, beta_loss=\"frobenius\",\n",
    "          alpha=.5, l1_ratio=.5, ) \n",
    "# We hypothesise that there are a lot of unnnecessary features, so we start by setting \n",
    "# a higher alpha for stronger regularisation (drive the weight of the unnecessary features to zero).\n",
    "# We experiment with a 0.5 L1-L2 regularisation ratio. \n",
    "# nndsvdar for faster computation: too little RAM on our PCs :(\n",
    "\n",
    "X_train_NMF = nmf_model.fit_transform(X_train)\n",
    "X_test_NMF = nmf_model.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Preliminaries: LogReg baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel= LogisticRegression()\n",
    "logmodel.fit(X_train_SS, y_train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.9904603599046036\n"
     ]
    }
   ],
   "source": [
    "print(\"train acc:\", metrics.accuracy_score( y_train, logmodel.predict(X_train_SS) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.39511085844229676\n"
     ]
    }
   ],
   "source": [
    "print(\"test acc:\", metrics.accuracy_score( y_test, logmodel.predict(X_test_SS) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/k1000mbp/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/k1000mbp/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/k1000mbp/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.35103181, 0.34049013, 0.34280717])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logmodel, X_train_SS, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clf( X, y, classifier='m_nb', tune_param = 1.0 ):\n",
    "    if classifier== 'm_nb':\n",
    "        classifier = MultinomialNB(alpha=tune_param, fit_prior=True, class_prior=None) \n",
    "        # Train \n",
    "        classifier.fit( X, y )\n",
    "    elif classifier== 'logreg':\n",
    "        classifier = LogisticRegression(C = tune_param, solver=\"saga\", \n",
    "                                        penalty='l2', multi_class=\"multinomial\") \n",
    "        # used l1 regularisation to drive weights of irrelevant features to zero (improve learning)\n",
    "        # saga solver chosen because it allows L1 reg as well as handle multinomial loss. bonus: allows \n",
    "        # parallel computation (but not relevant here since we are trying to use multinomial loss to improve \n",
    "        # the performance of the classifier over an OVR approach)\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
    "        \n",
    "        # Train \n",
    "        classifier.fit( X, y )\n",
    "    return classifier\n",
    "\n",
    "def evaluate( classifier, X, y ):\n",
    "    # Predict\n",
    "    y_pred = classifier.predict( X )\n",
    "    return y_pred \n",
    "\n",
    "def compute_acc(y_true, y_pred):\n",
    "    return metrics.accuracy_score( y_true, y_pred )\n",
    "\n",
    "def tune_clf(  X_train, y_train, X_test, y_test, classifier='m_nb' ):\n",
    "    train_acc, test_acc, cvs tune_params, clf_name = [],[], [], [], classifier\n",
    "    if classifier=='m_nb': \n",
    "        for alpha in np.linspace(0,1,11):\n",
    "            alpha = round(alpha, 1)\n",
    "            tune_params.append(alpha)\n",
    "            clf = train_clf( X_train, y_train, classifier=classifier, tune_param=alpha )\n",
    "            #print(clf)\n",
    "            y_pred_train = evaluate( clf, X_train, y_train )\n",
    "            tr_acc = compute_acc(y_train, y_pred_train)\n",
    "            train_acc.append( tr_acc )\n",
    "\n",
    "            y_pred_test = evaluate( clf, X_test, y_test )\n",
    "            d_acc = compute_acc(y_test, y_pred_test)\n",
    "            test_acc.append( d_acc )\n",
    "            print( alpha, round( tr_acc*100, 3), round( d_acc*100, 3) )\n",
    "    \n",
    "    elif classifier== 'logreg': \n",
    "        for C in np.logspace(-5, 1, 11):\n",
    "            tune_params.append(C)\n",
    "            clf = train_clf( X_train, y_train, classifier=classifier, tune_param=C )\n",
    "            y_pred_train = evaluate( clf, X_train, y_train )\n",
    "            tr_acc = compute_acc(y_train, y_pred_train)\n",
    "            train_acc.append( tr_acc )\n",
    "\n",
    "            y_pred_test = evaluate( clf, X_test, y_test )\n",
    "            d_acc = compute_acc(y_test, y_pred_test)\n",
    "            test_acc.append( d_acc )\n",
    "            print( C, round( tr_acc*100, 3), round( d_acc*100, 3) )\n",
    "    return train_acc, test_acc, tune_params, clf\n",
    "\n",
    "def plot_tune_clf( train_acc, test_acc, tune_params, clf_name):    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0.1,0.2,0.8,0.9] )\n",
    "    ax.plot( tune_params, train_acc, 'blue', label='train' )\n",
    "    ax.plot( tune_params, test_acc, 'red', label='test' )\n",
    "    plt.title(clf_name)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2c. Running the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/k1000mbp/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 98.967 28.709\n",
      "0.1 42.61 11.967\n",
      "0.2 19.737 6.566\n",
      "0.3 11.946 4.69\n",
      "0.4 8.152 3.411\n",
      "0.5 6.41 2.843\n",
      "0.6 5.196 2.644\n",
      "0.7 4.459 2.445\n",
      "0.8 3.801 2.132\n",
      "0.9 3.389 1.99\n",
      "1.0 3.071 1.876\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc_uni, test_acc_uni, tune_param, clf_name = tune_clf(X_train, y_train, X_test, y_test, classifier=\"m_nb\" )\n",
    "plot_tune_clf( train_acc_uni, test_acc_uni, tune_param, clf_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/k1000mbp/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 1.315 1.308\n",
      "3.9810717055349695e-05 1.59 1.535\n",
      "0.00015848931924611142 1.698 1.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/k1000mbp/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000630957344480193 1.612 1.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/k1000mbp/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0025118864315095794 1.662 1.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/k1000mbp/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 1.807 1.649\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-3325057d75ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_acc_uni\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc_uni\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"logreg\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_tune_clf\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_acc_uni\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc_uni\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_name\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-135be34c2c68>\u001b[0m in \u001b[0;36mtune_clf\u001b[0;34m(X_train, y_train, X_test, y_test, classifier)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mC\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mtune_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_clf\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-135be34c2c68>\u001b[0m in \u001b[0;36mtrain_clf\u001b[0;34m(X, y, classifier, tune_param)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1358\u001b[0m                       \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1360\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mlogistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight)\u001b[0m\n\u001b[1;32m    790\u001b[0m                 \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start_sag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m                 is_saga=(solver == 'saga'))\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py\u001b[0m in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    329\u001b[0m                             \u001b[0mintercept_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                             \u001b[0mis_saga\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                             verbose)\n\u001b[0m\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         warnings.warn(\"The max_iter was reached which means \"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc_uni, test_acc_uni, tune_param, clf_name = tune_clf(X_train, y_train, X_test, y_test, classifier=\"logreg\" )\n",
    "plot_tune_clf( train_acc_uni, test_acc_uni, tune_param, clf_name )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve author literary movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_authorbook_df = utils_loaddataframe.loaddataframe()\n",
    "corpus_authorbook_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.00000000e+01-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
