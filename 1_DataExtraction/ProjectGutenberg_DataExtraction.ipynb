{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SXRp4LcnC4I"
   },
   "source": [
    "# Extracting and storing RDF information and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preliminaries: dependencies and seed state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7wQupQrnC4J"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipedia\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import csv, datetime, time, random, string, re, urllib, os, collections  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from urllib.error import HTTPError\n",
    "import pickle \n",
    "with open('randomstate.pickle', 'rb') as f:\n",
    "    random.setstate(pickle.load(f)) \n",
    "# We use random sampling in some of the functions of our program. \n",
    "# To ensure we can replicate the same dataset, we include the use\n",
    "# of the same seed state whenever running this corpus builder.\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from bson import Binary, Code\n",
    "from bson.json_util import dumps,loads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIVWTPMRexID"
   },
   "source": [
    "### General overview\n",
    "\n",
    "Our solution is supported via two classes, their attributes and associated functions. The first class - GutenbergCorpusBuilder - is intended to handle author-title mining on the Project Gutenberg website, filtering and selection as well as storage of overall corpus data. The attributes of this class is designed for easy ingestion into a mongoDB, or similar non-relational, database. The other class - Author - is intended to hold the processes for accessing text files for books, processing them and storing them. The Author class also contains methods for collecting \n",
    "\n",
    "To build a corpus, a user will only need to interact with 2 methods from the GutenbergCorpusBuilder. These are namely, in the order of intended use: (i) get_library; and (ii) populate_corpus. \n",
    "\n",
    "The first method - __get_library__ - will crawl all of the 'Browse by Author' pages on the Project Gutenberg website and collect author information (including books he/she authored as well as wikipedia pages). By passing the 'min_book', 'max_book' as well as 'languages' and 'roles' parameters, the user can balance the content of the corpus in terms of author and book numbers, as well as have it filtered based on language(s) and author role(s). We note in particular, that the author role setting could be become significant for certain machine learning tasks (for instance incorporating books where an author is merely an editor or contributor could lead to degraded model performance for an author-genre classification task). \n",
    "\n",
    "The default values for this function, as well as our setting for the corpus generated are as follows: \n",
    "\n",
    "|Parameter\t|Default setting\t|Setting for this corpus\t|\n",
    "|---\t|---\t|---\t|\n",
    "|min_books   \t|1   \t|3   \t|\n",
    "|max_books   \t|float(inf)   \t|30   \t|\n",
    "|languages   \t|'all'   \t|['english']   \t|\n",
    "|roles   \t|'all'   \t|['as author']   \t|\n",
    "\n",
    "The parameters, their types and default settings are designed with the intention to allow the collection of all books available on Project Gutenberg. For the purpose of our collected corpus, we have chosen the parameters so as to obtain a balanced corpus that selects major as well as minor authors. \n",
    "\n",
    "\n",
    "The second method - __populate_corpus__ - will take the pre-filtered list of author and their books, instantiate a Author object, and begin collecting carefully data on the author and his/her books. At the background, the function will first retrieve all of the literary movements the author is associated with, from DBpedia. The primary information bottleneck lies with these literary movement labels - not all authors have DBpedia pages and for those that do, many do not have literary movements associated to them. As such, we only proceed with the next steps of adding an author to the final corpus if he/she has these literary movement labels. For authors that pass this filter, the method proceeds to extracts and stores the multilingual abstracts for the author. Finally, it proceeds to randomly pick a set of the author's book's (the size of this set is the same for all authors and equivalent to the min_books set for the corpus), and clean and segment the text files in a list of sentences. The cleaning is intended to exclude boilerplate Project Gutenberg metadata as well as book publisher information. An option is provided to clean the text files by exclude the Project Gutenberg metadata more precisely, albeit this extends the time required to process and collect the corpus.  \n",
    "\n",
    "The default values for this function, as well as our setting for the corpus generated are as follows: \n",
    "\n",
    "|Parameter\t|Default setting\t|Setting for this corpus\t|\n",
    "|---\t|---\t|---\t|\n",
    "|sent_num   \t|250   \t|250   \t|\n",
    "|precise_clean   \t|False   \t|True   \t|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUOWBinlnC4Z"
   },
   "source": [
    "### 1. A class to store a corpus obtained from the Project Gutenberg website. \n",
    "\n",
    "The corpus is build with functions within the class that filter the authors and books on the Project Gutenberg website. It also calls on the Author class (below), to process and generate information about sentences from an author's books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfplnCWJnC4Z"
   },
   "outputs": [],
   "source": [
    "class GutenbergCorpusBuilder: \n",
    "    '''\n",
    "    initiates a GutenbergCorpusBuilder object which stores information about selected authors that are found on \n",
    "    the Project Gutenberg(PG) website.\n",
    "    Authors are stored based on their unique PG numerical code. For each author, selected books and their \n",
    "    respective PG URL are stored.\n",
    "    \n",
    "    Inputs: corpusname - string representing name of the corpus being created.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, corpusname):\n",
    "        self.corpusname = corpusname\n",
    "        self.corpusversion = \"v\"+ str(datetime.datetime.now().year) + str(datetime.datetime.now().month) +\\\n",
    "        str(datetime.datetime.now().day)\n",
    "        \n",
    "        # a dictionary containing dictionaries.\n",
    "        # The top level keys - unique numbers for authors on the PG website,\n",
    "        # the values -  dictionaries containing author information: \n",
    "        # keys - 'authorname', 'books_info'; wiki_info'; \n",
    "        # values - strings or embedded dictionaries:\n",
    "        # authorname: string with extracted name;\n",
    "        # books_info: dictionary: key - book ID, value - book title;\n",
    "        # wiki_info: dictionary: key - language; value - wiki link extracted from PG\n",
    "        #\n",
    "        # {author ID: {authorname:'name', books_info:{bookID: book title}, wiki_info: {language: wiki link}}}\n",
    "        self.authors = dict()\n",
    "       \n",
    "        # a dictionary containing sets of sentences selected from each author's filtered books;\n",
    "        # the top level keys are the unique numbers for authors, the values are sets containing\n",
    "        # sentences from an author's book (as strings).\n",
    "        # {author ID: Author()}\n",
    "        self.corpus = dict()\n",
    "        \n",
    "        self.min_books = int # the min_books value passed into the get_library method.\n",
    "        self.max_books = int # the max_books value passed into the get_library method.\n",
    "        \n",
    "    def populate_corpus(self, sent_num=250, precise_clean=False):\n",
    "        '''\n",
    "        For each author in self.authors, generates an Author() class instance, populates all \n",
    "        attributes of the Author() class, adds to self.corpus.\n",
    "        \n",
    "        Inputs | sent_num: int - the total number of sentences to collect for a single author selected for the corpus. \n",
    "        '''\n",
    "        if len(self.authors) > 0:\n",
    "            _counter = 0\n",
    "            for authornum in self.authors: # authornum is a key (an author's unique number)\n",
    "                if authornum not in self.corpus.keys():\n",
    "                    # instantiate an Author()\n",
    "                    authorname = self.authors[authornum][\"authorname\"]\n",
    "                    authorwiki_info = self.authors[authornum][\"wiki_info\"].copy() \n",
    "                    authorbooks_info_keys = list(self.authors[authornum][\"books_info\"].copy().keys())\n",
    "                    \n",
    "                    _author = Author(authorname=authorname, authornum=authornum, \n",
    "                                    min_books=self.min_books)\n",
    "                    \n",
    "                    # run the populate_attributes() to extract and process the information for the author\n",
    "                    _author.populate_attributes(authorwiki_info=authorwiki_info, \n",
    "                                                authorbooks_info_keys=authorbooks_info_keys,\n",
    "                                                sent_num=sent_num, precise_clean=precise_clean)\n",
    "                    \n",
    "                    # store to Author() to corpus \n",
    "                    self.corpus[authornum] = _author\n",
    "\n",
    "                    # append wiki abstract info and literary movement to author's dictionary in \n",
    "                    # self.author so as to easily transmit each author's basic information into mongodb\n",
    "                    self.authors[authornum][\"authorabstracts\"] = _author.authorabstracts\n",
    "                    self.authors[authornum][\"literarymovements\"] = _author.literarymovements\n",
    "                _counter += 1\n",
    "                if _counter%100 == 0:\n",
    "                    print(\"{} authors have been processed, out of {} authors in selections\".format(_counter, len(self.authors)))\n",
    "        else: \n",
    "            print(\"The authors attribute is empty, please run get_library first or \\\n",
    "            check the parameters passed into get_library.\")\n",
    "        \n",
    "    \n",
    "    def get_library(self, min_books=1, max_books=float(\"inf\"), languages = \"all\", roles = \"all\"):\n",
    "        '''\n",
    "        Goes through the PG website's 'sort by author' pages. Extracts author and corresponding book \n",
    "        information that meet a number of selection criterion (see inputs). \n",
    "        \n",
    "        Inputs | \n",
    "            1. min_books: int - the minimum number of books available for an author, which meets the languages \n",
    "            and roles parameters. Default value is 1. \n",
    "            2. languages: either a str \"all\", or a list containing the languages (in lowercase) to count towards \n",
    "            the author's min_books level. The list of languages available can be found here \n",
    "            https://www.gutenberg.org/catalog/. Default is \"all\". \n",
    "            3. roles: either a str \"all\", or a list containing the roles that an author can have in a book. \n",
    "            These include: Commentator, Translator, Contributor, Photographer, Illustrator, Editor.\n",
    "            Default value is \"all\".\n",
    "        Outputs | saves the results to self.authors\n",
    "        '''\n",
    "        charlist = []\n",
    "        charlist[:0] =  [letter for letter in string.ascii_lowercase] + [\"other\"]\n",
    "\n",
    "        library = dict()\n",
    "        for char in charlist:\n",
    "            # Team comment: we select the authors and books via the \"Browse by Author\" lists instead of  \n",
    "            # the \"Browse by Books\" list. Although the latter has a more predictable page structure \n",
    "            # (i.e. 1 book name, followed by 1 author name, recursively), the former includes \n",
    "            # information about the Author's role in the book. We believe that this could have\n",
    "            # a meaningful impact on the predictive capabilities for models on different tasks, \n",
    "            # especially at larger scale.\n",
    "            \n",
    "            link = 'https://www.gutenberg.org/browse/authors/'+ char\n",
    "            page = requests.get(link)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            one_letter = self._unite_authors_nums_books(self._get_authors_numsnames(soup)[0],\\\n",
    "                                                            self._get_authors_numsnames(soup)[1],\\\n",
    "                                                            self._get_bookswiki_info(soup)[0],\\\n",
    "                                                            self._get_bookswiki_info(soup)[1],\\\n",
    "                                                            min_books, max_books, languages, roles)\n",
    "            \n",
    "            library.update(one_letter)\n",
    "            print(\"{} authors from the '{}' alphabetical category have been added.\".format(len(one_letter),char))\n",
    "            \n",
    "            # del variable to clear memory\n",
    "            del soup\n",
    "            \n",
    "            # Put the function to sleep for a randomised number of seconds (non-integer number between \n",
    "            # 0.5 and 4) to mimic human surfing patterns.\n",
    "            time.sleep(random.uniform(0.5,4))\n",
    "            \n",
    "        self.authors = library\n",
    "        self.min_books = min_books\n",
    "        self.max_books = max_books\n",
    "    \n",
    "    def _get_authors_numsnames(self, soup):\n",
    "        '''\n",
    "        A helper function for _unite_authors_nums_books. Extracts all author names from a BeautifulSoup \n",
    "        copy  of a 'Browse by Author' page on the PG website. \n",
    "        \n",
    "        Inputs | soup:a BeautifulSoup object - containg a copy of the PG 'Browse by Author' page. \n",
    "        Outputs | a tuple containing two lists: The first contains author's numbers on the page, the \n",
    "        second contains corresponding author's names on the page. \n",
    "        '''\n",
    "        authornames = []\n",
    "        # the author names are stored within the \"name\" attribute under each \"a\" class\n",
    "        # use regex wildcard so that find_all will catch and return all \"a names\" with values\n",
    "        authorname_BSlist = soup.find_all('a', {\"name\":re.compile(\"\\w*\")})\n",
    "\n",
    "        for authorname in authorname_BSlist:\n",
    "            # \\- and \\? to escape special characters. .rstrip to remove trailing whitespaces. \n",
    "            authornames.append(re.sub(r'[0-9,\\-\\?]*', '', authorname.text).rstrip())\n",
    "\n",
    "        authornums = []\n",
    "        # the author numbers are stored within the \"href\" attribute. Every line for a book \n",
    "        # on the page has a \"title\" attribute with the value \"Link to this author\". We will use\n",
    "        # this to shift to only the lines with the author's number. \n",
    "        authornums_BSlist = soup.find_all('a', {\"title\":\"Link to this author\"})\n",
    "\n",
    "        for authornum in authornums_BSlist:\n",
    "            authornums.append(authornum[\"href\"].lstrip(\"#\"))\n",
    "\n",
    "        return authornums, authornames\n",
    "\n",
    "    def _get_bookswiki_info(self, soup):\n",
    "        '''\n",
    "        A helper function for _unite_authors_nums_books. Extracts all the book titles and numbers from a \n",
    "        BeautifulSoup copy of a 'Browse by Author' page on the PG website. Also extracts author wikipedia \n",
    "        link information if it is available on the PG website. \n",
    "        \n",
    "        Inputs | soup:a BeautifulSoup object - containg a copy of the PG 'Browse by Author' page. \n",
    "        Outputs | a tuple containing two lists. \n",
    "            1. The first list contains dictionaries. Each dictionary contains information about an author's \n",
    "            books on PG. this includes: book titles, corresponding PG books numbers, the author's role in \n",
    "            each book, and the language of each book. \n",
    "            2. The second list contains dictionaries. Each dictionary contains information about an author's \n",
    "            wikipedia links on PG. An author's wiki dictionary may be empty, contain 1 link, or more than 1 \n",
    "            link. \n",
    "        '''\n",
    "        books_info = list()\n",
    "        wiki_info = list()\n",
    "\n",
    "        # content under the 'ul' tags: books, links as one list organized by ul\n",
    "        authorsbooks_BSlist = soup.find_all('ul')\n",
    "        # for each ul, access the content: books, links; each book is a bs object\n",
    "\n",
    "        for author in authorsbooks_BSlist:\n",
    "            # there are two classes of attributes within each ul tag. The book information\n",
    "            # 1. title and book PG number is under the 'pgdbetext' class. \n",
    "            books_BSlist = author.find_all(class_='pgdbetext')\n",
    "\n",
    "            authorbooks_info = {}\n",
    "            for book in books_BSlist:\n",
    "                # the book numbers are stored in the href attribute. e.g. \"ebooks/19323\"\n",
    "                booknum = book.find('a')['href'].split(\"/\")[-1]\n",
    "                PG_booktitle = book.text\n",
    "\n",
    "                # storing the information regarding a single author's books in a dictionary\n",
    "                authorbooks_info[booknum]=PG_booktitle\n",
    "            \n",
    "            # appending the dictionary containing one author's books to a list\n",
    "            books_info.append(authorbooks_info)\n",
    "            \n",
    "            # 2. for the author is/are under the 'pgdbxlink' class. \n",
    "            wiki_BSlist = author.find_all(class_='pgdbxlink')\n",
    "\n",
    "            authorwiki_info = {}\n",
    "\n",
    "            for wiki in wiki_BSlist:\n",
    "                # 1. the wiki links are stored in the href attribute. \n",
    "                PG_wikilink = wiki.find('a')['href'] # get the whole link\n",
    "                \n",
    "                # some of the lines tagged \"pgdbxlink\" include \"See also: xxx\" links. \n",
    "                # we filter them out here\n",
    "                if \"wikipedia.org\" in PG_wikilink:\n",
    "\n",
    "                    # 2. because PG stores the link in URL-safe format (e.g. \"\\x\" is \"%\"), we will face \n",
    "                    # issues with non-ASCII characters e.g. á whose URL-safe encoding cannot be passed \n",
    "                    # into the wikipedia package. use urllib.requests.unquote to resolve this \n",
    "                    # https://docs.python.org/2/library/urllib.html#utility-functions \n",
    "                    PG_wikilink = urllib.request.unquote(PG_wikilink)\n",
    "\n",
    "                    # 3. get the language code for the wikipage\n",
    "                    wikilang = re.findall(r'/\\w+', PG_wikilink)[0].strip('/')\n",
    "                    # storing the information regarding a single author's wikipedia links in a dictionary\n",
    "                    authorwiki_info[wikilang] = PG_wikilink\n",
    "\n",
    "            # appending the dictionary containing one author's wikipedia links to a list\n",
    "            wiki_info.append(authorwiki_info)\n",
    "            \n",
    "        return books_info, wiki_info\n",
    "\n",
    "    \n",
    "    def _unite_authors_nums_books(self, authornums, authornames, books_info, wiki_info, min_books, \n",
    "                                  max_books, languages, roles):\n",
    "        '''\n",
    "        A helper function for get_library. \n",
    "        \n",
    "        Inputs | \n",
    "            1. authornums:list - list of author numbers from a \"sort by author\" page on the PG website. \n",
    "            2. authornames:list - list of author names  from a \"sort by author\" page on the PG website. \n",
    "            3. books_info: list - a list containing dictionaries, each of which has information about \n",
    "            an author's books \n",
    "            4. wiki_info: list - a list containing dictionaries, each of which has information about \n",
    "            an author's wikipedia\n",
    "            page, as provided by the PG website. There may be none, one, or more wikilinks for an author. \n",
    "            5. min_books:int - the minimum number of books available for an author, which meets the languages \n",
    "            and roles parameters. default value is 1 (since an author listed on PG will have at least 1 book \n",
    "            to his name).\n",
    "            6, max_books:int - the minimum number of books available for an author, which meets the languages \n",
    "            and roles parameters. default value is infinity.\n",
    "            7. languages:either a str \"all\", or a list containing the languages (in lowercase) to count towards the author's \n",
    "            min_books level. The list of languages available can be found here \n",
    "            https://www.gutenberg.org/catalog/. default is \"all\". \n",
    "            8. roles: either a str \"all\", or a list containing the roles (in lowercase) that an author can \n",
    "            have in a book. These include: commentator, translator, contributor, photographer, illustrator, \n",
    "            commentator, editor. default value is \"all\".\n",
    "        Outputs | a dictionary containing PG numbers for authors who meet the min_books, languages and \n",
    "        roles requirements, as well as information each of these author's books. \n",
    "        '''\n",
    "        # we want to be sure that the authornums, authornames, books_info, and wiki_info are aligned \n",
    "        # before proceeding to merge them. \n",
    "        try:\n",
    "            assert len(authornums)==len(authornames) and len(authornums)==len(books_info) and len(authornums)==len(wiki_info)\n",
    "        except AssertionError as e:\n",
    "            e.args += (\"The length of authornums, authornames and books_info do not match.\",)\n",
    "            raise\n",
    "            \n",
    "        authorbooks_info = dict()\n",
    "        # if default parameters passed into the function, add all authors and their books to the corpus.  \n",
    "        if min_books == None and languages == \"all\" and roles == \"all\":\n",
    "            for i in range(len(authornums)):\n",
    "                authorbooks_info[authornums[i]]=\\\n",
    "                        {\"authorname\": authornames[i], \"books_info\": books_info[i], \"wiki_info\": wiki_info[i]}\n",
    "        else:\n",
    "            # place languages and roles input in sets, for use in .intersection below. \n",
    "            languages_set = set(languages)\n",
    "            roles_set = set(roles)\n",
    "            \n",
    "            for i in range(len(authornums)):\n",
    "                author_bookset = books_info[i]\n",
    "                _topop = []\n",
    "                for book in author_bookset: \n",
    "                    \n",
    "                    # using regex to find text in parentheses. Book language e.g. (English) and author role \n",
    "                    # e.g. (as Author) are contained in parentheses. Some books which are part of a series, \n",
    "                    # have (of N) in their titles too, where N is the number of books in that series. \n",
    "                    title_text_in_parentheses =\\\n",
    "                    re.findall(r'\\(([a-zA-Z]+\\s*[a-zA-Z]*[0-9]*)\\)', author_bookset[book])\n",
    "                    \n",
    "                    # lowercase the text in parentheses and put it into sets. \n",
    "                    _title_text_in_parentheses =\\\n",
    "                    set([i.lower() for i in title_text_in_parentheses])\n",
    "                    \n",
    "                    # if languages is set to \"all\" or if the intersection of _title_text_in_parentheses\n",
    "                    # and languages_set returns a non-empty set, pass to the next check. Otherwise add this \n",
    "                    # book number to the list of books to pop from this author_bookset\n",
    "                    if languages == \"all\" or _title_text_in_parentheses.intersection(languages_set): pass\n",
    "                    else:\n",
    "                        _topop.append(book) \n",
    "                        continue \n",
    "                    # do the same for author's role as for language above\n",
    "                    if roles == \"all\" or _title_text_in_parentheses.intersection(roles_set): pass\n",
    "                    else:\n",
    "                        _topop.append(book) \n",
    "                        continue    \n",
    "                # pop the books that don't meet the language and role specifications. \n",
    "                for pop in _topop:\n",
    "                    books_info[i].pop(pop)\n",
    "                    \n",
    "                # check if number of books meeting the language and role requirements meet the \n",
    "                # min_book requirement \n",
    "                if min_books <= len(books_info[i]) <= max_books:\n",
    "                    authorbooks_info[authornums[i]]=\\\n",
    "                            {\"authorname\": authornames[i], \"books_info\": books_info[i], \n",
    "                             \"wiki_info\": wiki_info[i]}\n",
    "                    \n",
    "        return authorbooks_info \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"There are {} authors entered in this corpus\".format(len(self.corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1WRLJZBtpZK"
   },
   "source": [
    "### 2. A class to store subcorpora obtained from the Project Gutenberg website for each Author. \n",
    "\n",
    "The subcorpus is build with functions within the class that pre-processes each .txt file for filtered author books on the Project Gutenberg website. It also obtains the abstracts and literary movement tags for each author from Wikipedia and DBPedia respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m96uH1j8mzXl"
   },
   "outputs": [],
   "source": [
    "class Author:\n",
    "    '''\n",
    "    Initiates a Author object which stores information about a selected author that is available on \n",
    "    the Project Gutenberg(PG) website. Other information drawn from (i) DBPedia - author literary movements\n",
    "    (ii) wikipedia - multilingual author abstract, (iii) PG - selected sentences from author's texts \n",
    "    \n",
    "    Inputs: authorname: str, authornum:str, authorwiki_info: dict, authorbooks_info_keys:list of numbers in \n",
    "    string, min_books: int\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, authorname, authornum, min_books):\n",
    "        '''\n",
    "        initiates the Author object with the author's name. \n",
    "        \n",
    "        Inputs | authorname: str, authornum:str, authorwiki_info: dict, \n",
    "        authorbooks_info_keys:list of author numbers (in str), min_books: int\n",
    "        '''\n",
    "        self.name = authorname\n",
    "        self.number = authornum\n",
    "        self.min_books = min_books  # the min_book setting at the GutenbergCorpus class that led\n",
    "                                    # to this author's selection for the corpus\n",
    "        \n",
    "        # a dictionary with the book numbers as keys and lists as values. Lists  \n",
    "        # contain strings that have been pre-processed by the segment_sentence method.\n",
    "        self.processed_subcorpus = dict()       \n",
    "        \n",
    "        self.authorabstracts = dict() \n",
    "        self.literarymovements = list()\n",
    "        \n",
    "        \n",
    "    def populate_attributes(self, authorwiki_info, authorbooks_info_keys, sent_num, precise_clean):\n",
    "        '''\n",
    "        A convenience function to call _build_subcorpus, _get_authorabstract and  _get_literarymovement, \n",
    "        which will respectively populate the processed_subcorpus, authorabstract and literarymovements\n",
    "        attributes for this Author instance.  \n",
    "        \n",
    "        Inputs | authorwiki_info: dict, authorbooks_info_keys:list of numbers (in str), min_books: int\n",
    "        Result | stores results to self.literarymovements, self.authorabstracts and self.processed_subcorpus \n",
    "        '''\n",
    "        # check for /data directory, else create for storing files from _build_subcorpus\n",
    "        if not os.path.isdir('./data'):  \n",
    "            os.mkdir(\"data\")\n",
    "        \n",
    "        self._get_literarymovement(authorwiki_info)\n",
    "        \n",
    "        # the information bottleneck is at the dbpedia literary movement labels.\n",
    "        # multilingual wiki abstract and text processing requires a large amount of resources \n",
    "        # so we only do these for authors that we manage to get literary movements for.\n",
    "        if len(self.literarymovements) > 0: \n",
    "            self._get_authorabstract(authorwiki_info)\n",
    "            self._build_subcorpus(authorbooks_info_keys=authorbooks_info_keys, \n",
    "                                  sent_num=sent_num, precise_clean=precise_clean)\n",
    "        \n",
    "    \n",
    "    def _build_subcorpus(self, authorbooks_info_keys, sent_num, precise_clean):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. Selects the books of an author's to extract sentences\n",
    "        from. the number of books is the same as min_book set for the corpus's author selection criteria.\n",
    "        if an author has more books than min_books, a random sampling is done. a basic pre-processing to \n",
    "        remove PG metadata and publisher information is done next. results are written to two sets of csv files. \n",
    "        the first contains only selected sentences, the second contains the entire processed book. Additionally, \n",
    "        the selected sentences for each book are written to plaintext files. \n",
    "        \n",
    "        Inputs | authorbooks_info_keys:list of numbers (in str), min_books: int\n",
    "        Result | saves to two sets of csv files: (i) selected sentences only; (ii) entire pre-processed \n",
    "        book. selected sentences also saved to plaintext files. also stores (i) to self.processed_subcorpus\n",
    "        '''\n",
    "        \n",
    "        _author_cleanbooks = dict()\n",
    "        \n",
    "        if len(authorbooks_info_keys) == self.min_books: \n",
    "            for booknum in authorbooks_info_keys: \n",
    "                all_sentencesinbook =\\\n",
    "                self._cleansegment_book(booknum=booknum, precise_clean=precise_clean)\n",
    "\n",
    "                if len(all_sentencesinbook) > sent_num/self.min_books:\n",
    "                    _author_cleanbooks[booknum] = all_sentencesinbook\n",
    "        else: \n",
    "            # 1. recursively select a number of books until len(_author_cleanbooks) matches min_books\n",
    "            #    at each recursion, apply _cleansegment_book on the book. If the cleaned book meets the \n",
    "            #    length requirement, add to _author_cleanbooks. Do this for up to 10 tries, failing\n",
    "            #    which we will exclude the author and all of his/her books from the corpus.\n",
    "            _tries = 0\n",
    "            _unvisited = set(authorbooks_info_keys)\n",
    "            while len(_author_cleanbooks) < self.min_books and _tries<=20 and len(_unvisited) > 0:\n",
    "                # randomly select min_books number from author. if author only has min_books, \n",
    "                # sampling will return the same set\n",
    "                try: # try to take a random sample (it could fail if _unvisited < sample size)\n",
    "                    _newnums = set(random.sample(_unvisited, self.min_books-len(_author_cleanbooks)))\n",
    "                    _unvisited = _unvisited.difference(_newnums)\n",
    "                    for booknum in _newnums: \n",
    "                        all_sentencesinbook =\\\n",
    "                        self._cleansegment_book(booknum=booknum, precise_clean=precise_clean)\n",
    "\n",
    "                        if len(all_sentencesinbook) > sent_num/self.min_books:\n",
    "                            _author_cleanbooks[booknum] = all_sentencesinbook\n",
    "                    _tries += 1\n",
    "                except: # break the while loop if sampling fails\n",
    "                    break \n",
    "                                \n",
    "        # 2. if min_books still not met, move to return. this effectively excludes author from corpus \n",
    "        if len(_author_cleanbooks) < self.min_books:\n",
    "            return \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # 3. extract k number of sentences from each accepted author's books. k is the total number\n",
    "        #   of sentences required for each author divided by the author min_books set for the corpus \n",
    "        _authors_sentences = dict()\n",
    "        for booknum in _author_cleanbooks: \n",
    "            _sample =\\\n",
    "            random.sample(_author_cleanbooks[booknum], round(sent_num/self.min_books))\n",
    "            # book to temporary dictionary, with the booknum as the key. \n",
    "            _authors_sentences[booknum] = _sample\n",
    "        \n",
    "        # 4. write the cleaned book and sampled sentences to file \n",
    "        for booknum in _authors_sentences: \n",
    "            self._write_tofile(booknum, all_sentencesinbook = _author_cleanbooks[booknum], \n",
    "                               sample_all_sentencesinbook = _authors_sentences[booknum])\n",
    "        \n",
    "        # 5. update the processed_subcorpus attribute for the author, with the sampled sentences\n",
    "        self.processed_subcorpus.update(_authors_sentences)\n",
    "        \n",
    "        \n",
    "    def _write_tofile(self, booknum, all_sentencesinbook, sample_all_sentencesinbook): \n",
    "        '''\n",
    "        A helper function to export processed texts and lists of sentences to csv and plaintext files. \n",
    "        Called by _build_subcorpus. \n",
    "        '''\n",
    "        # 1. write the entire cleaned and segmented book to a csv file. \n",
    "        if not os.path.isdir('./data/wholebook_csv'):\n",
    "            os.mkdir(\"data/wholebook_csv\")\n",
    "        with open(\"./data/wholebook_csv/\"+self.number+\"_\"+booknum+'.csv', 'a') as csv_file:\n",
    "            # we set file open mode to 'a' to append to file instead of overwriting\n",
    "            write_file = csv.writer(csv_file, dialect = 'excel')\n",
    "            write_file.writerow(all_sentencesinbook)\n",
    "            del csv_file # delete to free memory\n",
    "\n",
    "        # 2a. write the book sample to a csv file\n",
    "        if not os.path.isdir('./data/booksample_csv'):\n",
    "            os.mkdir(\"data/booksample_csv\")\n",
    "        with open(\"./data/booksample_csv/\"+self.number+\"_\"+booknum+'.csv', 'a') as csv_file:\n",
    "            write_file = csv.writer(csv_file, dialect = 'excel')\n",
    "            write_file.writerow(sample_all_sentencesinbook)\n",
    "            del csv_file # delete to free memory\n",
    "\n",
    "        # 2b. write the book sample to a txt file\n",
    "        if not os.path.isdir('./data/booksample_txt'):\n",
    "            os.mkdir(\"data/booksample_txt\")\n",
    "        with open(\"./data/booksample_txt/\"+self.number+\"_\"+booknum+'.txt', 'a') as txt_file:\n",
    "            txt_file.writelines(\"\\t\".join(sample_all_sentencesinbook))\n",
    "            del txt_file # delete to free memory\n",
    "\n",
    "            \n",
    "    def _cleansegment_book(self, booknum, precise_clean, \n",
    "                           urlpath = \"https://www.gutenberg.org/files/{}/{}.txt\"):\n",
    "        '''\n",
    "        takes a booknum, navigates to the PG page with the .txt file for this book. uses urlopen to \n",
    "        retrieve the contents of this file. if precise_clean = False, only retrieves lines between the\n",
    "        last \"*START\" and first \"*END\" line in the file. \n",
    "        \n",
    "        Inputs | booknum: int - the unique number on PG for a book, urlpath: str - the url structure for a book's  \n",
    "        page on PG, precise_clean: boolean \n",
    "        Outputs | all_sentencesinbook: list -  a list of sentences after the basic pre-processing \n",
    "        '''    \n",
    "        book_content = []\n",
    "        \n",
    "        # open target_url with the urllib.request.urlopen() method,\n",
    "        # for each line in response, decodes with the expected \n",
    "        # encoding format PG uses for plain .txt book files. \n",
    "        # see https://www.gutenberg.org/wiki/Gutenberg:Readers%27_FAQ#R.35._What_do_the_filenames_of_the_texts_mean.3F\n",
    "        for extenc_pair in [('', 'ascii'), ('-0', \"utf-8\"), ('-8', 'ISO 8859-1')]: \n",
    "            # iterate through likely filename endings and associated encodings on PG\n",
    "            try: \n",
    "                target_url = urlpath.format(booknum,booknum+extenc_pair[0])\n",
    "                with urllib.request.urlopen(target_url) as response: \n",
    "                    for line in response: \n",
    "                        # urlopen reads as bytes, to ease processing, we decode to string.\n",
    "                        # most PG .txt files are encoded in latin-1/ascii format. \n",
    "                        try:\n",
    "                            book_content.append(line.decode(extenc_pair[1]))\n",
    "                        except: # revert to latin-1 in the event of unexpected PG encoding behaviour \n",
    "                            book_content.append(line.decode(\"latin-1\"))\n",
    "                    response.close()\n",
    "                    del response\n",
    "            except HTTPError: \n",
    "                continue\n",
    "                \n",
    "        # remove PG metadata precisely, but slower to execute\n",
    "        if precise_clean == True: \n",
    "            start_index = 0                # index for the start of the text\n",
    "            stop_index = -1  # index for the end of the text  \n",
    "\n",
    "            # Each PG book .txt file is ended with metadata marked with \"* START\" and \"* END\" or \n",
    "            # minor variations. * START-tagged metadata tend to, but don't always just, appear in \n",
    "            # the first 25% of the .txt file, and vice-versa for * END tagged metadata. we split \n",
    "            # the file in the top and bottom thirds and run searches for * START and * END (for \n",
    "            # some savings in search time)\n",
    "            \n",
    "            _2third_marker = round(len(book_content)*0.67)\n",
    "                                         \n",
    "            #1. search for *END tags from the back of the file, for two-thirds of the file\n",
    "            for index_num in range(_2third_marker):\n",
    "                if re.match(r'\\*+\\s*END ', book_content[-index_num]):\n",
    "                    stop_index = -index_num\n",
    "            \n",
    "            #2. search for anomalous *START tags in the last two-thirds of the file, \n",
    "            #   but begining from the, possibly new, stop_index \n",
    "            for index_num in range(-stop_index, _2third_marker):\n",
    "                # searching for the last * END from the back, in the last half of the file \n",
    "                if re.match(r'\\*+\\s*START ', book_content[-index_num]):\n",
    "                    stop_index = -index_num\n",
    "            \n",
    "            #3. finally, search for the last START tag from the front, within the first two-thirds\n",
    "            for index_num in range(_2third_marker):\n",
    "                # searching for the last * START in the first half of the file \n",
    "                if re.match(r'\\*+\\s*START ', book_content[index_num]):\n",
    "                    start_index = index_num \n",
    "            \n",
    "            # slicing the section of the text between the start_index and stop_index. \n",
    "            book_content = book_content[start_index:stop_index]\n",
    "\n",
    "        # join all the text without \"\\r\\n\" i.e. return carriage and newline \n",
    "        clean_book_content = \" \".join([l.strip(\"\\r\\n\") for l in book_content if l != \"\\r\\n\"])\n",
    "        # use nltk's sent_tokenise\n",
    "        all_sentencesinbook = sent_tokenize(clean_book_content)\n",
    "\n",
    "        # strip first and last 10% of lines (as a buffer to avoid collecting generic publishing data)\n",
    "        _10pc = round(len(all_sentencesinbook)*0.10)\n",
    "        all_sentencesinbook = all_sentencesinbook[_10pc:-_10pc]\n",
    "\n",
    "        return all_sentencesinbook\n",
    "\n",
    "\n",
    "    def _get_authorabstract(self, author_wiki_info):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. Gets available author abstract from wikipedia using\n",
    "        the wikipedia python package. \n",
    "        \n",
    "        Input | authorwiki_info: dict\n",
    "        Result | stores results to self.authorabstracts\n",
    "        '''\n",
    "        _abstracts = {}\n",
    "\n",
    "        for wikilang in author_wiki_info: \n",
    "        # set the language \n",
    "            wikipedia.set_lang(wikilang)\n",
    "            wikiname = author_wiki_info[wikilang].split(\"/\")[-1]\n",
    "\n",
    "            try: # without disambiguation: we start with the presumption that PG has \n",
    "                # accurate author wikipedia links. set auto_suggest to False to prevent \n",
    "                # additional (unnecessary) handling of the author page name by the wikipedia package.\n",
    "                wikipage = wikipedia.page(title=wikiname, auto_suggest=False)\n",
    "                _abstracts[wikilang] = wikipage.summary\n",
    "\n",
    "            except PageError: \n",
    "                print(\"There is a PageError resulting with this wikiname: {}\".format(wiki_name) )\n",
    "                pass \n",
    "            except DisambiguationError: \n",
    "                print(\"There is a DisambiguationError resulting with this wikiname: {}\".format(wiki_name))\n",
    "                pass \n",
    "\n",
    "        self.authorabstracts = _abstracts    \n",
    "    \n",
    "      \n",
    "    def _get_literarymovement(self, authorwiki_info):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. takes an author's name, makes a DBpedia query \n",
    "        with the name using the SPARQLWrapper package, \n",
    "        returns the literary movements that the author is associated with. \n",
    "        \n",
    "        Input | authorwiki_info: dict\n",
    "        Result | stores results to self.literarymovements\n",
    "        '''\n",
    "        if len(authorwiki_info) > 0:\n",
    "            # since dbpedia is based off wikipedia, we will use the author's name as in \n",
    "            # the wikipedia link obtained from PG. \n",
    "            _authorwiki_info = authorwiki_info.copy().popitem()\n",
    "            wikiname = _authorwiki_info[1].split('/')[-1].replace(\"_\", \" \")\n",
    "            wikilang = _authorwiki_info[0]\n",
    "\n",
    "            sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "            query = '''SELECT ?text\n",
    "                WHERE {\n",
    "                ?writer rdf:type dbo:Writer ;\n",
    "                foaf:name %r @%s.\n",
    "                {?writer dbo:genre ?genre .}\n",
    "                UNION\n",
    "                {?writer dbo:movement ?genre .}\n",
    "                ?genre rdfs:label ?text\n",
    "                FILTER (lang(?text) = \"en\")\n",
    "                }''' %(wikiname, wikilang)\n",
    "            # using %r for names to handle non-ascii wikinames that get passed as bytes in %s\n",
    "            # see https://pyformat.info/ for e.g. \"Bahá'u'lláh\" becomes \"Bahá\\'u\\'lláh\"\n",
    "            sparql.setQuery(query)\n",
    "            sparql.setReturnFormat(JSON)\n",
    "            results = sparql.query().convert()\n",
    "            genres = set()\n",
    "            for i in range (len(results['results']['bindings'])):\n",
    "                genre = results['results']['bindings'][i]['text']['value']\n",
    "                genres.add((re.sub(r'\\([^)]*\\)', '', genre.lower())).rstrip())\n",
    "            self.literarymovements = list(genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9jRyG8qz1R1"
   },
   "source": [
    "### 3. Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hzx6BXsmMQMk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 authors have been processed, out of 2789 authors in selections\n",
      "200 authors have been processed, out of 2789 authors in selections\n",
      "300 authors have been processed, out of 2789 authors in selections\n",
      "400 authors have been processed, out of 2789 authors in selections\n",
      "500 authors have been processed, out of 2789 authors in selections\n",
      "600 authors have been processed, out of 2789 authors in selections\n",
      "700 authors have been processed, out of 2789 authors in selections\n",
      "800 authors have been processed, out of 2789 authors in selections\n",
      "900 authors have been processed, out of 2789 authors in selections\n",
      "1000 authors have been processed, out of 2789 authors in selections\n",
      "1100 authors have been processed, out of 2789 authors in selections\n",
      "1200 authors have been processed, out of 2789 authors in selections\n",
      "1300 authors have been processed, out of 2789 authors in selections\n",
      "1400 authors have been processed, out of 2789 authors in selections\n",
      "1500 authors have been processed, out of 2789 authors in selections\n",
      "1600 authors have been processed, out of 2789 authors in selections\n",
      "1700 authors have been processed, out of 2789 authors in selections\n",
      "1800 authors have been processed, out of 2789 authors in selections\n",
      "1900 authors have been processed, out of 2789 authors in selections\n",
      "2000 authors have been processed, out of 2789 authors in selections\n",
      "2100 authors have been processed, out of 2789 authors in selections\n",
      "2200 authors have been processed, out of 2789 authors in selections\n",
      "2300 authors have been processed, out of 2789 authors in selections\n",
      "2400 authors have been processed, out of 2789 authors in selections\n",
      "2500 authors have been processed, out of 2789 authors in selections\n",
      "2600 authors have been processed, out of 2789 authors in selections\n",
      "2700 authors have been processed, out of 2789 authors in selections\n"
     ]
    }
   ],
   "source": [
    "min_books = 3\n",
    "max_books = 30\n",
    "sent_num = 250 \n",
    "precise_clean=True\n",
    "# instantiate a GutenbergCorpusBuilder \n",
    "PGcorpus = GutenbergCorpusBuilder(corpusname=\"PG-eng-author-min{}\".format(min_books))\n",
    "# start collecting and filtering author and book details from the Project Gutenberg site\n",
    "PGcorpus.get_library(min_books = min_books, max_books = max_books, \n",
    "                     languages = [\"english\"], roles = [\"as author\"])\n",
    "# read text files, select sentences, pre-process sentences, store to subcorpora\n",
    "PGcorpus.populate_corpus(sent_num=sent_num, precise_clean=precise_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 General check of the content of the corpus files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 342 files in total within the corpus\n"
     ]
    }
   ],
   "source": [
    "# general sense\n",
    "import glob\n",
    "filenames = glob.glob(\"./data/booksample_txt/*.txt\")\n",
    "_to_delete = []\n",
    "for i in filenames: \n",
    "    with open(i) as file:\n",
    "        if \"gutenberg\" in file.read():\n",
    "            _to_delete.append(i)\n",
    "            print (\"This selected sentences file {} may still contain Gutenberg metadata\".format(i))\n",
    "print(\"There are {} files in total within the corpus\".format(len(filenames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__4 out of the 336 files in the corpus, about 1.2%, still contains Gutenberg metadata. These files belong to 3 authors. The Gutenberg metadata likely got selected as part of the sentences from the books because these books' file structure deviate substantially from the typical structure variants we observed. We will manually delete these 3 authors and their books  from the corpus.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the associated processed_subcorpus stored in the Author instances for the files/authors above\n",
    "for author in [\"a32063\", \"a3826\", \"a8240\"]:\n",
    "    PGcorpus.corpus[author].processed_subcorpus = dict()\n",
    "    \n",
    "# delete the associated processed_subcorpus stored in the /data directory\n",
    "for filename in _to_delete: \n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKrllplDkyqb"
   },
   "source": [
    "### 4. Test code - informal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJCo6Hhxkxn4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 112 authors entered in this corpus\n"
     ]
    }
   ],
   "source": [
    "# check that corpus contains only english books. it should return nothing. \n",
    "for i in PGcorpus.authors.keys(): \n",
    "    for i2 in PGcorpus.authors[i][\"books_info\"]:\n",
    "        if \"English\" not in PGcorpus.authors[i][\"books_info\"][i2]:\n",
    "            print(PGcorpus.authors[i][\"books_info\"][i2])\n",
    "\n",
    "# check that corpus contains only books where author role is as Author. it should return nothing.\n",
    "for i in PGcorpus.authors.keys(): \n",
    "    for i2 in PGcorpus.authors[i][\"books_info\"]:\n",
    "        if \"Author\" not in PGcorpus.authors[i][\"books_info\"][i2]:\n",
    "            print(PGcorpus.authors[i][\"books_info\"][i2])\n",
    "                                     \n",
    "# return number of authors selected into corpus \n",
    "counter = 0\n",
    "for i in PGcorpus.authors.keys():\n",
    "    if len(PGcorpus.corpus[i].processed_subcorpus) ==3:\n",
    "        counter +=1\n",
    "print (\"There are %d authors entered in this corpus\"%counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pickling the GutenbergCorpus object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picklemaker(filename, objectname):\n",
    "    # open the file for writing\n",
    "    fileObject = open(filename,'wb')\n",
    "\n",
    "    pickle.dump(objectname,fileObject)\n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()\n",
    "    \n",
    "picklemaker(PGcorpus.corpusname+PGcorpus.corpusversion+\".pickle\", PGcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a convenience function to load a previously pickled GutenbergCorpusBuilder object\n",
    "# def pickleloader(filename):\n",
    "#     # # open the file for writing\n",
    "#     fileObject = open(filename,'rb')\n",
    "    \n",
    "#     return pickle.load(fileObject)  \n",
    "\n",
    "#     # here we close the fileObject\n",
    "#     fileObject.close()\n",
    "\n",
    "# PGcorpus = pickleloader(\"PG-eng-author-min3v2019424.pickle\")\n",
    "# # run/re-run cells containing GutenbergCorpusBuilder and Author class before loading the pickle file\n",
    "# # to provide the unpickler attribute structures of the class for unpickling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UOe2Z-guIIG"
   },
   "source": [
    "### 6. pymongo implementation to store the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAZsdVRoz0xA"
   },
   "outputs": [],
   "source": [
    "# instantiate a MongoClient object. using the URI for the Mongo server. If it is locally hosted, \n",
    "# it is by default on the 27017 port. If using cloud, use the provided URI\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "corpusdb = client[\"corpus\"]\n",
    "authorscollection = corpusdb[\"authors\"]\n",
    "bookscollection = corpusdb[\"books\"]\n",
    "\n",
    "authorscollection.drop()\n",
    "bookscollection.drop()\n",
    "\n",
    "# insert the dicationary with each author's information into the authors collection. Only authors \n",
    "# whose books were eventually admitted into the corpus will be added to the database. \n",
    "for authornum in PGcorpus.authors: \n",
    "    PGcorpus.authors[authornum][\"authornum\"]=authornum\n",
    "    authorscollection.insert_one(PGcorpus.authors[authornum])\n",
    "\n",
    "# insert each of the selected books and their selected sentences into the books collection \n",
    "for authornum in PGcorpus.corpus: \n",
    "    author_subcorpus = PGcorpus.corpus[authornum].processed_subcorpus\n",
    "    if len(author_subcorpus) > 0:   \n",
    "        for booknum in author_subcorpus.keys():\n",
    "            bookscollection.insert_one({\"booknum\": booknum, \"authornum\":authornum, \n",
    "                                        \"selected_sents\": author_subcorpus[booknum]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5cc1484e936c7d5f414da28a'),\n",
       " 'authorname': 'Abbott Edwin Abbott',\n",
       " 'books_info': {'97': 'Flatland: A Romance of Many Dimensions (English) (as Author)',\n",
       "  '45506': 'Flatland: A Romance of Many Dimensions (English) (as Author)',\n",
       "  '201': 'Flatland: A Romance of Many Dimensions (Illustrated) (English) (as Author)',\n",
       "  '22600': 'How to Write Clearly: Rules and Exercises on English Composition (English) (as Author)',\n",
       "  '54223': 'Onesimus: Memoirs of a Disciple of St. Paul (English) (as Author)',\n",
       "  '48843': 'Philochristus: Memoirs of a Disciple of the Lord (English) (as Author)',\n",
       "  '56843': 'Silanus the Christian (English) (as Author)'},\n",
       " 'wiki_info': {'en': 'http://en.wikipedia.org/wiki/Edwin_Abbott_Abbott'},\n",
       " 'authorabstracts': {},\n",
       " 'literarymovements': [],\n",
       " 'authornum': 'a64'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some test code to check insertions \n",
    "authorscollection.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5cc14850936c7d5f414dad6f'),\n",
       " 'booknum': '16534',\n",
       " 'authornum': 'a4501',\n",
       " 'selected_sents': ['When did Joseph visit Jackson county the second time?',\n",
       "  'Brigham Young and the Twelve took an active part in getting volunteers.',\n",
       "  'Another boy by the name of Charles Merrick was discovered.',\n",
       "  'He advised the Saints to be like other people and not organize themselves with bishops, presidents, etc.',\n",
       "  'They were not in rebellion, and if the president had simply sent some one to investigate, he would have found out that truth; but he had acted on the spur of the moment, and the troops were already far on the way.',\n",
       "  'Topics.--1.',\n",
       "  'What is the state militia?',\n",
       "  'What does He say is good?',\n",
       "  'On that date Joseph ordained Sidney Rigdon to be his first counselor, and Frederick G. Williams to be his second counselor, and these three now became the First Presidency, which is the highest authority in the Church.',\n",
       "  '4.',\n",
       "  'What led Joseph and Oliver to ask the Lord about baptism?',\n",
       "  'A great many brethren agreed to this, and that winter and spring the move eastward to Illinois continued.',\n",
       "  'Note how eager the governor was to restore these few presumably abused people to their lands--but it was all right that twelve hundred \"Mormons\" should be driven from their property!',\n",
       "  '10. Who returned to Winter Quarters?',\n",
       "  'What is meant by the gathering?',\n",
       "  '5.',\n",
       "  'On the morning of July 22nd, Joseph arose from his bed and commenced administering to the sick.',\n",
       "  'One of the most important revelations given at this time was regarding the law of tithing.',\n",
       "  'Meanwhile, Nauvoo grew into a large city.',\n",
       "  '\"We will rid Jackson county of the \"Mormons\",\" they said, \"peaceably if we can, forcibly if we must.',\n",
       "  '12.',\n",
       "  'July 20th, they held another meeting which was more successful.',\n",
       "  'It would not be easy to understand the many causes that led to these troubles, but a few may be noted.',\n",
       "  'Calling of the Seventy.',\n",
       "  '8.',\n",
       "  'Priesthood.',\n",
       "  '3.',\n",
       "  'The brethren were then marched into the camp of the mob-militia where they were received with great shouts, curses, and yells.',\n",
       "  'Wishing to know more about this new revelation of God, he had sought out Joseph.',\n",
       "  'Questions and Review.--1.',\n",
       "  \"Again, the Saints could not buy out all the mobbers' land in Jackson, much as they would have liked so to do, as there was so much of it, and they had no money to pay for it in thirty days.\",\n",
       "  'For five years he continued to tell of the truth of the work of God in the meetings of the Saints in Utah.',\n",
       "  'However, the Saints engaged four lawyers to protect them in the courts.',\n",
       "  '6.',\n",
       "  'The Lord then told Nephi to take all who would listen to him and leave the other brothers and those who upheld them in their evil deeds.',\n",
       "  'ye fiends of the infernal pit!',\n",
       "  'Parley P. Pratt and a small company were sent ahead to find another location for a settlement.',\n",
       "  \"Joseph and his wife, Emma, stayed at Brother Whitney's house for some time.\",\n",
       "  'Elder Taylor now tried to jump from the window.',\n",
       "  \"Some of the brethren carried away a few hundred dollars' worth when they went to Salt Lake Valley the next summer.\",\n",
       "  'There were now about five thousand people in the valley, and prospects were not very encouraging, owing to the small crop raised.',\n",
       "  'Name some of the books he wrote.',\n",
       "  'But it was not to be.',\n",
       "  '16.',\n",
       "  '3.',\n",
       "  'Where was the second settlement in Utah made?',\n",
       "  '3.',\n",
       "  'How was Bishop Partridge abused?',\n",
       "  'Some of the mob painted and dressed themselves up as Indians.',\n",
       "  'Nearly all the leading brethren were in hiding; and, as they could not speak to the people in their meetings, they wrote epistles which were read to the Saints at their conferences.',\n",
       "  '8. Who were the United Brethren?',\n",
       "  'Joseph was busy setting the Church in order and in receiving the word of the Lord for the guidance of the Saints.',\n",
       "  'On these carts were loaded baggage and provisions, and the men and boys pulled them across the plains.',\n",
       "  'While teaching the people the need of repenting of their evil doings and being baptized for the remission of their sins, Brother Kimball felt someone pulling at his coat: \"Please sir, will you baptize me?\"',\n",
       "  'The sheriff kept after the mob to prevent them from burning houses, etc., and this made the mobbers very angry.',\n",
       "  'How long did it take them?',\n",
       "  'President John Taylor.',\n",
       "  'What prevented a band of pioneers from going to the mountains that summer?',\n",
       "  'He had at that time four sons, Laman, Lemuel, Sam, and Nephi.',\n",
       "  'Where was the army camped?',\n",
       "  'As the elders were walking out of the village, the young folks of the place ran to meet them, the older people stood in their doors to greet and bless them, while the children ran ahead, hand in hand, singing their songs of gladness.',\n",
       "  '7.',\n",
       "  '\"Stu--boy, stu--boy, take him Watch, lay hold of him!',\n",
       "  \"Mobbers' meetings in Independence.\",\n",
       "  'This officer mistreated Joseph shamefully.',\n",
       "  'How did the Saints know that Joseph was not a fallen prophet?',\n",
       "  '2.',\n",
       "  'Governor Dunklin talked very pleasantly about the rights of the Saints, but in the end he did nothing to protect the people or help them to gain possession of their property.',\n",
       "  '7.',\n",
       "  '4.',\n",
       "  'Here Elder Cannon translated the Book of Mormon into the native language, and sometime after he had it printed.',\n",
       "  '9.',\n",
       "  'What did Jesus say about persecution?',\n",
       "  'The rain fell in torrents, the lightning flashed, the thunder shook the earth.',\n",
       "  '10.',\n",
       "  \"How many Seventies' quorums are there in the Church?\",\n",
       "  'They had withdrawn from the Church of England, and were now praying that the Lord would send them more light.',\n",
       "  'The total number of souls in the Church was reported to be about two thousand; and all this was done in the short space of eight months.',\n",
       "  'This was the 26th of June.',\n",
       "  'I will not live another minute and hear such language.',\n",
       "  'The enemy was soon put to flight across the river.',\n",
       "  'They preached to some Indians near the city of Buffalo, presented them with the Book of Mormon, and proceeded on their journey into the state of Ohio.',\n",
       "  '5.']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookscollection.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exporting and importing the database as json dumps and loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the entire database into jsons\n",
    "\n",
    "if not os.path.isdir('./data/mongo_dumps'):  \n",
    "    os.mkdir(\"data/mongo_dumps\")\n",
    "with open(\"./data/mongo_dumps/jsondump_authors_mongo.json\", \"w+\") as jsondump_authors_mongo: \n",
    "    jsondump_authors_mongo.write(dumps(authorscollection.find()))\n",
    "    jsondump_authors_mongo.close()\n",
    "    \n",
    "with open(\"./data/mongo_dumps/jsondump_books_mongo.json\", \"w+\") as jsondump_books_mongo: \n",
    "    jsondump_books_mongo.write(dumps(bookscollection.find()))\n",
    "    jsondump_books_mongo.close()\n",
    "\n",
    "# # loading the exported jsons into the db\n",
    "# with open(\"/data/mongo_dumps/jsondump_books_mongo.json\", \"r\") as jsondump_authors_mongo: \n",
    "    \n",
    "#     try: # try inserting all at a go\n",
    "#         authorscollection.insert_many(loads(jsondump_authors_mongo.read()))\n",
    "#     except: # insert one by one in event of duplicates already existing in db\n",
    "#         try: \n",
    "#             for doc in loads(jsondump_authors_mongo.read()):\n",
    "#             authorscollection.insert_one(doc)\n",
    "#         except:\n",
    "#             continue \n",
    "    \n",
    "# with open(\"/data/mongo_dumps/jsondump_books_mongo.json\", \"r\") as jsondump_books_mongo: \n",
    "#     try: # try inserting all at a go\n",
    "#         bookscollection.insert_many(loads(jsondump_books_mongo.read()))\n",
    "#     except: # insert one by one in event of duplicates already existing in db\n",
    "#         try: \n",
    "#             for doc in loads(jsondump_books_mongo.read()):\n",
    "#             bookscollection.insert_one(doc)\n",
    "#         except:\n",
    "#             continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Exploring the raw corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science fiction', 29),\n",
       " ('romanticism', 18),\n",
       " (\"children's literature\", 14),\n",
       " ('fantasy', 14),\n",
       " ('poetry', 9),\n",
       " ('romance novel', 8),\n",
       " ('fiction', 7),\n",
       " ('horror fiction', 7),\n",
       " ('fantasy fiction', 6),\n",
       " ('naturalism', 6)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find number of literary movements in corpus, and the most common \n",
    "literary_movements = []\n",
    "for i in authorscollection.find():\n",
    "    literary_movements.extend(i[\"literarymovements\"])\n",
    "\n",
    "literary_movements_counter = collections.Counter(literary_movements)\n",
    "literary_movements_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most frequent lengths:  [(31, 248), (27, 245), (24, 239), (32, 237), (26, 228), (30, 226), (41, 222), (28, 221), (21, 220), (29, 219)] \n",
      " max length:  2811 \n",
      " min length:  1\n"
     ]
    }
   ],
   "source": [
    "# find the average sentence length in the corpus \n",
    "sentence_char_lengths = [] \n",
    "for book in bookscollection.find():\n",
    "    # find the number of characters in each sentence within the corpus \n",
    "    sentence_char_lengths.extend(len(sentence) for sentence in book[\"selected_sents\"])\n",
    "\n",
    "sentence_char_lengths_counter = collections.Counter(sentence_char_lengths)\n",
    "print(\"10 most frequent lengths: \", sentence_char_lengths_counter.most_common(10), \"\\n\",\n",
    "      \"max length: \", max(sentence_char_lengths_counter), \"\\n\",\n",
    "      \"min length: \", min(sentence_char_lengths_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most frequent lengths:  [(1, 1263), (6, 1216), (5, 1207), (7, 1134), (4, 1117), (3, 1068), (2, 1039), (8, 1015), (9, 967), (10, 967)] \n",
      " max length:  368 \n",
      " min length:  1\n"
     ]
    }
   ],
   "source": [
    "# find the average token length in the corpus \n",
    "sentence_token_lengths = [] \n",
    "for book in bookscollection.find():\n",
    "    # find the number of characters in each sentence within the corpus \n",
    "    sentence_token_lengths.extend(len(sentence.split()) for sentence in book[\"selected_sents\"])\n",
    "\n",
    "sentence_token_lengths_counter = collections.Counter(sentence_token_lengths)\n",
    "print(\"10 most frequent lengths: \", sentence_token_lengths_counter.most_common(10), \"\\n\",\n",
    "      \"max length: \", max(sentence_token_lengths_counter), \"\\n\",\n",
    "      \"min length: \", min(sentence_token_lengths_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataScienceProject-Session1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
