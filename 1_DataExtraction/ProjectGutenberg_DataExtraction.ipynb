{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SXRp4LcnC4I"
   },
   "source": [
    "# Extracting and storing RDF information and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preliminaries: dependencies and seed state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7wQupQrnC4J"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipedia\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import csv, datetime, time, random, string, re, urllib, os, collections  \n",
    "from pymongo import MongoClient\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from urllib.error import HTTPError\n",
    "import pickle \n",
    "with open('randomstate.pickle', 'rb') as f:\n",
    "    random.setstate(pickle.load(f)) \n",
    "# We use random sampling in some of the functions of our program. \n",
    "# To ensure we can replicate the same dataset, we include the use\n",
    "# of the same seed state whenever running this corpus builder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIVWTPMRexID"
   },
   "source": [
    "### General overview\n",
    "\n",
    "Our solution is supported via two classes, their attributes and associated functions. The first class - GutenbergCorpusBuilder - is intended to handle author-title mining on the Project Gutenberg website, filtering and selection as well as storage of overall corpus data. The attributes of this class is designed for easy ingestion into a mongoDB, or similar non-relational, database. The other class - Author - is intended to hold the processes for accessing text files for books, processing them and storing them. The Author class also contains methods for collecting \n",
    "\n",
    "To build a corpus, a user will only need to interact with 2 methods from the GutenbergCorpusBuilder. These are namely, in the order of intended use: (i) get_library; and (ii) populate_corpus. \n",
    "\n",
    "The first method - __get_library__ - will crawl all of the 'Browse by Author' pages on the Project Gutenberg website and collect author information (including books he/she authored as well as wikipedia pages). By passing the 'min_book', 'max_book' as well as 'languages' and 'roles' parameters, the user can balance the content of the corpus in terms of author and book numbers, as well as have it filtered based on language(s) and author role(s). We note in particular, that the author role setting could be become significant for certain machine learning tasks (for instance incorporating books where an author is merely an editor or contributor could lead to degraded model performance for an author-genre classification task). \n",
    "\n",
    "The default values for this function are: \n",
    "\n",
    "|Parameter\t|Default setting\t|Setting for this corpus\t|\n",
    "|---\t|---\t|---\t|\n",
    "|min_books   \t|1   \t|3   \t|\n",
    "|max_books   \t|float(inf)   \t|30   \t|\n",
    "|languages   \t|'all'   \t|[\"english]   \t|\n",
    "|roles   \t|'all'   \t|['as author']   \t|\n",
    "\n",
    "The parameters, their types and default settings are designed with the intention to allow the collection of all books available on Project Gutenberg. For the purpose of our collected corpus, we have chosen the parameters so as to obtain a balanced corpus that selects major as well as minor authors. \n",
    "\n",
    "The second method - __populate_corpus__ - will take the pre-filtered list of author and their books, instantiate a Author object, and begin collecting carefully data on the author and his/her books. At the background, the function will first retrieve all of the literary movements the author is associated with, from DBpedia. The primary information bottleneck lies with these literary movement labels - not all authors have DBpedia pages and for those that do, many do not have literary movements associated to them. As such, we only proceed with the next steps of adding an author to the final corpus if he/she has these literary movement labels. For authors that pass this filter, the method proceeds to extracts and stores the multilingual abstracts for the author. Finally, it proceeds to randomly pick a set of the author's book's (the size of this set is the same for all authors and equivalent to the min_books set for the corpus), and clean and segment the text files in a list of sentences. The cleaning is intended to exclude boilerplate Project Gutenberg metadata as well as book publisher information. An option is provided to clean the text files by exclude the Project Gutenberg metadata more precisely, albeit this extends the time required to process and collect the corpus.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUOWBinlnC4Z"
   },
   "source": [
    "### 1. A class to store a corpus obtained from the Project Gutenberg website. \n",
    "\n",
    "The corpus is build with functions within the class that filter the authors and books on the Project Gutenberg website. It also calls on the Author class (below), to process and generate information about sentences from an author's books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfplnCWJnC4Z"
   },
   "outputs": [],
   "source": [
    "class GutenbergCorpusBuilder: \n",
    "    '''\n",
    "    initiates a GutenbergCorpusBuilder object which stores information about selected authors that are found on \n",
    "    the Project Gutenberg(PG) website.\n",
    "    Authors are stored based on their unique PG numerical code. For each author, selected books and their \n",
    "    respective PG URL are stored.\n",
    "    \n",
    "    Inputs: corpusname - string representing name of the corpus being created.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, corpusname):\n",
    "        self.corpusname = corpusname\n",
    "        self.corpusversion = \"v\"+ str(datetime.datetime.now().year) + str(datetime.datetime.now().month) +\\\n",
    "        str(datetime.datetime.now().day)\n",
    "        \n",
    "        # a dictionary containing dictionaries.\n",
    "        # The top level keys - unique numbers for authors on the PG website,\n",
    "        # the values -  dictionaries containing author information: \n",
    "        # keys - 'authorname', 'books_info'; wiki_info'; \n",
    "        # values - strings or embedded dictionaries:\n",
    "        # authorname: string with extracted name;\n",
    "        # books_info: dictionary: key - book ID, value - book title;\n",
    "        # wiki_info: dictionary: key - language; value - wiki link extracted from PG\n",
    "        #\n",
    "        # {author ID: {authorname:'name', books_info:{bookID: book title}, wiki_info: {language: wiki link}}}\n",
    "        self.authors = dict()\n",
    "       \n",
    "        # a dictionary containing sets of sentences selected from each author's filtered books;\n",
    "        # the top level keys are the unique numbers for authors, the values are sets containing\n",
    "        # sentences from an author's book (as strings).\n",
    "        # {author ID: Author()}\n",
    "        self.corpus = dict()\n",
    "        \n",
    "        self.min_books = int # the min_books value passed into the get_library method.\n",
    "        self.max_books = int # the max_books value passed into the get_library method.\n",
    "        \n",
    "    def populate_corpus(self, sent_num=250):\n",
    "        '''\n",
    "        for each author in self.authors, generates an Author() class instance, populates all \n",
    "        attributes of the Author() class, adds to self.corpus.\n",
    "        \n",
    "        Inputs | sent_num: int - the total number of sentences to collect for a single author selected for the corpus. \n",
    "        '''\n",
    "        if len(self.authors) > 0:\n",
    "            _counter = 0\n",
    "            for authornum in self.authors: # authornum is a key (an author's unique number)\n",
    "                if authornum not in self.corpus.keys():\n",
    "                    # instantiate an Author()\n",
    "                    authorname = self.authors[authornum][\"authorname\"]\n",
    "                    authorwiki_info = self.authors[authornum][\"wiki_info\"].copy() \n",
    "                    authorbooks_info_keys = list(self.authors[authornum][\"books_info\"].copy().keys())\n",
    "                    \n",
    "                    _author = Author(authorname=authorname, authornum=authornum, \n",
    "                                     authorwiki_info=authorwiki_info, \n",
    "                                     authorbooks_info_keys=authorbooks_info_keys, \n",
    "                                    min_books=self.min_books)\n",
    "                    \n",
    "                    # run the populate_attributes() to extract and process the information for the author\n",
    "                    _author.populate_attributes(authorwiki_info=authorwiki_info, \n",
    "                                                authorbooks_info_keys=authorbooks_info_keys,\n",
    "                                                sent_num=sent_num)\n",
    "                    \n",
    "                    # store to Author() to corpus \n",
    "                    self.corpus[authornum] = _author\n",
    "\n",
    "                    # append wiki abstract info and literary movement to author's dictionary in \n",
    "                    # self.author so as to easily transmit each author's basic information into mongodb\n",
    "                    self.authors[authornum][\"authorabstracts\"] = _author.authorabstracts\n",
    "                    self.authors[authornum][\"literarymovements\"] = _author.literarymovements\n",
    "                _counter += 1\n",
    "                if _counter%100 == 0:\n",
    "                    print(\"{} authors have been processed, out of {} authors in selections\".format(_counter, len(self.authors)))\n",
    "        else: \n",
    "            print(\"The authors attribute is empty, please run get_library first or \\\n",
    "            check the parameters passed into get_library.\")\n",
    "         \n",
    "        \n",
    "    \n",
    "    def get_library(self, min_books=1, max_books=float(\"inf\"), languages = \"all\", roles = \"all\"):\n",
    "        '''\n",
    "        Goes through the PG website's 'sort by author' pages. Extracts author and corresponding book \n",
    "        information that meet a number of selection criterion (see inputs). \n",
    "        \n",
    "        Inputs | \n",
    "        1. min_books: int - the minimum number of books available for an author, which meets the languages \n",
    "        and roles parameters. Default value is 1. \n",
    "        5. languages: either a str \"all\", or a list containing the languages (in lowercase) to count towards \n",
    "        the author's min_books level. The list of languages available can be found here \n",
    "        https://www.gutenberg.org/catalog/. Default is \"all\". \n",
    "        6. roles: either a str \"all\", or a list containing the roles that an author can have in a book. \n",
    "        These include: Commentator, Translator, Contributor, Photographer, Illustrator, Editor.\n",
    "        Default value is \"all\".\n",
    "        Outputs | saves the results to self.authors\n",
    "        '''\n",
    "        charlist = []\n",
    "        charlist[:0] = [letter for letter in string.ascii_lowercase] + [\"other\"]\n",
    "\n",
    "        library = dict()\n",
    "        for char in charlist:\n",
    "            # Team comment: we select the authors and books via the \"Browse by Author\" lists instead of  \n",
    "            # the \"Browse by Books\" list. Although the latter has a more predictable page structure \n",
    "            # (i.e. 1 book name, followed by 1 author name, recursively), the former includes \n",
    "            # information about the Author's role in the book. We believe that this could have\n",
    "            # a meaningful impact on the predictive capabilities for models on different tasks, \n",
    "            # especially at larger scale.\n",
    "            \n",
    "            link = 'https://www.gutenberg.org/browse/authors/'+ char\n",
    "            page = requests.get(link)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            one_letter = self._unite_authors_nums_books(self._get_authors_numsnames(soup)[0],\\\n",
    "                                                            self._get_authors_numsnames(soup)[1],\\\n",
    "                                                            self._get_bookswiki_info(soup)[0],\\\n",
    "                                                            self._get_bookswiki_info(soup)[1],\\\n",
    "                                                            min_books, max_books, languages, roles)\n",
    "            \n",
    "            library.update(one_letter)\n",
    "            print(\"{} authors from the '{}' alphabetical category have been added.\".format(len(one_letter),char))\n",
    "            \n",
    "            # del variable to clear memory\n",
    "            del soup\n",
    "            \n",
    "            # Put the function to sleep for a randomised number of seconds (non-integer number between \n",
    "            # 0.5 and 4) to mimic human surfing patterns.\n",
    "            time.sleep(random.uniform(0.5,4))\n",
    "            \n",
    "        self.authors = library\n",
    "        self.min_books = min_books\n",
    "        self.max_books = max_books\n",
    "    \n",
    "    def _get_authors_numsnames(self, soup):\n",
    "        '''\n",
    "        A helper function for _unite_authors_nums_books. Extracts all author names from a BeautifulSoup \n",
    "        copy  of a 'Browse by Author' page on the PG website. \n",
    "        \n",
    "        Inputs | soup:a BeautifulSoup object - containg a copy of the PG 'Browse by Author' page. \n",
    "        Outputs | a tuple containing two lists: The first contains author's numbers on the page, the \n",
    "        second contains corresponding author's names on the page. \n",
    "        '''\n",
    "        authornames = []\n",
    "        # the author names are stored within the \"name\" attribute under each \"a\" class\n",
    "        # use regex wildcard so that find_all will catch and return all \"a names\" with values\n",
    "        authorname_BSlist = soup.find_all('a', {\"name\":re.compile(\"\\w*\")})\n",
    "\n",
    "        for authorname in authorname_BSlist:\n",
    "            # \\- and \\? to escape special characters. .rstrip to remove trailing whitespaces. \n",
    "            authornames.append(re.sub(r'[0-9,\\-\\?]*', '', authorname.text).rstrip())\n",
    "\n",
    "        authornums = []\n",
    "        # the author numbers are stored within the \"href\" attribute. Every line for a book \n",
    "        # on the page has a \"title\" attribute with the value \"Link to this author\". We will use\n",
    "        # this to shift to only the lines with the author's number. \n",
    "        authornums_BSlist = soup.find_all('a', {\"title\":\"Link to this author\"})\n",
    "\n",
    "        for authornum in authornums_BSlist:\n",
    "            authornums.append(authornum[\"href\"].lstrip(\"#\"))\n",
    "\n",
    "        return authornums, authornames\n",
    "\n",
    "    def _get_bookswiki_info(self, soup):\n",
    "        '''\n",
    "        A helper function for _unite_authors_nums_books. Extracts all the book titles and numbers from a \n",
    "        BeautifulSoup copy of a 'Browse by Author' page on the PG website. Also extracts author wikipedia \n",
    "        link information if it is available on the PG website. \n",
    "        \n",
    "        Inputs | soup:a BeautifulSoup object - containg a copy of the PG 'Browse by Author' page. \n",
    "        Outputs | a tuple containing two lists. \n",
    "          1. The first list contains dictionaries. Each dictionary contains information about an author's \n",
    "          books on PG. this includes: book titles, corresponding PG books numbers, the author's role in \n",
    "          each book, and the language of each book. \n",
    "          2. The second list contains dictionaries. Each dictionary contains information about an author's \n",
    "          wikipedia links on PG. An author's wiki dictionary may be empty, contain 1 link, or more than 1 \n",
    "          link. \n",
    "        '''\n",
    "        books_info = list()\n",
    "        wiki_info = list()\n",
    "\n",
    "        # content under the 'ul' tags: books, links as one list organized by ul\n",
    "        authorsbooks_BSlist = soup.find_all('ul')\n",
    "        # for each ul, access the content: books, links; each book is a bs object\n",
    "\n",
    "        for author in authorsbooks_BSlist:\n",
    "            # there are two classes of attributes within each ul tag. The book information\n",
    "            # 1. title and book PG number is under the 'pgdbetext' class. \n",
    "            books_BSlist = author.find_all(class_='pgdbetext')\n",
    "\n",
    "            authorbooks_info = {}\n",
    "            for book in books_BSlist:\n",
    "                # the book numbers are stored in the href attribute. e.g. \"ebooks/19323\"\n",
    "                booknum = book.find('a')['href'].split(\"/\")[-1]\n",
    "                PG_booktitle = book.text\n",
    "\n",
    "                # storing the information regarding a single author's books in a dictionary\n",
    "                authorbooks_info[booknum]=PG_booktitle\n",
    "            \n",
    "            # appending the dictionary containing one author's books to a list\n",
    "            books_info.append(authorbooks_info)\n",
    "            \n",
    "            # 2. for the author is/are under the 'pgdbxlink' class. \n",
    "            wiki_BSlist = author.find_all(class_='pgdbxlink')\n",
    "\n",
    "            authorwiki_info = {}\n",
    "\n",
    "            for wiki in wiki_BSlist:\n",
    "                # 1. the wiki links are stored in the href attribute. \n",
    "                PG_wikilink = wiki.find('a')['href'] # get the whole link\n",
    "                \n",
    "                # some of the lines tagged \"pgdbxlink\" include \"See also: xxx\" links. \n",
    "                # we filter them out here\n",
    "                if \"wikipedia.org\" in PG_wikilink:\n",
    "\n",
    "                    # 2. because PG stores the link in URL-safe format (e.g. \"\\x\" is \"%\"), we will face \n",
    "                    # issues with non-ASCII characters e.g. á whose URL-safe encoding cannot be passed \n",
    "                    # into the wikipedia package. use urllib.requests.unquote to resolve this \n",
    "                    # https://docs.python.org/2/library/urllib.html#utility-functions \n",
    "                    PG_wikilink = urllib.request.unquote(PG_wikilink)\n",
    "\n",
    "                    # 3. get the language code for the wikipage\n",
    "                    wikilang = re.findall(r'/\\w+', PG_wikilink)[0].strip('/')\n",
    "                    # storing the information regarding a single author's wikipedia links in a dictionary\n",
    "                    authorwiki_info[wikilang] = PG_wikilink\n",
    "\n",
    "            # appending the dictionary containing one author's wikipedia links to a list\n",
    "            wiki_info.append(authorwiki_info)\n",
    "            \n",
    "        return books_info, wiki_info\n",
    "\n",
    "    \n",
    "    def _unite_authors_nums_books(self, authornums, authornames, books_info, wiki_info, min_books, \n",
    "                                  max_books, languages, roles):\n",
    "        '''\n",
    "        A helper function for get_library. \n",
    "        \n",
    "        Inputs | \n",
    "        1. authornums:list - list of author numbers from a \"sort by author\" page on the PG website. \n",
    "        2. authornames:list - list of author names  from a \"sort by author\" page on the PG website. \n",
    "        3. books_info: list - a list containing dictionaries, each of which has information about \n",
    "        an author's books \n",
    "        4. wiki_info: list - a list containing dictionaries, each of which has information about \n",
    "        an author's wikipedia\n",
    "        page, as provided by the PG website. There may be none, one, or more wikilinks for an author. \n",
    "        5. min_books:int - the minimum number of books available for an author, which meets the languages \n",
    "        and roles parameters. default value is 1 (since an author listed on PG will have at least 1 book \n",
    "        to his name).\n",
    "        6, max_books:int - the minimum number of books available for an author, which meets the languages \n",
    "        and roles parameters. default value is infinity.\n",
    "        7. languages:either a str \"all\", or a list containing the languages (in lowercase) to count towards the author's \n",
    "        min_books level. The list of languages available can be found here \n",
    "        https://www.gutenberg.org/catalog/. default is \"all\". \n",
    "        8. roles: either a str \"all\", or a list containing the roles (in lowercase) that an author can \n",
    "        have in a book. These include: commentator, translator, contributor, photographer, illustrator, \n",
    "        commentator, editor. default value is \"all\".\n",
    "        Outputs | a dictionary containing PG numbers for authors who meet the min_books, languages and \n",
    "        roles requirements, as well as information each of these author's books. \n",
    "        '''\n",
    "        # we want to be sure that the authornums, authornames, books_info, and wiki_info are aligned \n",
    "        # before proceeding to merge them. \n",
    "        try:\n",
    "            assert len(authornums)==len(authornames) and len(authornums)==len(books_info) and len(authornums)==len(wiki_info)\n",
    "        except AssertionError as e:\n",
    "            e.args += (\"The length of authornums, authornames and books_info do not match.\",)\n",
    "            raise\n",
    "            \n",
    "        authorbooks_info = dict()\n",
    "        # if default parameters passed into the function, add all authors and their books to the corpus.  \n",
    "        if min_books == None and languages == \"all\" and roles == \"all\":\n",
    "            for i in range(len(authornums)):\n",
    "                authorbooks_info[authornums[i]]=\\\n",
    "                        {\"authorname\": authornames[i], \"books_info\": books_info[i], \"wiki_info\": wiki_info[i]}\n",
    "        else:\n",
    "            # place languages and roles input in sets, for use in .intersection below. \n",
    "            languages_set = set(languages)\n",
    "            roles_set = set(roles)\n",
    "            \n",
    "            for i in range(len(authornums)):\n",
    "                author_bookset = books_info[i]\n",
    "                _topop = []\n",
    "                for book in author_bookset: \n",
    "                    \n",
    "                    # using regex to find text in parentheses. Book language e.g. (English) and author role \n",
    "                    # e.g. (as Author) are contained in parentheses. Some books which are part of a series, \n",
    "                    # have (of N) in their titles too, where N is the number of books in that series. \n",
    "                    title_text_in_parentheses =\\\n",
    "                    re.findall(r'\\(([a-zA-Z]+\\s*[a-zA-Z]*[0-9]*)\\)', author_bookset[book])\n",
    "                    \n",
    "                    # lowercase the text in parentheses and put it into sets. \n",
    "                    _title_text_in_parentheses =\\\n",
    "                    set([i.lower() for i in title_text_in_parentheses])\n",
    "                    \n",
    "                    # if languages is set to \"all\" or if the intersection of _title_text_in_parentheses\n",
    "                    # and languages_set returns a non-empty set, pass to the next check. Otherwise add this \n",
    "                    # book number to the list of books to pop from this author_bookset\n",
    "                    if languages == \"all\" or _title_text_in_parentheses.intersection(languages_set): pass\n",
    "                    else:\n",
    "                        _topop.append(book) \n",
    "                        continue \n",
    "                    # do the same for author's role as for language above\n",
    "                    if roles == \"all\" or _title_text_in_parentheses.intersection(roles_set): pass\n",
    "                    else:\n",
    "                        _topop.append(book) \n",
    "                        continue    \n",
    "                # pop the books that don't meet the language and role specifications. \n",
    "                for pop in _topop:\n",
    "                    books_info[i].pop(pop)\n",
    "                    \n",
    "                # check if number of books meeting the language and role requirements meet the \n",
    "                # min_book requirement \n",
    "                if min_books <= len(books_info[i]) <= max_books:\n",
    "                    authorbooks_info[authornums[i]]=\\\n",
    "                            {\"authorname\": authornames[i], \"books_info\": books_info[i], \n",
    "                             \"wiki_info\": wiki_info[i]}\n",
    "                    \n",
    "        return authorbooks_info \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"There are {} authors entered in this corpus\".format(len(self.corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1WRLJZBtpZK"
   },
   "source": [
    "### 2. A class to store subcorpora obtained from the Project Gutenberg website for each Author. \n",
    "\n",
    "The subcorpus is build with functions within the class that pre-processes each .txt file for filtered author books on the Project Gutenberg website. It also obtains the abstracts and literary movement tags for each author from Wikipedia and DBPedia respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m96uH1j8mzXl"
   },
   "outputs": [],
   "source": [
    "class Author:\n",
    "    '''\n",
    "    Initiates a Author object which stores information about a selected author that is available on \n",
    "    the Project Gutenberg(PG) website. Other information drawn from (i) DBPedia - author literary movements\n",
    "    (ii) wikipedia - multilingual author abstract, (iii) PG - selected sentences from author's texts \n",
    "    \n",
    "    Inputs: authorname: str, authornum:str, authorwiki_info: dict, authorbooks_info_keys:list of numbers in \n",
    "    string, min_books: int\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, authorname, authornum, authorwiki_info, authorbooks_info_keys, \n",
    "                 min_books):\n",
    "        '''\n",
    "        initiates the Author object with the author's name. \n",
    "        \n",
    "        Inputs | authorname: str, authornum:str, authorwiki_info: dict, \n",
    "        authorbooks_info_keys:list of author numbers (in str), min_books: int\n",
    "        '''\n",
    "        self.name = authorname\n",
    "        self.number = authornum\n",
    "        self.min_books = min_books  # the min_book setting at the GutenbergCorpus class that led\n",
    "                                    # to this author's selection for the corpus\n",
    "        \n",
    "        # a dictionary with the book numbers as keys and lists as values. Lists  \n",
    "        # contain strings that have been pre-processed by the segment_sentence method.\n",
    "        self.processed_subcorpus = dict()       \n",
    "        \n",
    "        self.authorabstracts = dict() \n",
    "        self.literarymovements = list()\n",
    "        \n",
    "        \n",
    "    def populate_attributes(self, authorwiki_info, authorbooks_info_keys, sent_num):\n",
    "        '''\n",
    "        A convenience function to call _build_subcorpus, _get_authorabstract and  _get_literarymovement, \n",
    "        which will respectively populate the processed_subcorpus, authorabstract and literarymovements\n",
    "        attributes for this Author instance.  \n",
    "        \n",
    "        Inputs | authorwiki_info: dict, authorbooks_info_keys:list of numbers (in str), min_books: int\n",
    "        Result | stores results to self.literarymovements, self.authorabstracts and self.processed_subcorpus \n",
    "        '''\n",
    "        # check for /data directory, else create for storing files from _build_subcorpus\n",
    "        if not os.path.isdir('./data'):  \n",
    "            os.mkdir(\"data\")\n",
    "        \n",
    "        self._get_literarymovement(authorwiki_info)\n",
    "        \n",
    "        # the information bottleneck is at the dbpedia literary movement labels.\n",
    "        # multilingual wiki abstract and text processing requires a large amount of resources \n",
    "        # so we only do these for authors that we manage to get literary movements for.\n",
    "        if len(self.literarymovements) > 0: \n",
    "            self._get_authorabstract(authorwiki_info)\n",
    "            self._build_subcorpus(authorbooks_info_keys, sent_num)\n",
    "        \n",
    "    \n",
    "    def _build_subcorpus(self, authorbooks_info_keys, sent_num):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. Selects the books of an author's to extract sentences\n",
    "        from. the number of books is the same as min_book set for the corpus's author selection criteria.\n",
    "        if an author has more books than min_books, a random sampling is done. a basic pre-processing to \n",
    "        remove PG metadata and publisher information is done next. results are written to two sets of csv files. \n",
    "        the first contains only selected sentences, the second contains the entire processed book. \n",
    "        \n",
    "        Inputs | authorbooks_info_keys:list of numbers (in str), min_books: int\n",
    "        Result | saves to two sets of csv files: (i) selected sentences only; (ii) entire pre-processed \n",
    "        book. also stores (i) to self.processed_subcorpus\n",
    "        '''\n",
    "        # check if the author has more books than min_books, if so randomly select min_books number from author\n",
    "        if len(authorbooks_info_keys) > self.min_books:\n",
    "            selectedbooks_numbers = random.sample(authorbooks_info_keys, self.min_books)\n",
    "        else: \n",
    "            selectedbooks_numbers = authorbooks_info_keys\n",
    "            \n",
    "        for booknum in selectedbooks_numbers: \n",
    "            # check if this book has already been processed and stored. if so, skip. \n",
    "            if booknum in self.processed_subcorpus:\n",
    "                continue\n",
    "            all_sentencesinbook =\\\n",
    "            self._cleansegment_book(booknum, urlpath = \"https://www.gutenberg.org/files/{}/{}.txt\")\n",
    "            if len(all_sentencesinbook) >0: \n",
    "                # 1. write the entire cleaned and segmented book to a csv file. \n",
    "                if not os.path.isdir('./data/wholebook_csv'):\n",
    "                    os.mkdir(\"data/wholebook_csv\")\n",
    "                with open(\"./data/wholebook_csv/\"+self.number+\"_\"+booknum+'.csv', 'w') as csv_file:\n",
    "                    write_file = csv.writer(csv_file, dialect = 'excel')\n",
    "                    write_file.writerow(all_sentencesinbook)\n",
    "                    csv_file.close()\n",
    "                    del csv_file\n",
    "\n",
    "                # 2. extract n number of sentences from this book. where n = sent_num/self.min_books\n",
    "                try:\n",
    "                    sample_all_sentencesinbook =\\\n",
    "                random.sample(all_sentencesinbook, round(sent_num/self.min_books))\n",
    "                except: # in the rare case of very extremely short books, go on to sample the whole book\n",
    "                    sample_all_sentencesinbook = all_sentencesinbook\n",
    "\n",
    "                # 2a. write the book sample to a csv file\n",
    "                if not os.path.isdir('./data/booksample_csv'):\n",
    "                    os.mkdir(\"data/booksample_csv\")\n",
    "\n",
    "                with open(\"./data/booksample_csv/\"+self.number+\"_\"+booknum+'.csv', 'w') as csv_file:\n",
    "                    write_file = csv.writer(csv_file, dialect = 'excel')\n",
    "                    write_file.writerow(sample_all_sentencesinbook)\n",
    "                    csv_file.close()\n",
    "                    del csv_file\n",
    "                    \n",
    "                # 2b. write the book sample to a txt file\n",
    "                if not os.path.isdir('./data/booksample_txt'):\n",
    "                    os.mkdir(\"data/booksample_txt\")\n",
    "\n",
    "                with open(\"./data/booksample_txt/\"+self.number+\"_\"+booknum+'.txt', 'w+') as txt_file:\n",
    "                    txt_file.writelines(\"\\t\".join(sample_all_sentencesinbook))\n",
    "                    txt_file.close()\n",
    "                    del txt_file\n",
    "\n",
    "                # 3. store to self.processed_subcorpus with booknum as key. \n",
    "                self.processed_subcorpus[booknum] = sample_all_sentencesinbook\n",
    "\n",
    "            else: # for books with links that fail in opening and processing under _cleansegment_book\n",
    "                print('_build_subcorpus for {} failed'.format(booknum))\n",
    "                pass\n",
    "\n",
    "    \n",
    "    def _cleansegment_book(self, booknum, urlpath = \"https://www.gutenberg.org/files/{}/{}.txt\", \n",
    "                          precise_clean = False):\n",
    "        '''\n",
    "        takes a booknum, navigates to the PG page with the .txt file for this book. uses urlopen to \n",
    "        retrieve the contents of this file. if precise_clean = False, only retrieves lines between the\n",
    "        last \"*START\" and first \"*END\" line in the file. \n",
    "        \n",
    "        Inputs | booknum: int - the unique number on PG for a book, urlpath: str - the url structure for a book's  \n",
    "        page on PG, precise_clean: boolean \n",
    "        Outputs | all_sentencesinbook: list -  a list of sentences after the basic pre-processing \n",
    "        '''    \n",
    "        book_content = []\n",
    "        \n",
    "        # open target_url with the urllib.request.urlopen() method,\n",
    "        # for each line in response, decodes with \"latin-1\", which is the expected \n",
    "        # encoding format PG uses for plain .txt book files. \n",
    "        for extenc_pair in [('', 'ascii'), ('-0', \"utf-8\"), ('-8', 'ISO 8859-1')]: \n",
    "            # iterate through likely filename endings and associated encodings on PG\n",
    "            # see https://www.gutenberg.org/wiki/Gutenberg:Readers%27_FAQ#R.35._What_do_the_filenames_of_the_texts_mean.3F\n",
    "            try: \n",
    "                target_url = urlpath.format(booknum,booknum+extenc_pair[0])\n",
    "                with urllib.request.urlopen(target_url) as response: \n",
    "                    for line in response: \n",
    "                        # urlopen reads as bytes, to ease processing, we decode to string.\n",
    "                        # most PG .txt files are encoded in latin-1 format. \n",
    "                        try:\n",
    "                            book_content.append(line.decode(extenc_pair[1]))\n",
    "                        except: \n",
    "                            book_content.append(line.decode(\"latin-1\"))\n",
    "                    response.close()\n",
    "                    del response\n",
    "            except HTTPError: \n",
    "                continue\n",
    "                \n",
    "        # remove PG metadata precisely, but much slower to execute\n",
    "        if precise_clean == True: \n",
    "            start_index=0 #index from the first part of the text\n",
    "            stop_index=0  #index from the second part of the text  \n",
    "\n",
    "            # Each PG book .txt file is ended with metadata marked with \"* START\" and \"* END\" or \n",
    "            # minor variations. * START-tagged metadata appear in the first 10% of the .txt file, \n",
    "            # and vice-versa for * END. we split the file in the top and bottom 10% halves and \n",
    "            # run searches for * START and * END (for savings in search time)\n",
    "            for index_num in range(round(len(book_content)/10)):\n",
    "                # searching for the last * START in the first half of the file \n",
    "                if re.match(r'\\*+\\s*START ', book_content[index_num]):\n",
    "                    start_index = index_num+1 \n",
    "\n",
    "                # searching for the last * END from the back, in the last half of the file \n",
    "                if re.match(r'\\*+\\s*END ', book_content[-index_num]):\n",
    "                    stop_index = -index_num-1 \n",
    "\n",
    "            # in cases where the book has no *END or similar metadata, stop_index will remain at \n",
    "            # 0 and the subsequent slicing will result in 0 lines of the book being sliced out. \n",
    "            # decrement here to solve \n",
    "            if stop_index == 0: stop_index -= 1\n",
    "\n",
    "            # slicing the section of the text between the start_index and stop_index. \n",
    "            book_content = book_content[start_index:stop_index]\n",
    "\n",
    "        # join all the text without \"\\r\\n\" i.e. return carriage and newline \n",
    "        clean_book_content = \" \".join([l.strip(\"\\r\\n\") for l in book_content if l != \"\\r\\n\"])\n",
    "        # use nltk's sent_tokenise\n",
    "        all_sentencesinbook = sent_tokenize(clean_book_content)\n",
    "\n",
    "        # strip first and last 15% of lines (as a buffer to avoid collecting generic publishing data)\n",
    "        _15pc = round(len(all_sentencesinbook)*0.15)\n",
    "        all_sentencesinbook = all_sentencesinbook[_15pc:-_15pc]\n",
    "\n",
    "        return all_sentencesinbook\n",
    "\n",
    "\n",
    "    def _get_authorabstract(self, author_wiki_info):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. Gets available author abstract from wikipedia using\n",
    "        the wikipedia python package. \n",
    "        \n",
    "        Input | authorwiki_info: dict\n",
    "        Result | stores results to self.authorabstracts\n",
    "        '''\n",
    "        _abstracts = {}\n",
    "\n",
    "        for wikilang in author_wiki_info: \n",
    "        # set the language \n",
    "            wikipedia.set_lang(wikilang)\n",
    "            wikiname = author_wiki_info[wikilang].split(\"/\")[-1]\n",
    "\n",
    "            try: # without disambiguation: we start with the presumption that PG has \n",
    "                # accurate author wikipedia links. set auto_suggest to False to prevent \n",
    "                # additional (unnecessary) handling of the author page name by the wikipedia package.\n",
    "                wikipage = wikipedia.page(title=wikiname, auto_suggest=False)\n",
    "                _abstracts[wikilang] = wikipage.summary\n",
    "\n",
    "            except PageError: \n",
    "                print(\"There is a PageError resulting with this wikiname: {}\".format(wiki_name) )\n",
    "                pass \n",
    "            except DisambiguationError: \n",
    "                print(\"There is a DisambiguationError resulting with this wikiname: {}\".format(wiki_name))\n",
    "                pass \n",
    "\n",
    "        self.authorabstracts = _abstracts    \n",
    "    \n",
    "      \n",
    "    def _get_literarymovement(self, authorwiki_info):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. takes an author's name, makes a DBpedia query \n",
    "        with the name using the SPARQLWrapper package, \n",
    "        returns the literary movements that the author is associated with. \n",
    "        \n",
    "        Input | authorwiki_info: dict\n",
    "        Result | stores results to self.literarymovements\n",
    "        '''\n",
    "        if len(authorwiki_info) > 0:\n",
    "            # since dbpedia is based off wikipedia, we will use the author's name as in \n",
    "            # the wikipedia link obtained from PG. \n",
    "            _authorwiki_info = authorwiki_info.copy().popitem()\n",
    "            wikiname = _authorwiki_info[1].split('/')[-1].replace(\"_\", \" \")\n",
    "            wikilang = _authorwiki_info[0]\n",
    "\n",
    "            sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "            query = '''SELECT ?text\n",
    "                WHERE {\n",
    "                ?writer rdf:type dbo:Writer ;\n",
    "                foaf:name %r @%s.\n",
    "                {?writer dbo:genre ?genre .}\n",
    "                UNION\n",
    "                {?writer dbo:movement ?genre .}\n",
    "                ?genre rdfs:label ?text\n",
    "                FILTER (lang(?text) = \"en\")\n",
    "                }''' %(wikiname, wikilang)\n",
    "            # using %r for names to handle non-ascii wikinames that get passed as bytes in %s\n",
    "            # see https://pyformat.info/ for e.g. \"Bahá'u'lláh\" becomes \"Bahá\\'u\\'lláh\"\n",
    "            sparql.setQuery(query)\n",
    "            sparql.setReturnFormat(JSON)\n",
    "            results = sparql.query().convert()\n",
    "            genres = set()\n",
    "            for i in range (len(results['results']['bindings'])):\n",
    "                genre = results['results']['bindings'][i]['text']['value']\n",
    "                genres.add((re.sub(r'\\([^)]*\\)', '', genre.lower())).rstrip())\n",
    "            self.literarymovements = list(genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9jRyG8qz1R1"
   },
   "source": [
    "### 3. Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hzx6BXsmMQMk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 authors from the 'a' alphabetical category have been added.\n",
      "297 authors from the 'b' alphabetical category have been added.\n",
      "211 authors from the 'c' alphabetical category have been added.\n",
      "141 authors from the 'd' alphabetical category have been added.\n",
      "61 authors from the 'e' alphabetical category have been added.\n",
      "122 authors from the 'f' alphabetical category have been added.\n",
      "141 authors from the 'g' alphabetical category have been added.\n",
      "210 authors from the 'h' alphabetical category have been added.\n",
      "16 authors from the 'i' alphabetical category have been added.\n",
      "63 authors from the 'j' alphabetical category have been added.\n",
      "68 authors from the 'k' alphabetical category have been added.\n",
      "148 authors from the 'l' alphabetical category have been added.\n",
      "265 authors from the 'm' alphabetical category have been added.\n",
      "39 authors from the 'n' alphabetical category have been added.\n",
      "47 authors from the 'o' alphabetical category have been added.\n",
      "129 authors from the 'p' alphabetical category have been added.\n",
      "3 authors from the 'q' alphabetical category have been added.\n",
      "116 authors from the 'r' alphabetical category have been added.\n",
      "276 authors from the 's' alphabetical category have been added.\n",
      "89 authors from the 't' alphabetical category have been added.\n",
      "17 authors from the 'u' alphabetical category have been added.\n",
      "35 authors from the 'v' alphabetical category have been added.\n",
      "174 authors from the 'w' alphabetical category have been added.\n",
      "1 authors from the 'x' alphabetical category have been added.\n",
      "17 authors from the 'y' alphabetical category have been added.\n",
      "4 authors from the 'z' alphabetical category have been added.\n",
      "2 authors from the 'other' alphabetical category have been added.\n",
      "100 authors have been processed, out of 2787 authors in selections\n",
      "200 authors have been processed, out of 2787 authors in selections\n",
      "300 authors have been processed, out of 2787 authors in selections\n",
      "400 authors have been processed, out of 2787 authors in selections\n",
      "500 authors have been processed, out of 2787 authors in selections\n",
      "600 authors have been processed, out of 2787 authors in selections\n",
      "700 authors have been processed, out of 2787 authors in selections\n",
      "800 authors have been processed, out of 2787 authors in selections\n",
      "900 authors have been processed, out of 2787 authors in selections\n",
      "1000 authors have been processed, out of 2787 authors in selections\n",
      "1100 authors have been processed, out of 2787 authors in selections\n",
      "1200 authors have been processed, out of 2787 authors in selections\n",
      "1300 authors have been processed, out of 2787 authors in selections\n",
      "1400 authors have been processed, out of 2787 authors in selections\n",
      "1500 authors have been processed, out of 2787 authors in selections\n",
      "1600 authors have been processed, out of 2787 authors in selections\n",
      "1700 authors have been processed, out of 2787 authors in selections\n",
      "1800 authors have been processed, out of 2787 authors in selections\n",
      "1900 authors have been processed, out of 2787 authors in selections\n",
      "2000 authors have been processed, out of 2787 authors in selections\n",
      "2100 authors have been processed, out of 2787 authors in selections\n",
      "2200 authors have been processed, out of 2787 authors in selections\n",
      "2300 authors have been processed, out of 2787 authors in selections\n",
      "2400 authors have been processed, out of 2787 authors in selections\n",
      "2500 authors have been processed, out of 2787 authors in selections\n",
      "2600 authors have been processed, out of 2787 authors in selections\n",
      "2700 authors have been processed, out of 2787 authors in selections\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    min_books = 3\n",
    "    max_books = 30\n",
    "    sent_num = 250 \n",
    "    # instantiate a GutenbergCorpusBuilder \n",
    "    PGcorpus = GutenbergCorpusBuilder(corpusname=\"PG-eng-author-min{}\".format(min_books))\n",
    "    # start collecting and filtering author and book details from the Project Gutenberg site\n",
    "    PGcorpus.get_library(min_books = min_books, max_books = max_books, \n",
    "                         languages = [\"english\"], roles = [\"as author\"])\n",
    "    # read text files, select sentences, pre-process sentences, store to subcorpora\n",
    "    PGcorpus.populate_corpus(sent_num=250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKrllplDkyqb"
   },
   "source": [
    "### 4. Test code - informal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJCo6Hhxkxn4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 189 authors entered in this corpus\n"
     ]
    }
   ],
   "source": [
    "# check that corpus contains only english books. it should return nothing. \n",
    "for i in PGcorpus.authors.keys(): \n",
    "    for i2 in PGcorpus.authors[i][\"books_info\"]:\n",
    "        if \"English\" not in PGcorpus.authors[i][\"books_info\"][i2]:\n",
    "            print(PGcorpus.authors[i][\"books_info\"][i2])\n",
    "\n",
    "# check that corpus contains only books where author role is as Author. it should return nothing.\n",
    "for i in PGcorpus.authors.keys(): \n",
    "    for i2 in PGcorpus.authors[i][\"books_info\"]:\n",
    "        if \"Author\" not in PGcorpus.authors[i][\"books_info\"][i2]:\n",
    "            print(PGcorpus.authors[i][\"books_info\"][i2])\n",
    "                                     \n",
    "# return number of authors selected into corpus \n",
    "counter = 0\n",
    "for i in PGcorpus.authors.keys():\n",
    "    if len(PGcorpus.corpus[i].processed_subcorpus) ==3:\n",
    "        counter +=1\n",
    "print (\"There are %d authors entered in this corpus\"%counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pickling the GutenbergCorpus object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picklemaker(filename, objectname):\n",
    "    # open the file for writing\n",
    "    fileObject = open(filename,'wb')\n",
    "\n",
    "    pickle.dump(objectname,fileObject)\n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()\n",
    "    \n",
    "picklemaker(PGcorpus.corpusname+PGcorpus.corpusversion+\".pickle\", PGcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a convenience function to load a previously pickled GutenbergCorpusBuilder object\n",
    "def pickleloader(filename):\n",
    "    # # open the file for writing\n",
    "    fileObject = open(filename,'rb')\n",
    "    \n",
    "    return pickle.load(fileObject)  \n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()\n",
    "\n",
    "PGcorpus = pickleloader(\"PG-eng-author-min3v2019419.pickle\")\n",
    "# run/re-run cells containing GutenbergCorpusBuilder and Author class before loading the pickle file\n",
    "# to provide the unpickler attribute structures of the class for unpickling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UOe2Z-guIIG"
   },
   "source": [
    "### 6. pymongo implementation to store the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAZsdVRoz0xA"
   },
   "outputs": [],
   "source": [
    "# instantiate a MongoClient object. using the URI for the Mongo server. If it is locally hosted, \n",
    "# it is by default on the 27017 port. If using cloud, use the provided URI\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "corpusdb = client[\"corpus\"]\n",
    "authorscollection = corpusdb[\"authors\"]\n",
    "bookscollection = corpusdb[\"books\"]\n",
    "\n",
    "authorscollection.drop()\n",
    "bookscollection.drop()\n",
    "\n",
    "# insert the documents into the authors collection \n",
    "for authornum in PGcorpus.authors: \n",
    "    PGcorpus.authors[authornum][\"authornum\"]=authornum\n",
    "    authorscollection.insert_one(PGcorpus.authors[authornum])\n",
    "\n",
    "    # add the selected sentences for each book into the books collection \n",
    "    author_subcorpus = PGcorpus.corpus[authornum].processed_subcorpus\n",
    "    for booknum in author_subcorpus.keys():\n",
    "        bookscollection.insert_one({\"booknum\": booknum, \"selected_sents\": author_subcorpus[booknum]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5cb91a22936c7d12d2f0512d'),\n",
       " 'authorname': 'Whishaw Frederick',\n",
       " 'books_info': {'45098': 'Boris the Bear-Hunter (English) (as Author)',\n",
       "  '36341': 'Gunpowder Treason and Plot, and Other Stories for Boys (English) (as Author)',\n",
       "  '56522': 'Mazeppa (English) (as Author)',\n",
       "  '42967': 'Moscow: A Story of the French Invasion of 1812 (English) (as Author)',\n",
       "  '46813': 'The Romance of the Woods (English) (as Author)'},\n",
       " 'wiki_info': {'en': 'http://en.wikipedia.org/wiki/Fred_Whishaw'},\n",
       " 'authorabstracts': {'en': 'Frederick James Whishaw (14 March 1854 – 8 July 1934) was a Russian-born British novelist, historian, poet and musician. A popular author of children\\'s fiction at the turn of the 20th century, he published over forty volumes of his work between 1884 and 1914.\\nHe was a prolific historical novelist, many of his books being set in Czarist Russia, and his \"schoolboy\" and adventure serials appeared in many boys\\' magazines of the era. Several of these were published as full-length novels, such as Gubbins Minor and Some Other Fellows (1897), The Boys of Brierley Grange (1906) and The Competitors: A Tale of Upton House School (1906).  Other stories, such as The White Witch of the Matabele (1897) or The Three Scouts: A Story of the Boer War (1900), depicted colonial Africa.\\nWhishaw was also one of the first translators of Fyodor Dostoevsky, the first in the English language.  He had several of the Russian author\\'s novels published between 1886 and 1888.'},\n",
       " 'literarymovements': ['poetry',\n",
       "  'fiction',\n",
       "  'historical fiction',\n",
       "  'adventure fiction',\n",
       "  'non-fiction',\n",
       "  'social commentary',\n",
       "  'travel literature',\n",
       "  \"children's literature\",\n",
       "  'philosophy and literature'],\n",
       " 'authornum': 'a38216'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some test code to check insertions \n",
    "authorscollection.find_one({\"authornum\":\"a38216\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Exploring the raw corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science fiction', 29),\n",
       " ('romanticism', 18),\n",
       " (\"children's literature\", 14),\n",
       " ('fantasy', 14),\n",
       " ('poetry', 9),\n",
       " ('romance novel', 8),\n",
       " ('fiction', 7),\n",
       " ('horror fiction', 7),\n",
       " ('fantasy fiction', 6),\n",
       " ('naturalism', 6)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find number of literary movements in corpus, and the most common \n",
    "literary_movements = []\n",
    "for i in authorscollection.find():\n",
    "    literary_movements.extend(i[\"literarymovements\"])\n",
    "\n",
    "literary_movements_counter = collections.Counter(literary_movements)\n",
    "literary_movements_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most frequent lengths:  [(6, 587), (4, 430), (24, 405), (44, 385), (21, 363), (28, 360), (32, 360), (23, 356), (26, 353), (31, 353)] \n",
      " max length:  2811 \n",
      " min length:  1\n"
     ]
    }
   ],
   "source": [
    "# find the average sentence length in the corpus \n",
    "sentence_char_lengths = [] \n",
    "for book in bookscollection.find():\n",
    "    # find the number of characters in each sentence within the corpus \n",
    "    sentence_char_lengths.extend(len(sentence) for sentence in book[\"selected_sents\"])\n",
    "\n",
    "sentence_char_lengths_counter = collections.Counter(sentence_char_lengths)\n",
    "print(\"10 most frequent lengths: \", sentence_char_lengths_counter.most_common(10), \"\\n\",\n",
    "      \"max length: \", max(sentence_char_lengths_counter), \"\\n\",\n",
    "      \"min length: \", min(sentence_char_lengths_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most frequent lengths:  [(1, 2330), (6, 1946), (4, 1914), (7, 1866), (5, 1849), (10, 1706), (8, 1634), (9, 1625), (2, 1612), (3, 1559)] \n",
      " max length:  451 \n",
      " min length:  1\n"
     ]
    }
   ],
   "source": [
    "# find the average token length in the corpus \n",
    "sentence_token_lengths = [] \n",
    "for book in bookscollection.find():\n",
    "    # find the number of characters in each sentence within the corpus \n",
    "    sentence_token_lengths.extend(len(sentence.split()) for sentence in book[\"selected_sents\"])\n",
    "\n",
    "sentence_token_lengths_counter = collections.Counter(sentence_token_lengths)\n",
    "print(\"10 most frequent lengths: \", sentence_lengths_counter.most_common(10), \"\\n\",\n",
    "      \"max length: \", max(sentence_token_lengths_counter), \"\\n\",\n",
    "      \"min length: \", min(sentence_token_lengths_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataScienceProject-Session1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
