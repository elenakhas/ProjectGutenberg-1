{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SXRp4LcnC4I"
   },
   "source": [
    "# Extracting and storing RDF information and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preliminaries: dependencies and seed state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7wQupQrnC4J"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipedia\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import csv, datetime, time, random, string, re, urllib, os, collections  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from urllib.error import HTTPError\n",
    "import pickle \n",
    "with open('randomstate.pickle', 'rb') as f:\n",
    "    random.setstate(pickle.load(f)) \n",
    "# We use random sampling in some of the functions of our program. \n",
    "# To ensure we can replicate the same dataset, we include the use\n",
    "# of the same seed state whenever running this corpus builder.\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from bson import Binary, Code\n",
    "from bson.json_util import dumps,loads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIVWTPMRexID"
   },
   "source": [
    "### General overview\n",
    "\n",
    "Our solution is supported via two classes, their attributes and associated functions. The first class - GutenbergCorpusBuilder - is intended to handle author-title mining on the Project Gutenberg website, filtering and selection as well as storage of overall corpus data. The attributes of this class is designed for easy ingestion into a mongoDB, or similar non-relational, database. The other class - Author - is intended to hold the processes for accessing text files for books, processing them and storing them. The Author class also contains methods for collecting \n",
    "\n",
    "To build a corpus, a user will only need to interact with 2 methods from the GutenbergCorpusBuilder. These are namely, in the order of intended use: (i) get_library; and (ii) populate_corpus. \n",
    "\n",
    "The first method - __get_library__ - will crawl all of the 'Browse by Author' pages on the Project Gutenberg website and collect author information (including books he/she authored as well as wikipedia pages). By passing the 'min_book', 'max_book' as well as 'languages' and 'roles' parameters, the user can balance the content of the corpus in terms of author and book numbers, as well as have it filtered based on language(s) and author role(s). We note in particular, that the author role setting could be become significant for certain machine learning tasks (for instance incorporating books where an author is merely an editor or contributor could lead to degraded model performance for an author-genre classification task). \n",
    "\n",
    "The default values for this function, as well as our setting for the corpus generated are as follows: \n",
    "\n",
    "|Parameter\t|Default setting\t|Setting for this corpus\t|\n",
    "|---\t|---\t|---\t|\n",
    "|min_books   \t|1   \t|3   \t|\n",
    "|max_books   \t|float(inf)   \t|30   \t|\n",
    "|languages   \t|'all'   \t|['english']   \t|\n",
    "|roles   \t|'all'   \t|['as author']   \t|\n",
    "\n",
    "The parameters, their types and default settings are designed with the intention to allow the collection of all books available on Project Gutenberg. For the purpose of our collected corpus, we have chosen the parameters so as to obtain a balanced corpus that selects major as well as minor authors. \n",
    "\n",
    "\n",
    "The second method - __populate_corpus__ - will take the pre-filtered list of author and their books, instantiate a Author object, and begin collecting carefully data on the author and his/her books. At the background, the function will first retrieve all of the literary movements the author is associated with, from DBpedia. The primary information bottleneck lies with these literary movement labels - not all authors have DBpedia pages and for those that do, many do not have literary movements associated to them. As such, we only proceed with the next steps of adding an author to the final corpus if he/she has these literary movement labels. For authors that pass this filter, the method proceeds to extracts and stores the multilingual abstracts for the author. Finally, it proceeds to randomly pick a set of the author's book's (the size of this set is the same for all authors and equivalent to the min_books set for the corpus), and clean and segment the text files in a list of sentences. The cleaning is intended to exclude boilerplate Project Gutenberg metadata as well as book publisher information. An option is provided to clean the text files by exclude the Project Gutenberg metadata more precisely, albeit this extends the time required to process and collect the corpus.  \n",
    "\n",
    "The default values for this function, as well as our setting for the corpus generated are as follows: \n",
    "\n",
    "|Parameter\t|Default setting\t|Setting for this corpus\t|\n",
    "|---\t|---\t|---\t|\n",
    "|sent_num   \t|250   \t|250   \t|\n",
    "|precise_clean   \t|False   \t|True   \t|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUOWBinlnC4Z"
   },
   "source": [
    "### 1. A class to store a corpus obtained from the Project Gutenberg website. \n",
    "\n",
    "The corpus is build with functions within the class that filter the authors and books on the Project Gutenberg website. It also calls on the Author class (below), to process and generate information about sentences from an author's books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfplnCWJnC4Z"
   },
   "outputs": [],
   "source": [
    "class GutenbergCorpusBuilder: \n",
    "    '''\n",
    "    initiates a GutenbergCorpusBuilder object which stores information about selected authors that are found on \n",
    "    the Project Gutenberg(PG) website.\n",
    "    Authors are stored based on their unique PG numerical code. For each author, selected books and their \n",
    "    respective PG URL are stored.\n",
    "    \n",
    "    Inputs: corpusname - string representing name of the corpus being created.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, corpusname):\n",
    "        self.corpusname = corpusname\n",
    "        self.corpusversion = \"v\"+ str(datetime.datetime.now().year) + str(datetime.datetime.now().month) +\\\n",
    "        str(datetime.datetime.now().day)\n",
    "        \n",
    "        # a dictionary containing dictionaries.\n",
    "        # The top level keys - unique numbers for authors on the PG website,\n",
    "        # the values -  dictionaries containing author information: \n",
    "        # keys - 'authorname', 'books_info'; wiki_info'; \n",
    "        # values - strings or embedded dictionaries:\n",
    "        # authorname: string with extracted name;\n",
    "        # books_info: dictionary: key - book ID, value - book title;\n",
    "        # wiki_info: dictionary: key - language; value - wiki link extracted from PG\n",
    "        #\n",
    "        # {author ID: {authorname:'name', books_info:{bookID: book title}, wiki_info: {language: wiki link}}}\n",
    "        self.authors = dict()\n",
    "       \n",
    "        # a dictionary containing sets of sentences selected from each author's filtered books;\n",
    "        # the top level keys are the unique numbers for authors, the values are sets containing\n",
    "        # sentences from an author's book (as strings).\n",
    "        # {author ID: Author()}\n",
    "        self.corpus = dict()\n",
    "        \n",
    "        self.min_books = int # the min_books value passed into the get_library method.\n",
    "        self.max_books = int # the max_books value passed into the get_library method.\n",
    "        \n",
    "    def populate_corpus(self, sent_num=250, precise_clean=False):\n",
    "        '''\n",
    "        for each author in self.authors, generates an Author() class instance, populates all \n",
    "        attributes of the Author() class, adds to self.corpus.\n",
    "        \n",
    "        Inputs | sent_num: int - the total number of sentences to collect for a single author selected for the corpus. \n",
    "        '''\n",
    "        if len(self.authors) > 0:\n",
    "            _counter = 0\n",
    "            for authornum in self.authors: # authornum is a key (an author's unique number)\n",
    "                if authornum not in self.corpus.keys():\n",
    "                    # instantiate an Author()\n",
    "                    authorname = self.authors[authornum][\"authorname\"]\n",
    "                    authorwiki_info = self.authors[authornum][\"wiki_info\"].copy() \n",
    "                    authorbooks_info_keys = list(self.authors[authornum][\"books_info\"].copy().keys())\n",
    "                    \n",
    "                    _author = Author(authorname=authorname, authornum=authornum, \n",
    "                                    min_books=self.min_books)\n",
    "                    \n",
    "                    # run the populate_attributes() to extract and process the information for the author\n",
    "                    _author.populate_attributes(authorwiki_info=authorwiki_info, \n",
    "                                                authorbooks_info_keys=authorbooks_info_keys,\n",
    "                                                sent_num=sent_num, precise_clean=precise_clean)\n",
    "                    \n",
    "                    # store to Author() to corpus \n",
    "                    self.corpus[authornum] = _author\n",
    "\n",
    "                    # append wiki abstract info and literary movement to author's dictionary in \n",
    "                    # self.author so as to easily transmit each author's basic information into mongodb\n",
    "                    self.authors[authornum][\"authorabstracts\"] = _author.authorabstracts\n",
    "                    self.authors[authornum][\"literarymovements\"] = _author.literarymovements\n",
    "                _counter += 1\n",
    "                if _counter%100 == 0:\n",
    "                    print(\"{} authors have been processed, out of {} authors in selections\".format(_counter, len(self.authors)))\n",
    "        else: \n",
    "            print(\"The authors attribute is empty, please run get_library first or \\\n",
    "            check the parameters passed into get_library.\")\n",
    "        \n",
    "    \n",
    "    def get_library(self, min_books=1, max_books=float(\"inf\"), languages = \"all\", roles = \"all\"):\n",
    "        '''\n",
    "        Goes through the PG website's 'sort by author' pages. Extracts author and corresponding book \n",
    "        information that meet a number of selection criterion (see inputs). \n",
    "        \n",
    "        Inputs | \n",
    "        1. min_books: int - the minimum number of books available for an author, which meets the languages \n",
    "        and roles parameters. Default value is 1. \n",
    "        5. languages: either a str \"all\", or a list containing the languages (in lowercase) to count towards \n",
    "        the author's min_books level. The list of languages available can be found here \n",
    "        https://www.gutenberg.org/catalog/. Default is \"all\". \n",
    "        6. roles: either a str \"all\", or a list containing the roles that an author can have in a book. \n",
    "        These include: Commentator, Translator, Contributor, Photographer, Illustrator, Editor.\n",
    "        Default value is \"all\".\n",
    "        Outputs | saves the results to self.authors\n",
    "        '''\n",
    "        charlist = []\n",
    "        charlist[:0] =  [letter for letter in string.ascii_lowercase] + [\"other\"]\n",
    "\n",
    "        library = dict()\n",
    "        for char in charlist:\n",
    "            # Team comment: we select the authors and books via the \"Browse by Author\" lists instead of  \n",
    "            # the \"Browse by Books\" list. Although the latter has a more predictable page structure \n",
    "            # (i.e. 1 book name, followed by 1 author name, recursively), the former includes \n",
    "            # information about the Author's role in the book. We believe that this could have\n",
    "            # a meaningful impact on the predictive capabilities for models on different tasks, \n",
    "            # especially at larger scale.\n",
    "            \n",
    "            link = 'https://www.gutenberg.org/browse/authors/'+ char\n",
    "            page = requests.get(link)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            one_letter = self._unite_authors_nums_books(self._get_authors_numsnames(soup)[0],\\\n",
    "                                                            self._get_authors_numsnames(soup)[1],\\\n",
    "                                                            self._get_bookswiki_info(soup)[0],\\\n",
    "                                                            self._get_bookswiki_info(soup)[1],\\\n",
    "                                                            min_books, max_books, languages, roles)\n",
    "            \n",
    "            library.update(one_letter)\n",
    "            print(\"{} authors from the '{}' alphabetical category have been added.\".format(len(one_letter),char))\n",
    "            \n",
    "            # del variable to clear memory\n",
    "            del soup\n",
    "            \n",
    "            # Put the function to sleep for a randomised number of seconds (non-integer number between \n",
    "            # 0.5 and 4) to mimic human surfing patterns.\n",
    "            time.sleep(random.uniform(0.5,4))\n",
    "            \n",
    "        self.authors = library\n",
    "        self.min_books = min_books\n",
    "        self.max_books = max_books\n",
    "    \n",
    "    def _get_authors_numsnames(self, soup):\n",
    "        '''\n",
    "        A helper function for _unite_authors_nums_books. Extracts all author names from a BeautifulSoup \n",
    "        copy  of a 'Browse by Author' page on the PG website. \n",
    "        \n",
    "        Inputs | soup:a BeautifulSoup object - containg a copy of the PG 'Browse by Author' page. \n",
    "        Outputs | a tuple containing two lists: The first contains author's numbers on the page, the \n",
    "        second contains corresponding author's names on the page. \n",
    "        '''\n",
    "        authornames = []\n",
    "        # the author names are stored within the \"name\" attribute under each \"a\" class\n",
    "        # use regex wildcard so that find_all will catch and return all \"a names\" with values\n",
    "        authorname_BSlist = soup.find_all('a', {\"name\":re.compile(\"\\w*\")})\n",
    "\n",
    "        for authorname in authorname_BSlist:\n",
    "            # \\- and \\? to escape special characters. .rstrip to remove trailing whitespaces. \n",
    "            authornames.append(re.sub(r'[0-9,\\-\\?]*', '', authorname.text).rstrip())\n",
    "\n",
    "        authornums = []\n",
    "        # the author numbers are stored within the \"href\" attribute. Every line for a book \n",
    "        # on the page has a \"title\" attribute with the value \"Link to this author\". We will use\n",
    "        # this to shift to only the lines with the author's number. \n",
    "        authornums_BSlist = soup.find_all('a', {\"title\":\"Link to this author\"})\n",
    "\n",
    "        for authornum in authornums_BSlist:\n",
    "            authornums.append(authornum[\"href\"].lstrip(\"#\"))\n",
    "\n",
    "        return authornums, authornames\n",
    "\n",
    "    def _get_bookswiki_info(self, soup):\n",
    "        '''\n",
    "        A helper function for _unite_authors_nums_books. Extracts all the book titles and numbers from a \n",
    "        BeautifulSoup copy of a 'Browse by Author' page on the PG website. Also extracts author wikipedia \n",
    "        link information if it is available on the PG website. \n",
    "        \n",
    "        Inputs | soup:a BeautifulSoup object - containg a copy of the PG 'Browse by Author' page. \n",
    "        Outputs | a tuple containing two lists. \n",
    "          1. The first list contains dictionaries. Each dictionary contains information about an author's \n",
    "          books on PG. this includes: book titles, corresponding PG books numbers, the author's role in \n",
    "          each book, and the language of each book. \n",
    "          2. The second list contains dictionaries. Each dictionary contains information about an author's \n",
    "          wikipedia links on PG. An author's wiki dictionary may be empty, contain 1 link, or more than 1 \n",
    "          link. \n",
    "        '''\n",
    "        books_info = list()\n",
    "        wiki_info = list()\n",
    "\n",
    "        # content under the 'ul' tags: books, links as one list organized by ul\n",
    "        authorsbooks_BSlist = soup.find_all('ul')\n",
    "        # for each ul, access the content: books, links; each book is a bs object\n",
    "\n",
    "        for author in authorsbooks_BSlist:\n",
    "            # there are two classes of attributes within each ul tag. The book information\n",
    "            # 1. title and book PG number is under the 'pgdbetext' class. \n",
    "            books_BSlist = author.find_all(class_='pgdbetext')\n",
    "\n",
    "            authorbooks_info = {}\n",
    "            for book in books_BSlist:\n",
    "                # the book numbers are stored in the href attribute. e.g. \"ebooks/19323\"\n",
    "                booknum = book.find('a')['href'].split(\"/\")[-1]\n",
    "                PG_booktitle = book.text\n",
    "\n",
    "                # storing the information regarding a single author's books in a dictionary\n",
    "                authorbooks_info[booknum]=PG_booktitle\n",
    "            \n",
    "            # appending the dictionary containing one author's books to a list\n",
    "            books_info.append(authorbooks_info)\n",
    "            \n",
    "            # 2. for the author is/are under the 'pgdbxlink' class. \n",
    "            wiki_BSlist = author.find_all(class_='pgdbxlink')\n",
    "\n",
    "            authorwiki_info = {}\n",
    "\n",
    "            for wiki in wiki_BSlist:\n",
    "                # 1. the wiki links are stored in the href attribute. \n",
    "                PG_wikilink = wiki.find('a')['href'] # get the whole link\n",
    "                \n",
    "                # some of the lines tagged \"pgdbxlink\" include \"See also: xxx\" links. \n",
    "                # we filter them out here\n",
    "                if \"wikipedia.org\" in PG_wikilink:\n",
    "\n",
    "                    # 2. because PG stores the link in URL-safe format (e.g. \"\\x\" is \"%\"), we will face \n",
    "                    # issues with non-ASCII characters e.g. á whose URL-safe encoding cannot be passed \n",
    "                    # into the wikipedia package. use urllib.requests.unquote to resolve this \n",
    "                    # https://docs.python.org/2/library/urllib.html#utility-functions \n",
    "                    PG_wikilink = urllib.request.unquote(PG_wikilink)\n",
    "\n",
    "                    # 3. get the language code for the wikipage\n",
    "                    wikilang = re.findall(r'/\\w+', PG_wikilink)[0].strip('/')\n",
    "                    # storing the information regarding a single author's wikipedia links in a dictionary\n",
    "                    authorwiki_info[wikilang] = PG_wikilink\n",
    "\n",
    "            # appending the dictionary containing one author's wikipedia links to a list\n",
    "            wiki_info.append(authorwiki_info)\n",
    "            \n",
    "        return books_info, wiki_info\n",
    "\n",
    "    \n",
    "    def _unite_authors_nums_books(self, authornums, authornames, books_info, wiki_info, min_books, \n",
    "                                  max_books, languages, roles):\n",
    "        '''\n",
    "        A helper function for get_library. \n",
    "        \n",
    "        Inputs | \n",
    "        1. authornums:list - list of author numbers from a \"sort by author\" page on the PG website. \n",
    "        2. authornames:list - list of author names  from a \"sort by author\" page on the PG website. \n",
    "        3. books_info: list - a list containing dictionaries, each of which has information about \n",
    "        an author's books \n",
    "        4. wiki_info: list - a list containing dictionaries, each of which has information about \n",
    "        an author's wikipedia\n",
    "        page, as provided by the PG website. There may be none, one, or more wikilinks for an author. \n",
    "        5. min_books:int - the minimum number of books available for an author, which meets the languages \n",
    "        and roles parameters. default value is 1 (since an author listed on PG will have at least 1 book \n",
    "        to his name).\n",
    "        6, max_books:int - the minimum number of books available for an author, which meets the languages \n",
    "        and roles parameters. default value is infinity.\n",
    "        7. languages:either a str \"all\", or a list containing the languages (in lowercase) to count towards the author's \n",
    "        min_books level. The list of languages available can be found here \n",
    "        https://www.gutenberg.org/catalog/. default is \"all\". \n",
    "        8. roles: either a str \"all\", or a list containing the roles (in lowercase) that an author can \n",
    "        have in a book. These include: commentator, translator, contributor, photographer, illustrator, \n",
    "        commentator, editor. default value is \"all\".\n",
    "        Outputs | a dictionary containing PG numbers for authors who meet the min_books, languages and \n",
    "        roles requirements, as well as information each of these author's books. \n",
    "        '''\n",
    "        # we want to be sure that the authornums, authornames, books_info, and wiki_info are aligned \n",
    "        # before proceeding to merge them. \n",
    "        try:\n",
    "            assert len(authornums)==len(authornames) and len(authornums)==len(books_info) and len(authornums)==len(wiki_info)\n",
    "        except AssertionError as e:\n",
    "            e.args += (\"The length of authornums, authornames and books_info do not match.\",)\n",
    "            raise\n",
    "            \n",
    "        authorbooks_info = dict()\n",
    "        # if default parameters passed into the function, add all authors and their books to the corpus.  \n",
    "        if min_books == None and languages == \"all\" and roles == \"all\":\n",
    "            for i in range(len(authornums)):\n",
    "                authorbooks_info[authornums[i]]=\\\n",
    "                        {\"authorname\": authornames[i], \"books_info\": books_info[i], \"wiki_info\": wiki_info[i]}\n",
    "        else:\n",
    "            # place languages and roles input in sets, for use in .intersection below. \n",
    "            languages_set = set(languages)\n",
    "            roles_set = set(roles)\n",
    "            \n",
    "            for i in range(len(authornums)):\n",
    "                author_bookset = books_info[i]\n",
    "                _topop = []\n",
    "                for book in author_bookset: \n",
    "                    \n",
    "                    # using regex to find text in parentheses. Book language e.g. (English) and author role \n",
    "                    # e.g. (as Author) are contained in parentheses. Some books which are part of a series, \n",
    "                    # have (of N) in their titles too, where N is the number of books in that series. \n",
    "                    title_text_in_parentheses =\\\n",
    "                    re.findall(r'\\(([a-zA-Z]+\\s*[a-zA-Z]*[0-9]*)\\)', author_bookset[book])\n",
    "                    \n",
    "                    # lowercase the text in parentheses and put it into sets. \n",
    "                    _title_text_in_parentheses =\\\n",
    "                    set([i.lower() for i in title_text_in_parentheses])\n",
    "                    \n",
    "                    # if languages is set to \"all\" or if the intersection of _title_text_in_parentheses\n",
    "                    # and languages_set returns a non-empty set, pass to the next check. Otherwise add this \n",
    "                    # book number to the list of books to pop from this author_bookset\n",
    "                    if languages == \"all\" or _title_text_in_parentheses.intersection(languages_set): pass\n",
    "                    else:\n",
    "                        _topop.append(book) \n",
    "                        continue \n",
    "                    # do the same for author's role as for language above\n",
    "                    if roles == \"all\" or _title_text_in_parentheses.intersection(roles_set): pass\n",
    "                    else:\n",
    "                        _topop.append(book) \n",
    "                        continue    \n",
    "                # pop the books that don't meet the language and role specifications. \n",
    "                for pop in _topop:\n",
    "                    books_info[i].pop(pop)\n",
    "                    \n",
    "                # check if number of books meeting the language and role requirements meet the \n",
    "                # min_book requirement \n",
    "                if min_books <= len(books_info[i]) <= max_books:\n",
    "                    authorbooks_info[authornums[i]]=\\\n",
    "                            {\"authorname\": authornames[i], \"books_info\": books_info[i], \n",
    "                             \"wiki_info\": wiki_info[i]}\n",
    "                    \n",
    "        return authorbooks_info \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"There are {} authors entered in this corpus\".format(len(self.corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1WRLJZBtpZK"
   },
   "source": [
    "### 2. A class to store subcorpora obtained from the Project Gutenberg website for each Author. \n",
    "\n",
    "The subcorpus is build with functions within the class that pre-processes each .txt file for filtered author books on the Project Gutenberg website. It also obtains the abstracts and literary movement tags for each author from Wikipedia and DBPedia respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m96uH1j8mzXl"
   },
   "outputs": [],
   "source": [
    "class Author:\n",
    "    '''\n",
    "    Initiates a Author object which stores information about a selected author that is available on \n",
    "    the Project Gutenberg(PG) website. Other information drawn from (i) DBPedia - author literary movements\n",
    "    (ii) wikipedia - multilingual author abstract, (iii) PG - selected sentences from author's texts \n",
    "    \n",
    "    Inputs: authorname: str, authornum:str, authorwiki_info: dict, authorbooks_info_keys:list of numbers in \n",
    "    string, min_books: int\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, authorname, authornum, min_books):\n",
    "        '''\n",
    "        initiates the Author object with the author's name. \n",
    "        \n",
    "        Inputs | authorname: str, authornum:str, authorwiki_info: dict, \n",
    "        authorbooks_info_keys:list of author numbers (in str), min_books: int\n",
    "        '''\n",
    "        self.name = authorname\n",
    "        self.number = authornum\n",
    "        self.min_books = min_books  # the min_book setting at the GutenbergCorpus class that led\n",
    "                                    # to this author's selection for the corpus\n",
    "        \n",
    "        # a dictionary with the book numbers as keys and lists as values. Lists  \n",
    "        # contain strings that have been pre-processed by the segment_sentence method.\n",
    "        self.processed_subcorpus = dict()       \n",
    "        \n",
    "        self.authorabstracts = dict() \n",
    "        self.literarymovements = list()\n",
    "        \n",
    "        \n",
    "    def populate_attributes(self, authorwiki_info, authorbooks_info_keys, sent_num, precise_clean):\n",
    "        '''\n",
    "        A convenience function to call _build_subcorpus, _get_authorabstract and  _get_literarymovement, \n",
    "        which will respectively populate the processed_subcorpus, authorabstract and literarymovements\n",
    "        attributes for this Author instance.  \n",
    "        \n",
    "        Inputs | authorwiki_info: dict, authorbooks_info_keys:list of numbers (in str), min_books: int\n",
    "        Result | stores results to self.literarymovements, self.authorabstracts and self.processed_subcorpus \n",
    "        '''\n",
    "        # check for /data directory, else create for storing files from _build_subcorpus\n",
    "        if not os.path.isdir('./data'):  \n",
    "            os.mkdir(\"data\")\n",
    "        \n",
    "        self._get_literarymovement(authorwiki_info)\n",
    "        \n",
    "        # the information bottleneck is at the dbpedia literary movement labels.\n",
    "        # multilingual wiki abstract and text processing requires a large amount of resources \n",
    "        # so we only do these for authors that we manage to get literary movements for.\n",
    "        if len(self.literarymovements) > 0: \n",
    "            self._get_authorabstract(authorwiki_info)\n",
    "            self._build_subcorpus(authorbooks_info_keys=authorbooks_info_keys, \n",
    "                                  sent_num=sent_num, precise_clean=precise_clean)\n",
    "        \n",
    "    \n",
    "    def _build_subcorpus(self, authorbooks_info_keys, sent_num, precise_clean):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. Selects the books of an author's to extract sentences\n",
    "        from. the number of books is the same as min_book set for the corpus's author selection criteria.\n",
    "        if an author has more books than min_books, a random sampling is done. a basic pre-processing to \n",
    "        remove PG metadata and publisher information is done next. results are written to two sets of csv files. \n",
    "        the first contains only selected sentences, the second contains the entire processed book. Additionally, \n",
    "        the selected sentences for each book are written to plaintext files. \n",
    "        \n",
    "        Inputs | authorbooks_info_keys:list of numbers (in str), min_books: int\n",
    "        Result | saves to two sets of csv files: (i) selected sentences only; (ii) entire pre-processed \n",
    "        book. selected sentences also saved to plaintext files. also stores (i) to self.processed_subcorpus\n",
    "        '''\n",
    "        \n",
    "        _author_cleanbooks = dict()\n",
    "        \n",
    "        if len(authorbooks_info_keys) == self.min_books: \n",
    "            for booknum in authorbooks_info_keys: \n",
    "                all_sentencesinbook =\\\n",
    "                self._cleansegment_book(booknum=booknum, precise_clean=precise_clean)\n",
    "\n",
    "                if len(all_sentencesinbook) > sent_num/self.min_books:\n",
    "                    _author_cleanbooks[booknum] = all_sentencesinbook\n",
    "        else: \n",
    "            # 1. recursively select a number of books until len(_author_cleanbooks) matches min_books\n",
    "            #    at each recursion, apply _cleansegment_book on the book. If the cleaned book meets the \n",
    "            #    length requirement, add to _author_cleanbooks. Do this for up to 10 tries, failing\n",
    "            #    which we will exclude the author and all of his/her books from the corpus.\n",
    "            _tries = 0\n",
    "            _unvisited = set(authorbooks_info_keys)\n",
    "            while len(_author_cleanbooks) < self.min_books and _tries<=10 and len(_unvisited) > 0:\n",
    "                # randomly select min_books number from author. if author only has min_books, \n",
    "                # sampling will return the same set\n",
    "                try: # try to take a random sample (it could fail if _unvisited < sample size)\n",
    "                    _newnums = set(random.sample(_unvisited, self.min_books-len(_author_cleanbooks)))\n",
    "                    _unvisited = _unvisited.difference(_newnums)\n",
    "                    for booknum in _newnums: \n",
    "                        all_sentencesinbook =\\\n",
    "                        self._cleansegment_book(booknum=booknum, precise_clean=precise_clean)\n",
    "\n",
    "                        if len(all_sentencesinbook) > sent_num/self.min_books:\n",
    "                            _author_cleanbooks[booknum] = all_sentencesinbook\n",
    "                    _tries += 1\n",
    "                except: # break the while loop if sampling fails\n",
    "                    break \n",
    "                                \n",
    "        # 2. if min_books still not met, move to return. this effectively excludes author from corpus \n",
    "        if len(_author_cleanbooks) < self.min_books:\n",
    "            return \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # 3. extract k number of sentences from each accepted author's books. k is the total number\n",
    "        #   of sentences required for each author divided by the author min_books set for the corpus \n",
    "        _authors_sentences = dict()\n",
    "        for booknum in _author_cleanbooks: \n",
    "            _sample =\\\n",
    "            random.sample(_author_cleanbooks[booknum], round(sent_num/self.min_books))\n",
    "            # book to temporary dictionary, with the booknum as the key. \n",
    "            _authors_sentences[booknum] = _sample\n",
    "        \n",
    "        # 4. write the cleaned book and sampled sentences to file \n",
    "        for booknum in _authors_sentences: \n",
    "            self._write_tofile(booknum, all_sentencesinbook = _author_cleanbooks[booknum], \n",
    "                               sample_all_sentencesinbook = _authors_sentences[booknum])\n",
    "        \n",
    "        # 5. update the processed_subcorpus attribute for the author, with the sampled sentences\n",
    "        self.processed_subcorpus.update(_authors_sentences)\n",
    "        \n",
    "        \n",
    "    def _write_tofile(self, booknum, all_sentencesinbook, sample_all_sentencesinbook): \n",
    "        '''\n",
    "        A helper function to export processed texts and lists of sentences to csv and plaintext files. \n",
    "        Called by _build_subcorpus. \n",
    "        '''\n",
    "        # 1. write the entire cleaned and segmented book to a csv file. \n",
    "        if not os.path.isdir('./data/wholebook_csv'):\n",
    "            os.mkdir(\"data/wholebook_csv\")\n",
    "        with open(\"./data/wholebook_csv/\"+self.number+\"_\"+booknum+'.csv', 'a') as csv_file:\n",
    "            # we set file open mode to 'a' to append to file instead of overwriting\n",
    "            write_file = csv.writer(csv_file, dialect = 'excel')\n",
    "            write_file.writerow(all_sentencesinbook)\n",
    "            del csv_file # delete to free memory\n",
    "\n",
    "        # 2a. write the book sample to a csv file\n",
    "        if not os.path.isdir('./data/booksample_csv'):\n",
    "            os.mkdir(\"data/booksample_csv\")\n",
    "        with open(\"./data/booksample_csv/\"+self.number+\"_\"+booknum+'.csv', 'a') as csv_file:\n",
    "            write_file = csv.writer(csv_file, dialect = 'excel')\n",
    "            write_file.writerow(sample_all_sentencesinbook)\n",
    "            del csv_file # delete to free memory\n",
    "\n",
    "        # 2b. write the book sample to a txt file\n",
    "        if not os.path.isdir('./data/booksample_txt'):\n",
    "            os.mkdir(\"data/booksample_txt\")\n",
    "        with open(\"./data/booksample_txt/\"+self.number+\"_\"+booknum+'.txt', 'a') as txt_file:\n",
    "            txt_file.writelines(\"\\t\".join(sample_all_sentencesinbook))\n",
    "            del txt_file # delete to free memory\n",
    "\n",
    "            \n",
    "    def _cleansegment_book(self, booknum, precise_clean, \n",
    "                           urlpath = \"https://www.gutenberg.org/files/{}/{}.txt\"):\n",
    "        '''\n",
    "        takes a booknum, navigates to the PG page with the .txt file for this book. uses urlopen to \n",
    "        retrieve the contents of this file. if precise_clean = False, only retrieves lines between the\n",
    "        last \"*START\" and first \"*END\" line in the file. \n",
    "        \n",
    "        Inputs | booknum: int - the unique number on PG for a book, urlpath: str - the url structure for a book's  \n",
    "        page on PG, precise_clean: boolean \n",
    "        Outputs | all_sentencesinbook: list -  a list of sentences after the basic pre-processing \n",
    "        '''    \n",
    "        book_content = []\n",
    "        \n",
    "        # open target_url with the urllib.request.urlopen() method,\n",
    "        # for each line in response, decodes with the expected \n",
    "        # encoding format PG uses for plain .txt book files. \n",
    "        # see https://www.gutenberg.org/wiki/Gutenberg:Readers%27_FAQ#R.35._What_do_the_filenames_of_the_texts_mean.3F\n",
    "        for extenc_pair in [('', 'ascii'), ('-0', \"utf-8\"), ('-8', 'ISO 8859-1')]: \n",
    "            # iterate through likely filename endings and associated encodings on PG\n",
    "            try: \n",
    "                target_url = urlpath.format(booknum,booknum+extenc_pair[0])\n",
    "                with urllib.request.urlopen(target_url) as response: \n",
    "                    for line in response: \n",
    "                        # urlopen reads as bytes, to ease processing, we decode to string.\n",
    "                        # most PG .txt files are encoded in latin-1/ascii format. \n",
    "                        try:\n",
    "                            book_content.append(line.decode(extenc_pair[1]))\n",
    "                        except: # revert to latin-1 in the event of unexpected PG encoding behaviour \n",
    "                            book_content.append(line.decode(\"latin-1\"))\n",
    "                    response.close()\n",
    "                    del response\n",
    "            except HTTPError: \n",
    "                continue\n",
    "                \n",
    "        # remove PG metadata precisely, but slower to execute\n",
    "        if precise_clean == True: \n",
    "            start_index = 0                # index for the start of the text\n",
    "            stop_index = -1  # index for the end of the text  \n",
    "\n",
    "            # Each PG book .txt file is ended with metadata marked with \"* START\" and \"* END\" or \n",
    "            # minor variations. * START-tagged metadata tend to, but don't always just, appear in \n",
    "            # the first 25% of the .txt file, and vice-versa for * END tagged metadata. we split \n",
    "            # the file in the top and bottom thirds and run searches for * START and * END (for \n",
    "            # some savings in search time)\n",
    "            \n",
    "            _2third_marker = round(len(book_content)*0.67)\n",
    "                                         \n",
    "            #1. search for *END tags from the back of the file, for two-thirds of the file\n",
    "            for index_num in range(_2third_marker):\n",
    "                if re.match(r'\\*+\\s*END ', book_content[-index_num]):\n",
    "                    stop_index = -index_num\n",
    "            \n",
    "            #2. search for anomalous *START tags in the last two-thirds of the file, \n",
    "            #   but begining from the, possibly new, stop_index \n",
    "            for index_num in range(-stop_index, _2third_marker):\n",
    "                # searching for the last * END from the back, in the last half of the file \n",
    "                if re.match(r'\\*+\\s*START ', book_content[-index_num]):\n",
    "                    stop_index = -index_num\n",
    "            \n",
    "            #3. finally, search for the last START tag from the front, within the first two-thirds\n",
    "            for index_num in range(_2third_marker):\n",
    "                # searching for the last * START in the first half of the file \n",
    "                if re.match(r'\\*+\\s*START ', book_content[index_num]):\n",
    "                    start_index = index_num \n",
    "            \n",
    "            # slicing the section of the text between the start_index and stop_index. \n",
    "            book_content = book_content[start_index:stop_index]\n",
    "\n",
    "        # join all the text without \"\\r\\n\" i.e. return carriage and newline \n",
    "        clean_book_content = \" \".join([l.strip(\"\\r\\n\") for l in book_content if l != \"\\r\\n\"])\n",
    "        # use nltk's sent_tokenise\n",
    "        all_sentencesinbook = sent_tokenize(clean_book_content)\n",
    "\n",
    "        # strip first and last 10% of lines (as a buffer to avoid collecting generic publishing data)\n",
    "        _10pc = round(len(all_sentencesinbook)*0.10)\n",
    "        all_sentencesinbook = all_sentencesinbook[_10pc:-_10pc]\n",
    "\n",
    "        return all_sentencesinbook\n",
    "\n",
    "\n",
    "    def _get_authorabstract(self, author_wiki_info):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. Gets available author abstract from wikipedia using\n",
    "        the wikipedia python package. \n",
    "        \n",
    "        Input | authorwiki_info: dict\n",
    "        Result | stores results to self.authorabstracts\n",
    "        '''\n",
    "        _abstracts = {}\n",
    "\n",
    "        for wikilang in author_wiki_info: \n",
    "        # set the language \n",
    "            wikipedia.set_lang(wikilang)\n",
    "            wikiname = author_wiki_info[wikilang].split(\"/\")[-1]\n",
    "\n",
    "            try: # without disambiguation: we start with the presumption that PG has \n",
    "                # accurate author wikipedia links. set auto_suggest to False to prevent \n",
    "                # additional (unnecessary) handling of the author page name by the wikipedia package.\n",
    "                wikipage = wikipedia.page(title=wikiname, auto_suggest=False)\n",
    "                _abstracts[wikilang] = wikipage.summary\n",
    "\n",
    "            except PageError: \n",
    "                print(\"There is a PageError resulting with this wikiname: {}\".format(wiki_name) )\n",
    "                pass \n",
    "            except DisambiguationError: \n",
    "                print(\"There is a DisambiguationError resulting with this wikiname: {}\".format(wiki_name))\n",
    "                pass \n",
    "\n",
    "        self.authorabstracts = _abstracts    \n",
    "    \n",
    "      \n",
    "    def _get_literarymovement(self, authorwiki_info):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. takes an author's name, makes a DBpedia query \n",
    "        with the name using the SPARQLWrapper package, \n",
    "        returns the literary movements that the author is associated with. \n",
    "        \n",
    "        Input | authorwiki_info: dict\n",
    "        Result | stores results to self.literarymovements\n",
    "        '''\n",
    "        if len(authorwiki_info) > 0:\n",
    "            # since dbpedia is based off wikipedia, we will use the author's name as in \n",
    "            # the wikipedia link obtained from PG. \n",
    "            _authorwiki_info = authorwiki_info.copy().popitem()\n",
    "            wikiname = _authorwiki_info[1].split('/')[-1].replace(\"_\", \" \")\n",
    "            wikilang = _authorwiki_info[0]\n",
    "\n",
    "            sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "            query = '''SELECT ?text\n",
    "                WHERE {\n",
    "                ?writer rdf:type dbo:Writer ;\n",
    "                foaf:name %r @%s.\n",
    "                {?writer dbo:genre ?genre .}\n",
    "                UNION\n",
    "                {?writer dbo:movement ?genre .}\n",
    "                ?genre rdfs:label ?text\n",
    "                FILTER (lang(?text) = \"en\")\n",
    "                }''' %(wikiname, wikilang)\n",
    "            # using %r for names to handle non-ascii wikinames that get passed as bytes in %s\n",
    "            # see https://pyformat.info/ for e.g. \"Bahá'u'lláh\" becomes \"Bahá\\'u\\'lláh\"\n",
    "            sparql.setQuery(query)\n",
    "            sparql.setReturnFormat(JSON)\n",
    "            results = sparql.query().convert()\n",
    "            genres = set()\n",
    "            for i in range (len(results['results']['bindings'])):\n",
    "                genre = results['results']['bindings'][i]['text']['value']\n",
    "                genres.add((re.sub(r'\\([^)]*\\)', '', genre.lower())).rstrip())\n",
    "            self.literarymovements = list(genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9jRyG8qz1R1"
   },
   "source": [
    "### 3. Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hzx6BXsmMQMk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 authors from the 'a' alphabetical category have been added.\n",
      "297 authors from the 'b' alphabetical category have been added.\n",
      "212 authors from the 'c' alphabetical category have been added.\n",
      "141 authors from the 'd' alphabetical category have been added.\n",
      "61 authors from the 'e' alphabetical category have been added.\n",
      "122 authors from the 'f' alphabetical category have been added.\n",
      "141 authors from the 'g' alphabetical category have been added.\n",
      "210 authors from the 'h' alphabetical category have been added.\n",
      "16 authors from the 'i' alphabetical category have been added.\n",
      "63 authors from the 'j' alphabetical category have been added.\n",
      "68 authors from the 'k' alphabetical category have been added.\n",
      "148 authors from the 'l' alphabetical category have been added.\n",
      "265 authors from the 'm' alphabetical category have been added.\n",
      "39 authors from the 'n' alphabetical category have been added.\n",
      "47 authors from the 'o' alphabetical category have been added.\n",
      "129 authors from the 'p' alphabetical category have been added.\n",
      "3 authors from the 'q' alphabetical category have been added.\n",
      "117 authors from the 'r' alphabetical category have been added.\n",
      "276 authors from the 's' alphabetical category have been added.\n",
      "89 authors from the 't' alphabetical category have been added.\n",
      "17 authors from the 'u' alphabetical category have been added.\n",
      "35 authors from the 'v' alphabetical category have been added.\n",
      "174 authors from the 'w' alphabetical category have been added.\n",
      "1 authors from the 'x' alphabetical category have been added.\n",
      "17 authors from the 'y' alphabetical category have been added.\n",
      "4 authors from the 'z' alphabetical category have been added.\n",
      "2 authors from the 'other' alphabetical category have been added.\n",
      "100 authors have been processed, out of 2789 authors in selections\n",
      "200 authors have been processed, out of 2789 authors in selections\n",
      "300 authors have been processed, out of 2789 authors in selections\n",
      "400 authors have been processed, out of 2789 authors in selections\n",
      "500 authors have been processed, out of 2789 authors in selections\n",
      "600 authors have been processed, out of 2789 authors in selections\n",
      "700 authors have been processed, out of 2789 authors in selections\n",
      "800 authors have been processed, out of 2789 authors in selections\n",
      "900 authors have been processed, out of 2789 authors in selections\n",
      "1000 authors have been processed, out of 2789 authors in selections\n",
      "1100 authors have been processed, out of 2789 authors in selections\n",
      "1200 authors have been processed, out of 2789 authors in selections\n",
      "1300 authors have been processed, out of 2789 authors in selections\n",
      "1400 authors have been processed, out of 2789 authors in selections\n",
      "1500 authors have been processed, out of 2789 authors in selections\n",
      "1600 authors have been processed, out of 2789 authors in selections\n",
      "1700 authors have been processed, out of 2789 authors in selections\n",
      "1800 authors have been processed, out of 2789 authors in selections\n",
      "1900 authors have been processed, out of 2789 authors in selections\n",
      "2000 authors have been processed, out of 2789 authors in selections\n",
      "2100 authors have been processed, out of 2789 authors in selections\n",
      "2200 authors have been processed, out of 2789 authors in selections\n",
      "2300 authors have been processed, out of 2789 authors in selections\n",
      "2400 authors have been processed, out of 2789 authors in selections\n",
      "2500 authors have been processed, out of 2789 authors in selections\n",
      "2600 authors have been processed, out of 2789 authors in selections\n",
      "2700 authors have been processed, out of 2789 authors in selections\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    min_books = 3\n",
    "    max_books = 30\n",
    "    sent_num = 250 \n",
    "    precise_clean=True\n",
    "    # instantiate a GutenbergCorpusBuilder \n",
    "    PGcorpus = GutenbergCorpusBuilder(corpusname=\"PG-eng-author-min{}\".format(min_books))\n",
    "    # start collecting and filtering author and book details from the Project Gutenberg site\n",
    "    PGcorpus.get_library(min_books = min_books, max_books = max_books, \n",
    "                         languages = [\"english\"], roles = [\"as author\"])\n",
    "    # read text files, select sentences, pre-process sentences, store to subcorpora\n",
    "    PGcorpus.populate_corpus(sent_num=sent_num, precise_clean=precise_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 General check of the content of the corpus files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This selected sentences file ./data/booksample_txt/a32063_26521.txt may still contain Gutenberg metadata\n",
      "This selected sentences file ./data/booksample_txt/a3826_19000.txt may still contain Gutenberg metadata\n",
      "This selected sentences file ./data/booksample_txt/a32063_29948.txt may still contain Gutenberg metadata\n",
      "This selected sentences file ./data/booksample_txt/a8240_50876.txt may still contain Gutenberg metadata\n",
      "There are 336 files in total within the corpus\n"
     ]
    }
   ],
   "source": [
    "# general sense\n",
    "import glob\n",
    "filenames = glob.glob(\"./data/booksample_txt/*.txt\")\n",
    "_to_delete = []\n",
    "for i in filenames: \n",
    "    with open(i) as file:\n",
    "        if \"gutenberg\" in file.read():\n",
    "            _to_delete.append(i)\n",
    "            print (\"This selected sentences file {} may still contain Gutenberg metadata\".format(i))\n",
    "print(\"There are {} files in total within the corpus\".format(len(filenames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__4 out of the 336 files in the corpus, about 1.2%, still contains Gutenberg metadata. These files belong to 3 authors. The Gutenberg metadata likely got selected as part of the sentences from the books because these books' file structure deviate substantially from the typical structure variants we observed. We will manually delete these 3 authors and their books  from the corpus.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/booksample_txt/a32063_26521.txt',\n",
       " './data/booksample_txt/a3826_19000.txt',\n",
       " './data/booksample_txt/a32063_29948.txt',\n",
       " './data/booksample_txt/a8240_50876.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the associated processed_subcorpus stored in the Author instances for the files/authors above\n",
    "for author in [\"a32063\", \"a3826\", \"a8240\"]:\n",
    "    PGcorpus.corpus[author].processed_subcorpus = dict()\n",
    "    \n",
    "# delete the associated processed_subcorpus stored in the /data directory\n",
    "for filename in _to_delete: \n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKrllplDkyqb"
   },
   "source": [
    "### 4. Test code - informal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJCo6Hhxkxn4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 109 authors entered in this corpus\n"
     ]
    }
   ],
   "source": [
    "# check that corpus contains only english books. it should return nothing. \n",
    "for i in PGcorpus.authors.keys(): \n",
    "    for i2 in PGcorpus.authors[i][\"books_info\"]:\n",
    "        if \"English\" not in PGcorpus.authors[i][\"books_info\"][i2]:\n",
    "            print(PGcorpus.authors[i][\"books_info\"][i2])\n",
    "\n",
    "# check that corpus contains only books where author role is as Author. it should return nothing.\n",
    "for i in PGcorpus.authors.keys(): \n",
    "    for i2 in PGcorpus.authors[i][\"books_info\"]:\n",
    "        if \"Author\" not in PGcorpus.authors[i][\"books_info\"][i2]:\n",
    "            print(PGcorpus.authors[i][\"books_info\"][i2])\n",
    "                                     \n",
    "# return number of authors selected into corpus \n",
    "counter = 0\n",
    "for i in PGcorpus.authors.keys():\n",
    "    if len(PGcorpus.corpus[i].processed_subcorpus) ==3:\n",
    "        counter +=1\n",
    "print (\"There are %d authors entered in this corpus\"%counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pickling the GutenbergCorpus object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picklemaker(filename, objectname):\n",
    "    # open the file for writing\n",
    "    fileObject = open(filename,'wb')\n",
    "\n",
    "    pickle.dump(objectname,fileObject)\n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()\n",
    "    \n",
    "picklemaker(PGcorpus.corpusname+PGcorpus.corpusversion+\".pickle\", PGcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a convenience function to load a previously pickled GutenbergCorpusBuilder object\n",
    "def pickleloader(filename):\n",
    "    # # open the file for writing\n",
    "    fileObject = open(filename,'rb')\n",
    "    \n",
    "    return pickle.load(fileObject)  \n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()\n",
    "\n",
    "PGcorpus = pickleloader(\"PG-eng-author-min3v2019419.pickle\")\n",
    "# run/re-run cells containing GutenbergCorpusBuilder and Author class before loading the pickle file\n",
    "# to provide the unpickler attribute structures of the class for unpickling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UOe2Z-guIIG"
   },
   "source": [
    "### 6. pymongo implementation to store the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAZsdVRoz0xA"
   },
   "outputs": [],
   "source": [
    "# instantiate a MongoClient object. using the URI for the Mongo server. If it is locally hosted, \n",
    "# it is by default on the 27017 port. If using cloud, use the provided URI\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "corpusdb = client[\"corpus\"]\n",
    "authorscollection = corpusdb[\"authors\"]\n",
    "bookscollection = corpusdb[\"books\"]\n",
    "\n",
    "authorscollection.drop()\n",
    "bookscollection.drop()\n",
    "\n",
    "# insert the dicationary with each author's information into the authors collection. Only authors \n",
    "# whose books were eventually admitted into the corpus will be added to the database. \n",
    "for authornum in PGcorpus.authors: \n",
    "    PGcorpus.authors[authornum][\"authornum\"]=authornum\n",
    "    authorscollection.insert_one(PGcorpus.authors[authornum])\n",
    "\n",
    "# insert each of the selected books and their selected sentences into the books collection \n",
    "for authornum in PGcorpus.corpus: \n",
    "    author_subcorpus = PGcorpus.corpus[authornum].processed_subcorpus\n",
    "    if len(author_subcorpus) > 0:   \n",
    "        for booknum in author_subcorpus.keys():\n",
    "            bookscollection.insert_one({\"booknum\": booknum, \"selected_sents\": author_subcorpus[booknum]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5cba59b8936c7d234e4da28a'),\n",
       " 'authorname': 'Abbott Edwin Abbott',\n",
       " 'books_info': {'97': 'Flatland: A Romance of Many Dimensions (English) (as Author)',\n",
       "  '45506': 'Flatland: A Romance of Many Dimensions (English) (as Author)',\n",
       "  '201': 'Flatland: A Romance of Many Dimensions (Illustrated) (English) (as Author)',\n",
       "  '22600': 'How to Write Clearly: Rules and Exercises on English Composition (English) (as Author)',\n",
       "  '54223': 'Onesimus: Memoirs of a Disciple of St. Paul (English) (as Author)',\n",
       "  '48843': 'Philochristus: Memoirs of a Disciple of the Lord (English) (as Author)',\n",
       "  '56843': 'Silanus the Christian (English) (as Author)'},\n",
       " 'wiki_info': {'en': 'http://en.wikipedia.org/wiki/Edwin_Abbott_Abbott'},\n",
       " 'authorabstracts': {},\n",
       " 'literarymovements': [],\n",
       " 'authornum': 'a64'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some test code to check insertions \n",
    "authorscollection.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exporting and importing the database as json dumps and loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the entire database into jsons\n",
    "\n",
    "if not os.path.isdir('./data/mongo_dumps'):  \n",
    "    os.mkdir(\"data/mongo_dumps\")\n",
    "with open(\"./data/mongo_dumps/jsondump_authors_mongo.json\", \"w+\") as jsondump_authors_mongo: \n",
    "    jsondump_authors_mongo.write(dumps(authorscollection.find()))\n",
    "    jsondump_authors_mongo.close()\n",
    "    \n",
    "with open(\"./data/mongo_dumps/jsondump_books_mongo.json\", \"w+\") as jsondump_books_mongo: \n",
    "    jsondump_books_mongo.write(dumps(bookscollection.find()))\n",
    "    jsondump_books_mongo.close()\n",
    "\n",
    "# # loading the exported jsons into the db\n",
    "# with open(\"/data/mongo_dumps/jsondump_books_mongo.json\", \"r\") as jsondump_authors_mongo: \n",
    "    \n",
    "#     try: # try inserting all at a go\n",
    "#         authorscollection.insert_many(loads(jsondump_authors_mongo.read()))\n",
    "#     except: # insert one by one in event of duplicates already existing in db\n",
    "#         try: \n",
    "#             for doc in loads(jsondump_authors_mongo.read()):\n",
    "#             authorscollection.insert_one(doc)\n",
    "#         except:\n",
    "#             continue \n",
    "    \n",
    "# with open(\"/data/mongo_dumps/jsondump_books_mongo.json\", \"r\") as jsondump_books_mongo: \n",
    "#     try: # try inserting all at a go\n",
    "#         bookscollection.insert_many(loads(jsondump_books_mongo.read()))\n",
    "#     except: # insert one by one in event of duplicates already existing in db\n",
    "#         try: \n",
    "#             for doc in loads(jsondump_books_mongo.read()):\n",
    "#             bookscollection.insert_one(doc)\n",
    "#         except:\n",
    "#             continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Exploring the raw corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science fiction', 29),\n",
       " ('romanticism', 18),\n",
       " (\"children's literature\", 14),\n",
       " ('fantasy', 14),\n",
       " ('poetry', 9),\n",
       " ('romance novel', 8),\n",
       " ('fiction', 7),\n",
       " ('horror fiction', 7),\n",
       " ('fantasy fiction', 6),\n",
       " ('naturalism', 6)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find number of literary movements in corpus, and the most common \n",
    "literary_movements = []\n",
    "for i in authorscollection.find():\n",
    "    literary_movements.extend(i[\"literarymovements\"])\n",
    "\n",
    "literary_movements_counter = collections.Counter(literary_movements)\n",
    "literary_movements_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5cba59b9936c7d234e4dad6d'),\n",
       " 'booknum': '28',\n",
       " 'selected_sents': ['The Four Oxen and the Lion A Lion used to prowl about a field in which Four Oxen used to dwell.',\n",
       "  'The Fox and the Stork At one time the Fox and the Stork were on visiting terms and seemed very good friends.',\n",
       "  'At first he turned to flee, but finding that the Lion did not pursue him, he turned back and went up to him.',\n",
       "  'This proposal met with general applause, until an old mouse got up and said: \"That is all very well, but who is to bell the Cat?\"',\n",
       "  'Then one of them stooped down to stroke it, but the Serpent raised its head and put out its fangs and was about to sting the child to death.',\n",
       "  'He pointed out how inconvenient a tail was when they were pursued by their enemies, the dogs; how much it was in the way when they desired to sit down and hold a friendly conversation with one another.',\n",
       "  '\"Why, what is that?\"',\n",
       "  'But after a day or two the Members began to find that they themselves were not in a very active condition: the Hands could hardly move, and the Mouth was all parched and dry, while the Legs were unable to support the rest.',\n",
       "  '\"My hands are numb with the cold,\" said the Man, \"and my breath warms them.\"',\n",
       "  'This pleased the boy so much that a few days afterwards he tried the same trick, and again the villagers came to his help.',\n",
       "  'The Tree and the Reed \"Well, little one,\" said a Tree to a Reed that was growing at its foot, \"why do you not plant your feet deeply in the ground, and raise your head boldly in the air as I do?\"',\n",
       "  'Immediately he began to revile and attack his enemy.',\n",
       "  'So he took him into the public gardens and showed him a statue of Hercules overcoming the Lion and tearing his mouth in two.',\n",
       "  'Then again: Keep what you have.',\n",
       "  '\"It is sure to cry soon, and a daintier morsel I haven\\'t had for many a long day.\"',\n",
       "  'Gratitude is the sign of noble souls.',\n",
       "  '\"The porridge is too hot, and my breath will cool it.\"',\n",
       "  'In the struggle the Donkey fell over the bridge, and his fore-feet being tied together he was drowned.',\n",
       "  '\"Oh, it is nothing,\" said the Dog.',\n",
       "  'So a day was appointed when the Fox should visit the Stork; but when they were seated at table all that was for their dinner was contained in a very long-necked jar with a narrow mouth, in which the Fox could not insert his snout, so all he could manage to do was to lick the outside of the jar.',\n",
       "  \"The Ant and the Grasshopper In a field one summer's day a Grasshopper was hopping about, chirping and singing to its heart's content.\",\n",
       "  '\"And what do you do that for?\"',\n",
       "  'The Fox thought first of one way, then of another, and while he was debating the hounds came nearer and nearer, and at last the Fox in his confusion was caught up by the hounds and soon killed by the huntsmen.',\n",
       "  'When they had assembled together the Fox proposed that they should all do away with their tails.',\n",
       "  '\"You will all agree,\" said he, \"that our chief danger consists in the sly and treacherous manner in which the enemy approaches us.',\n",
       "  'If you put me back into the river I shall soon grow, then you can make a fine meal off me.\"',\n",
       "  'The Man aimed a blow at his little enemy, but acks palm came on his head instead; again the Fly tormented him, but this time the Man was wiser and said: \"You will only injure yourself if you take notice of despicable enemies.\"',\n",
       "  'The Jay and the Peacock A Jay venturing into a yard where Peacocks used to walk, found there a number of feathers which had fallen from the Peacocks when they were moulting.',\n",
       "  'By this means we should always know when she was about, and could easily retire while she was in the neighbourhood.\"',\n",
       "  'What, going so soon?\"',\n",
       "  'When the tide rose they both floated off down the stream.',\n",
       "  \"The Serpent and the File A Serpent in the course of its wanderings came into an armourer's shop.\",\n",
       "  'Then came the turn of the Envious man, who could not bear to think that his neighbour had any joy at all.',\n",
       "  '\"I have a whole bag of tricks,\" he said, \"which contains a hundred ways of escaping my enemies.\"',\n",
       "  'The Dog in the Manger A Dog looking out for its afternoon nap jumped into the Manger of an Ox and lay there cosily upon the straw.',\n",
       "  '\"What do you do that for?\"',\n",
       "  '\"That is only the place where the collar is put on at night to keep me chained up; it chafes a bit, but one soon gets used to it.\"',\n",
       "  'The Hunter agreed, but said: \"If you desire to conquer the Stag, you must permit me to place this piece of iron between your jaws, so that I may guide you with these reins, and allow this saddle to be placed upon your back so that I may keep steady upon you as we follow after the enemy.\"',\n",
       "  '\"Just the thing to quench my thirst,\" quoth he.',\n",
       "  'But they hadn\\'t gone far when they passed two women, one of whom said to the other: \"Shame on that lazy lout to let his poor little son trudge along.\"',\n",
       "  'The Fox and the Mask A Fox had by some means got into the store-room of a theatre.',\n",
       "  'But soon the Ox, returning from its afternoon work, came up to the Manger and wanted to eat some of the straw.',\n",
       "  'Miss Puss, who had been looking on, said: \"Better one safe way than a hundred on which you cannot reckon.\"',\n",
       "  'But when the Man raised his spoon to his mouth he began blowing upon it.',\n",
       "  'Luckily at the last moment peace was made, and no battle took place, so the Bat came to the Birds and wished to join in the rejoicings, but they all turned against him and he had to fly away.',\n",
       "  'He expressed his desire to see his Mother, and to speak with her before he was led to execution, and of course this was granted.',\n",
       "  'said the Grasshopper; we have got plenty of food at present.\"',\n",
       "  'All the bystanders were horrified, and asked him what he could mean by such brutal and inhuman conduct.',\n",
       "  'The Trees were good-natured and gave him one of their branches.',\n",
       "  'Then he took another pebble and dropped that into the Pitcher.',\n",
       "  'In a rage he turned round upon it and tried to dart his fangs into it; but he could do no harm to heavy iron and had soon to give over his wrath.',\n",
       "  '\"When I was young I began with stealing little things, and brought them home to Mother.',\n",
       "  'The Peacock and Juno A Peacock once placed a petition before Juno desiring to have the voice of a nightingale in addition to his other attractions; but Juno refused his request.',\n",
       "  'But as soon as he came near to Androcles he recognised his friend, and fawned upon him, and licked his hands like a friendly dog.',\n",
       "  '\"I will not apologise for the dinner,\" said the Stork: \"One bad turn deserves another.\"',\n",
       "  'Turning round again with a One, Two, Three, he jumped up, but with no greater success.',\n",
       "  \"The Lamb that belonged to the sheep, whose skin the Wolf was wearing, began to follow the Wolf in the Sheep's clothing; so, leading the Lamb a little apart, he soon made a meal off her, and for some time he succeeded in deceiving the sheep, and enjoying hearty meals.\",\n",
       "  'Self-conceit may lead to self-destruction.',\n",
       "  \"Avaricious and Envious Two neighbours came before Jupiter and prayed him to grant their hearts' desire.\",\n",
       "  '\"Why bother about winter?\"',\n",
       "  'A hedgehog strolling by took pity upon the Fox and went up to him: \"You are in a bad way, neighbour,\" said the hedgehog; \"shall I relieve you by driving off those Mosquitoes who are sucking your blood?\"',\n",
       "  'Soon the Lion was let loose from his den, and rushed bounding and roaring towards his victim.',\n",
       "  'So he cast his net into the river and soon drew it forth filled with fish.',\n",
       "  '\"I am much too small for your eating just now.',\n",
       "  'The Tortoise and the Birds A Tortoise desired to change its place of residence, so he asked an Eagle to carry him to his new home, promising her a rich reward for her trouble.',\n",
       "  'He tried, and he tried, but at last had to give up in despair.',\n",
       "  'The Goose With the Golden Eggs One day a countryman going to the nest of his Goose found there an egg all yellow and glittering.',\n",
       "  'The Emperor and all his Court came to see the spectacle, and Androcles was led out into the middle of the arena.',\n",
       "  '\"It is to punish her,\" he said.',\n",
       "  'The Horse agreed to the conditions, and the Hunter soon saddled and bridled him.',\n",
       "  'So the Waggoner threw down his whip, and knelt down and prayed to Hercules the Strong.',\n",
       "  'The Labourer let him loose, and he flew up to a branch of a tree and said: \"Never believe a captive\\'s promise; that\\'s one thing.',\n",
       "  'He rushed down towards the village calling out \"Wolf, Wolf,\" and the villagers came out to meet him, and some of them stopped with him for a considerable time.',\n",
       "  '\"Ah, Cousin,\" said the Dog.',\n",
       "  'Then the Grasshopper knew: It is best to prepare for the days of necessity.',\n",
       "  'As they were walking along by its side a countryman passed them and said: \"You fools, what is a Donkey for but to ride upon?\"',\n",
       "  'An Ant passed by, bearing along with great toil an ear of corn he was taking to the nest.',\n",
       "  '\"I have always heard say that a nightingale on toast is dainty morsel.\"',\n",
       "  '\"That is all very well,\" said one of the older foxes; \"but I do not think you would have recommended us to dispense with our chief ornament if you had not happened to lose it yourself.\"',\n",
       "  'The Man contended that he and his fellows were stronger than lions by reason of their greater intelligence.',\n",
       "  'All fled at his approach, both men and animals, and he was a proud Ass that day.',\n",
       "  'Then he took another pebble and dropped that into the Pitcher.',\n",
       "  'But soon they passed a group of men, one of whom said: \"See that lazy youngster, he lets his father walk while he rides.\"']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookscollection.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most frequent lengths:  [(6, 462), (4, 407), (24, 390), (30, 371), (26, 361), (28, 360), (33, 356), (32, 353), (23, 350), (35, 350)] \n",
      " max length:  6151 \n",
      " min length:  1\n"
     ]
    }
   ],
   "source": [
    "# find the average sentence length in the corpus \n",
    "sentence_char_lengths = [] \n",
    "for book in bookscollection.find():\n",
    "    # find the number of characters in each sentence within the corpus \n",
    "    sentence_char_lengths.extend(len(sentence) for sentence in book[\"selected_sents\"])\n",
    "\n",
    "sentence_char_lengths_counter = collections.Counter(sentence_char_lengths)\n",
    "print(\"10 most frequent lengths: \", sentence_char_lengths_counter.most_common(10), \"\\n\",\n",
    "      \"max length: \", max(sentence_char_lengths_counter), \"\\n\",\n",
    "      \"min length: \", min(sentence_char_lengths_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most frequent lengths:  [(1, 2225), (6, 1930), (7, 1857), (4, 1839), (5, 1786), (8, 1696), (2, 1679), (10, 1678), (9, 1637), (3, 1559)] \n",
      " max length:  606 \n",
      " min length:  1\n"
     ]
    }
   ],
   "source": [
    "# find the average token length in the corpus \n",
    "sentence_token_lengths = [] \n",
    "for book in bookscollection.find():\n",
    "    # find the number of characters in each sentence within the corpus \n",
    "    sentence_token_lengths.extend(len(sentence.split()) for sentence in book[\"selected_sents\"])\n",
    "\n",
    "sentence_token_lengths_counter = collections.Counter(sentence_token_lengths)\n",
    "print(\"10 most frequent lengths: \", sentence_token_lengths_counter.most_common(10), \"\\n\",\n",
    "      \"max length: \", max(sentence_token_lengths_counter), \"\\n\",\n",
    "      \"min length: \", min(sentence_token_lengths_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataScienceProject-Session1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
