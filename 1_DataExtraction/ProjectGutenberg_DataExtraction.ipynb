{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SXRp4LcnC4I"
   },
   "source": [
    "# Extracting and storing RDF information and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preliminaries: dependencies and seed state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7wQupQrnC4J"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipedia\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import csv, datetime, time, random, string, re, urllib, os, collections  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from urllib.error import HTTPError\n",
    "import pickle \n",
    "with open('randomstate.pickle', 'rb') as f:\n",
    "    random.setstate(pickle.load(f)) \n",
    "# We use random sampling in some of the functions of our program. \n",
    "# To ensure we can replicate the same dataset, we include the use\n",
    "# of the same seed state whenever running this corpus builder.\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from bson import Binary, Code\n",
    "from bson.json_util import dumps,loads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIVWTPMRexID"
   },
   "source": [
    "### General overview\n",
    "\n",
    "Our solution is supported via two classes, their attributes and associated functions. The first class - GutenbergCorpusBuilder - is intended to handle author-title mining on the Project Gutenberg website, filtering and selection as well as storage of overall corpus data. The attributes of this class is designed for easy ingestion into a mongoDB, or similar non-relational, database. The other class - Author - is intended to hold the processes for accessing text files for books, processing them and storing them. The Author class also contains methods for collecting \n",
    "\n",
    "To build a corpus, a user will only need to interact with 2 methods from the GutenbergCorpusBuilder. These are namely, in the order of intended use: (i) get_library; and (ii) populate_corpus. \n",
    "\n",
    "The first method - __get_library__ - will crawl all of the 'Browse by Author' pages on the Project Gutenberg website and collect author information (including books he/she authored as well as wikipedia pages). By passing the 'min_book', 'max_book' as well as 'languages' and 'roles' parameters, the user can balance the content of the corpus in terms of author and book numbers, as well as have it filtered based on language(s) and author role(s). We note in particular, that the author role setting could be become significant for certain machine learning tasks (for instance incorporating books where an author is merely an editor or contributor could lead to degraded model performance for an author-genre classification task). \n",
    "\n",
    "The default values for this function, as well as our setting for the corpus generated are as follows: \n",
    "\n",
    "|Parameter\t|Default setting\t|Setting for this corpus\t|\n",
    "|---\t|---\t|---\t|\n",
    "|min_books   \t|1   \t|3   \t|\n",
    "|max_books   \t|float(inf)   \t|30   \t|\n",
    "|languages   \t|'all'   \t|['english']   \t|\n",
    "|roles   \t|'all'   \t|['as author']   \t|\n",
    "\n",
    "The parameters, their types and default settings are designed with the intention to allow the collection of all books available on Project Gutenberg. For the purpose of our collected corpus, we have chosen the parameters so as to obtain a balanced corpus that selects major as well as minor authors. \n",
    "\n",
    "\n",
    "The second method - __populate_corpus__ - will take the pre-filtered list of author and their books, instantiate a Author object, and begin collecting carefully data on the author and his/her books. At the background, the function will first retrieve all of the literary movements the author is associated with, from DBpedia. The primary information bottleneck lies with these literary movement labels - not all authors have DBpedia pages and for those that do, many do not have literary movements associated to them. As such, we only proceed with the next steps of adding an author to the final corpus if he/she has these literary movement labels. For authors that pass this filter, the method proceeds to extracts and stores the multilingual abstracts for the author. Finally, it proceeds to randomly pick a set of the author's book's (the size of this set is the same for all authors and equivalent to the min_books set for the corpus), and clean and segment the text files in a list of sentences. The cleaning is intended to exclude boilerplate Project Gutenberg metadata as well as book publisher information. An option is provided to clean the text files by exclude the Project Gutenberg metadata more precisely, albeit this extends the time required to process and collect the corpus.  \n",
    "\n",
    "The default values for this function, as well as our setting for the corpus generated are as follows: \n",
    "\n",
    "|Parameter\t|Default setting\t|Setting for this corpus\t|\n",
    "|---\t|---\t|---\t|\n",
    "|sent_num   \t|250   \t|250   \t|\n",
    "|precise_clean   \t|False   \t|True   \t|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUOWBinlnC4Z"
   },
   "source": [
    "### 1. A class to store a corpus obtained from the Project Gutenberg website. \n",
    "\n",
    "The corpus is build with functions within the class that filter the authors and books on the Project Gutenberg website. It also calls on the Author class (below), to process and generate information about sentences from an author's books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfplnCWJnC4Z"
   },
   "outputs": [],
   "source": [
    "class GutenbergCorpusBuilder: \n",
    "    '''\n",
    "    initiates a GutenbergCorpusBuilder object which stores information about selected authors that are found on \n",
    "    the Project Gutenberg(PG) website.\n",
    "    Authors are stored based on their unique PG numerical code. For each author, selected books and their \n",
    "    respective PG URL are stored.\n",
    "    \n",
    "    Inputs: corpusname - string representing name of the corpus being created.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, corpusname):\n",
    "        self.corpusname = corpusname\n",
    "        self.corpusversion = \"v\"+ str(datetime.datetime.now().year) + str(datetime.datetime.now().month) +\\\n",
    "        str(datetime.datetime.now().day)\n",
    "        \n",
    "        # a dictionary containing dictionaries.\n",
    "        # The top level keys - unique numbers for authors on the PG website,\n",
    "        # the values -  dictionaries containing author information: \n",
    "        # keys - 'authorname', 'books_info'; wiki_info'; \n",
    "        # values - strings or embedded dictionaries:\n",
    "        # authorname: string with extracted name;\n",
    "        # books_info: dictionary: key - book ID, value - book title;\n",
    "        # wiki_info: dictionary: key - language; value - wiki link extracted from PG\n",
    "        #\n",
    "        # {author ID: {authorname:'name', books_info:{bookID: book title}, wiki_info: {language: wiki link}}}\n",
    "        self.authors = dict()\n",
    "       \n",
    "        # a dictionary containing sets of sentences selected from each author's filtered books;\n",
    "        # the top level keys are the unique numbers for authors, the values are sets containing\n",
    "        # sentences from an author's book (as strings).\n",
    "        # {author ID: Author()}\n",
    "        self.corpus = dict()\n",
    "        \n",
    "        self.min_books = int # the min_books value passed into the get_library method.\n",
    "        self.max_books = int # the max_books value passed into the get_library method.\n",
    "        \n",
    "    def populate_corpus(self, sent_num=250, precise_clean=False):\n",
    "        '''\n",
    "        For each author in self.authors, generates an Author() class instance, populates all \n",
    "        attributes of the Author() class, adds to self.corpus.\n",
    "        \n",
    "        Inputs | sent_num: int - the total number of sentences to collect for a single author selected for the corpus. \n",
    "        '''\n",
    "        if len(self.authors) > 0:\n",
    "            _counter = 0\n",
    "            for authornum in self.authors: # authornum is a key (an author's unique number)\n",
    "                if authornum not in self.corpus.keys():\n",
    "                    # instantiate an Author()\n",
    "                    authorname = self.authors[authornum][\"authorname\"]\n",
    "                    authorwiki_info = self.authors[authornum][\"wiki_info\"].copy() \n",
    "                    authorbooks_info_keys = list(self.authors[authornum][\"books_info\"].copy().keys())\n",
    "                    \n",
    "                    _author = Author(authorname=authorname, authornum=authornum, \n",
    "                                    min_books=self.min_books)\n",
    "                    \n",
    "                    # run the populate_attributes() to extract and process the information for the author\n",
    "                    _author.populate_attributes(authorwiki_info=authorwiki_info, \n",
    "                                                authorbooks_info_keys=authorbooks_info_keys,\n",
    "                                                sent_num=sent_num, precise_clean=precise_clean)\n",
    "                    \n",
    "                    # store to Author() to corpus \n",
    "                    self.corpus[authornum] = _author\n",
    "\n",
    "                    # append wiki abstract info and literary movement to author's dictionary in \n",
    "                    # self.author so as to easily transmit each author's basic information into mongodb\n",
    "                    self.authors[authornum][\"authorabstracts\"] = _author.authorabstracts\n",
    "                    self.authors[authornum][\"literarymovements\"] = _author.literarymovements\n",
    "                _counter += 1\n",
    "                if _counter%100 == 0:\n",
    "                    print(\"{} authors have been processed, out of {} authors in selections\".format(_counter, len(self.authors)))\n",
    "        else: \n",
    "            print(\"The authors attribute is empty, please run get_library first or \\\n",
    "            check the parameters passed into get_library.\")\n",
    "        \n",
    "    \n",
    "    def get_library(self, min_books=1, max_books=float(\"inf\"), languages = \"all\", roles = \"all\"):\n",
    "        '''\n",
    "        Goes through the PG website's 'sort by author' pages. Extracts author and corresponding book \n",
    "        information that meet a number of selection criterion (see inputs). \n",
    "        \n",
    "        Inputs | \n",
    "            1. min_books: int - the minimum number of books available for an author, which meets the languages \n",
    "            and roles parameters. Default value is 1. \n",
    "            2. languages: either a str \"all\", or a list containing the languages (in lowercase) to count towards \n",
    "            the author's min_books level. The list of languages available can be found here \n",
    "            https://www.gutenberg.org/catalog/. Default is \"all\". \n",
    "            3. roles: either a str \"all\", or a list containing the roles that an author can have in a book. \n",
    "            These include: Commentator, Translator, Contributor, Photographer, Illustrator, Editor.\n",
    "            Default value is \"all\".\n",
    "        Outputs | saves the results to self.authors\n",
    "        '''\n",
    "        charlist = []\n",
    "        charlist[:0] =  [letter for letter in string.ascii_lowercase] + [\"other\"]\n",
    "\n",
    "        library = dict()\n",
    "        for char in charlist:\n",
    "            # Team comment: we select the authors and books via the \"Browse by Author\" lists instead of  \n",
    "            # the \"Browse by Books\" list. Although the latter has a more predictable page structure \n",
    "            # (i.e. 1 book name, followed by 1 author name, recursively), the former includes \n",
    "            # information about the Author's role in the book. We believe that this could have\n",
    "            # a meaningful impact on the predictive capabilities for models on different tasks, \n",
    "            # especially at larger scale.\n",
    "            \n",
    "            link = 'https://www.gutenberg.org/browse/authors/'+ char\n",
    "            page = requests.get(link)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            one_letter = self._unite_authors_nums_books(self._get_authors_numsnames(soup)[0],\\\n",
    "                                                            self._get_authors_numsnames(soup)[1],\\\n",
    "                                                            self._get_bookswiki_info(soup)[0],\\\n",
    "                                                            self._get_bookswiki_info(soup)[1],\\\n",
    "                                                            min_books, max_books, languages, roles)\n",
    "            \n",
    "            library.update(one_letter)\n",
    "            print(\"{} authors from the '{}' alphabetical category have been added.\".format(len(one_letter),char))\n",
    "            \n",
    "            # del variable to clear memory\n",
    "            del soup\n",
    "            \n",
    "            # Put the function to sleep for a randomised number of seconds (non-integer number between \n",
    "            # 0.5 and 4) to mimic human surfing patterns.\n",
    "            time.sleep(random.uniform(0.5,4))\n",
    "            \n",
    "        self.authors = library\n",
    "        self.min_books = min_books\n",
    "        self.max_books = max_books\n",
    "    \n",
    "    def _get_authors_numsnames(self, soup):\n",
    "        '''\n",
    "        A helper function for _unite_authors_nums_books. Extracts all author names from a BeautifulSoup \n",
    "        copy  of a 'Browse by Author' page on the PG website. \n",
    "        \n",
    "        Inputs | soup:a BeautifulSoup object - containg a copy of the PG 'Browse by Author' page. \n",
    "        Outputs | a tuple containing two lists: The first contains author's numbers on the page, the \n",
    "        second contains corresponding author's names on the page. \n",
    "        '''\n",
    "        authornames = []\n",
    "        # the author names are stored within the \"name\" attribute under each \"a\" class\n",
    "        # use regex wildcard so that find_all will catch and return all \"a names\" with values\n",
    "        authorname_BSlist = soup.find_all('a', {\"name\":re.compile(\"\\w*\")})\n",
    "\n",
    "        for authorname in authorname_BSlist:\n",
    "            # \\- and \\? to escape special characters. .rstrip to remove trailing whitespaces. \n",
    "            authornames.append(re.sub(r'[0-9,\\-\\?]*', '', authorname.text).rstrip())\n",
    "\n",
    "        authornums = []\n",
    "        # the author numbers are stored within the \"href\" attribute. Every line for a book \n",
    "        # on the page has a \"title\" attribute with the value \"Link to this author\". We will use\n",
    "        # this to shift to only the lines with the author's number. \n",
    "        authornums_BSlist = soup.find_all('a', {\"title\":\"Link to this author\"})\n",
    "\n",
    "        for authornum in authornums_BSlist:\n",
    "            authornums.append(authornum[\"href\"].lstrip(\"#\"))\n",
    "\n",
    "        return authornums, authornames\n",
    "\n",
    "    def _get_bookswiki_info(self, soup):\n",
    "        '''\n",
    "        A helper function for _unite_authors_nums_books. Extracts all the book titles and numbers from a \n",
    "        BeautifulSoup copy of a 'Browse by Author' page on the PG website. Also extracts author wikipedia \n",
    "        link information if it is available on the PG website. \n",
    "        \n",
    "        Inputs | soup:a BeautifulSoup object - containg a copy of the PG 'Browse by Author' page. \n",
    "        Outputs | a tuple containing two lists. \n",
    "            1. The first list contains dictionaries. Each dictionary contains information about an author's \n",
    "            books on PG. this includes: book titles, corresponding PG books numbers, the author's role in \n",
    "            each book, and the language of each book. \n",
    "            2. The second list contains dictionaries. Each dictionary contains information about an author's \n",
    "            wikipedia links on PG. An author's wiki dictionary may be empty, contain 1 link, or more than 1 \n",
    "            link. \n",
    "        '''\n",
    "        books_info = list()\n",
    "        wiki_info = list()\n",
    "\n",
    "        # content under the 'ul' tags: books, links as one list organized by ul\n",
    "        authorsbooks_BSlist = soup.find_all('ul')\n",
    "        # for each ul, access the content: books, links; each book is a bs object\n",
    "\n",
    "        for author in authorsbooks_BSlist:\n",
    "            # there are two classes of attributes within each ul tag. The book information\n",
    "            # 1. title and book PG number is under the 'pgdbetext' class. \n",
    "            books_BSlist = author.find_all(class_='pgdbetext')\n",
    "\n",
    "            authorbooks_info = {}\n",
    "            for book in books_BSlist:\n",
    "                # the book numbers are stored in the href attribute. e.g. \"ebooks/19323\"\n",
    "                booknum = book.find('a')['href'].split(\"/\")[-1]\n",
    "                PG_booktitle = book.text\n",
    "\n",
    "                # storing the information regarding a single author's books in a dictionary\n",
    "                authorbooks_info[booknum]=PG_booktitle\n",
    "            \n",
    "            # appending the dictionary containing one author's books to a list\n",
    "            books_info.append(authorbooks_info)\n",
    "            \n",
    "            # 2. for the author is/are under the 'pgdbxlink' class. \n",
    "            wiki_BSlist = author.find_all(class_='pgdbxlink')\n",
    "\n",
    "            authorwiki_info = {}\n",
    "\n",
    "            for wiki in wiki_BSlist:\n",
    "                # 1. the wiki links are stored in the href attribute. \n",
    "                PG_wikilink = wiki.find('a')['href'] # get the whole link\n",
    "                \n",
    "                # some of the lines tagged \"pgdbxlink\" include \"See also: xxx\" links. \n",
    "                # we filter them out here\n",
    "                if \"wikipedia.org\" in PG_wikilink:\n",
    "\n",
    "                    # 2. because PG stores the link in URL-safe format (e.g. \"\\x\" is \"%\"), we will face \n",
    "                    # issues with non-ASCII characters e.g. á whose URL-safe encoding cannot be passed \n",
    "                    # into the wikipedia package. use urllib.requests.unquote to resolve this \n",
    "                    # https://docs.python.org/2/library/urllib.html#utility-functions \n",
    "                    PG_wikilink = urllib.request.unquote(PG_wikilink)\n",
    "\n",
    "                    # 3. get the language code for the wikipage\n",
    "                    wikilang = re.findall(r'/\\w+', PG_wikilink)[0].strip('/')\n",
    "                    # storing the information regarding a single author's wikipedia links in a dictionary\n",
    "                    authorwiki_info[wikilang] = PG_wikilink\n",
    "\n",
    "            # appending the dictionary containing one author's wikipedia links to a list\n",
    "            wiki_info.append(authorwiki_info)\n",
    "            \n",
    "        return books_info, wiki_info\n",
    "\n",
    "    \n",
    "    def _unite_authors_nums_books(self, authornums, authornames, books_info, wiki_info, min_books, \n",
    "                                  max_books, languages, roles):\n",
    "        '''\n",
    "        A helper function for get_library. \n",
    "        \n",
    "        Inputs | \n",
    "            1. authornums:list - list of author numbers from a \"sort by author\" page on the PG website. \n",
    "            2. authornames:list - list of author names  from a \"sort by author\" page on the PG website. \n",
    "            3. books_info: list - a list containing dictionaries, each of which has information about \n",
    "            an author's books \n",
    "            4. wiki_info: list - a list containing dictionaries, each of which has information about \n",
    "            an author's wikipedia\n",
    "            page, as provided by the PG website. There may be none, one, or more wikilinks for an author. \n",
    "            5. min_books:int - the minimum number of books available for an author, which meets the languages \n",
    "            and roles parameters. default value is 1 (since an author listed on PG will have at least 1 book \n",
    "            to his name).\n",
    "            6, max_books:int - the minimum number of books available for an author, which meets the languages \n",
    "            and roles parameters. default value is infinity.\n",
    "            7. languages:either a str \"all\", or a list containing the languages (in lowercase) to count towards the author's \n",
    "            min_books level. The list of languages available can be found here \n",
    "            https://www.gutenberg.org/catalog/. default is \"all\". \n",
    "            8. roles: either a str \"all\", or a list containing the roles (in lowercase) that an author can \n",
    "            have in a book. These include: commentator, translator, contributor, photographer, illustrator, \n",
    "            commentator, editor. default value is \"all\".\n",
    "        Outputs | a dictionary containing PG numbers for authors who meet the min_books, languages and \n",
    "        roles requirements, as well as information each of these author's books. \n",
    "        '''\n",
    "        # we want to be sure that the authornums, authornames, books_info, and wiki_info are aligned \n",
    "        # before proceeding to merge them. \n",
    "        try:\n",
    "            assert len(authornums)==len(authornames) and len(authornums)==len(books_info) and len(authornums)==len(wiki_info)\n",
    "        except AssertionError as e:\n",
    "            e.args += (\"The length of authornums, authornames and books_info do not match.\",)\n",
    "            raise\n",
    "            \n",
    "        authorbooks_info = dict()\n",
    "        # if default parameters passed into the function, add all authors and their books to the corpus.  \n",
    "        if min_books == None and languages == \"all\" and roles == \"all\":\n",
    "            for i in range(len(authornums)):\n",
    "                authorbooks_info[authornums[i]]=\\\n",
    "                        {\"authorname\": authornames[i], \"books_info\": books_info[i], \"wiki_info\": wiki_info[i]}\n",
    "        else:\n",
    "            # place languages and roles input in sets, for use in .intersection below. \n",
    "            languages_set = set(languages)\n",
    "            roles_set = set(roles)\n",
    "            \n",
    "            for i in range(len(authornums)):\n",
    "                author_bookset = books_info[i]\n",
    "                _topop = []\n",
    "                for book in author_bookset: \n",
    "                    \n",
    "                    # using regex to find text in parentheses. Book language e.g. (English) and author role \n",
    "                    # e.g. (as Author) are contained in parentheses. Some books which are part of a series, \n",
    "                    # have (of N) in their titles too, where N is the number of books in that series. \n",
    "                    title_text_in_parentheses =\\\n",
    "                    re.findall(r'\\(([a-zA-Z]+\\s*[a-zA-Z]*[0-9]*)\\)', author_bookset[book])\n",
    "                    \n",
    "                    # lowercase the text in parentheses and put it into sets. \n",
    "                    _title_text_in_parentheses =\\\n",
    "                    set([i.lower() for i in title_text_in_parentheses])\n",
    "                    \n",
    "                    # if languages is set to \"all\" or if the intersection of _title_text_in_parentheses\n",
    "                    # and languages_set returns a non-empty set, pass to the next check. Otherwise add this \n",
    "                    # book number to the list of books to pop from this author_bookset\n",
    "                    if languages == \"all\" or _title_text_in_parentheses.intersection(languages_set): pass\n",
    "                    else:\n",
    "                        _topop.append(book) \n",
    "                        continue \n",
    "                    # do the same for author's role as for language above\n",
    "                    if roles == \"all\" or _title_text_in_parentheses.intersection(roles_set): pass\n",
    "                    else:\n",
    "                        _topop.append(book) \n",
    "                        continue    \n",
    "                # pop the books that don't meet the language and role specifications. \n",
    "                for pop in _topop:\n",
    "                    books_info[i].pop(pop)\n",
    "                    \n",
    "                # check if number of books meeting the language and role requirements meet the \n",
    "                # min_book requirement \n",
    "                if min_books <= len(books_info[i]) <= max_books:\n",
    "                    authorbooks_info[authornums[i]]=\\\n",
    "                            {\"authorname\": authornames[i], \"books_info\": books_info[i], \n",
    "                             \"wiki_info\": wiki_info[i]}\n",
    "                    \n",
    "        return authorbooks_info \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"There are {} authors entered in this corpus\".format(len(self.corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1WRLJZBtpZK"
   },
   "source": [
    "### 2. A class to store subcorpora obtained from the Project Gutenberg website for each Author. \n",
    "\n",
    "The subcorpus is build with functions within the class that pre-processes each .txt file for filtered author books on the Project Gutenberg website. It also obtains the abstracts and literary movement tags for each author from Wikipedia and DBPedia respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m96uH1j8mzXl"
   },
   "outputs": [],
   "source": [
    "class Author:\n",
    "    '''\n",
    "    Initiates a Author object which stores information about a selected author that is available on \n",
    "    the Project Gutenberg(PG) website. Other information drawn from (i) DBPedia - author literary movements\n",
    "    (ii) wikipedia - multilingual author abstract, (iii) PG - selected sentences from author's texts \n",
    "    \n",
    "    Inputs: authorname: str, authornum:str, authorwiki_info: dict, authorbooks_info_keys:list of numbers in \n",
    "    string, min_books: int\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, authorname, authornum, min_books):\n",
    "        '''\n",
    "        initiates the Author object with the author's name. \n",
    "        \n",
    "        Inputs | authorname: str, authornum:str, authorwiki_info: dict, \n",
    "        authorbooks_info_keys:list of author numbers (in str), min_books: int\n",
    "        '''\n",
    "        self.name = authorname\n",
    "        self.number = authornum\n",
    "        self.min_books = min_books  # the min_book setting at the GutenbergCorpus class that led\n",
    "                                    # to this author's selection for the corpus\n",
    "        \n",
    "        # a dictionary with the book numbers as keys and lists as values. Lists  \n",
    "        # contain strings that have been pre-processed by the segment_sentence method.\n",
    "        self.processed_subcorpus = dict()       \n",
    "        \n",
    "        self.authorabstracts = dict() \n",
    "        self.literarymovements = list()\n",
    "        \n",
    "        \n",
    "    def populate_attributes(self, authorwiki_info, authorbooks_info_keys, sent_num, precise_clean):\n",
    "        '''\n",
    "        A convenience function to call _build_subcorpus, _get_authorabstract and  _get_literarymovement, \n",
    "        which will respectively populate the processed_subcorpus, authorabstract and literarymovements\n",
    "        attributes for this Author instance.  \n",
    "        \n",
    "        Inputs | authorwiki_info: dict, authorbooks_info_keys:list of numbers (in str), min_books: int\n",
    "        Result | stores results to self.literarymovements, self.authorabstracts and self.processed_subcorpus \n",
    "        '''\n",
    "        # check for /data directory, else create for storing files from _build_subcorpus\n",
    "        if not os.path.isdir('./data'):  \n",
    "            os.mkdir(\"data\")\n",
    "        \n",
    "        self._get_literarymovement(authorwiki_info)\n",
    "        \n",
    "        # the information bottleneck is at the dbpedia literary movement labels.\n",
    "        # multilingual wiki abstract and text processing requires a large amount of resources \n",
    "        # so we only do these for authors that we manage to get literary movements for.\n",
    "        if len(self.literarymovements) > 0: \n",
    "            self._get_authorabstract(authorwiki_info)\n",
    "            self._build_subcorpus(authorbooks_info_keys=authorbooks_info_keys, \n",
    "                                  sent_num=sent_num, precise_clean=precise_clean)\n",
    "        \n",
    "    \n",
    "    def _build_subcorpus(self, authorbooks_info_keys, sent_num, precise_clean):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. Selects the books of an author's to extract sentences\n",
    "        from. the number of books is the same as min_book set for the corpus's author selection criteria.\n",
    "        if an author has more books than min_books, a random sampling is done. a basic pre-processing to \n",
    "        remove PG metadata and publisher information is done next. results are written to two sets of csv files. \n",
    "        the first contains only selected sentences, the second contains the entire processed book. Additionally, \n",
    "        the selected sentences for each book are written to plaintext files. \n",
    "        \n",
    "        Inputs | authorbooks_info_keys:list of numbers (in str), min_books: int\n",
    "        Result | saves to two sets of csv files: (i) selected sentences only; (ii) entire pre-processed \n",
    "        book. selected sentences also saved to plaintext files. also stores (i) to self.processed_subcorpus\n",
    "        '''\n",
    "        \n",
    "        _author_cleanbooks = dict()\n",
    "        \n",
    "        if len(authorbooks_info_keys) == self.min_books: \n",
    "            for booknum in authorbooks_info_keys: \n",
    "                all_sentencesinbook =\\\n",
    "                self._cleansegment_book(booknum=booknum, precise_clean=precise_clean)\n",
    "\n",
    "                if len(all_sentencesinbook) > sent_num/self.min_books:\n",
    "                    _author_cleanbooks[booknum] = all_sentencesinbook\n",
    "        else: \n",
    "            # 1. recursively select a number of books until len(_author_cleanbooks) matches min_books\n",
    "            #    at each recursion, apply _cleansegment_book on the book. If the cleaned book meets the \n",
    "            #    length requirement, add to _author_cleanbooks. Do this for up to 10 tries, failing\n",
    "            #    which we will exclude the author and all of his/her books from the corpus.\n",
    "            _tries = 0\n",
    "            _unvisited = set(authorbooks_info_keys)\n",
    "            while len(_author_cleanbooks) < self.min_books and _tries<=10 and len(_unvisited) > 0:\n",
    "                # randomly select min_books number from author. if author only has min_books, \n",
    "                # sampling will return the same set\n",
    "                try: # try to take a random sample (it could fail if _unvisited < sample size)\n",
    "                    _newnums = set(random.sample(_unvisited, self.min_books-len(_author_cleanbooks)))\n",
    "                    _unvisited = _unvisited.difference(_newnums)\n",
    "                    for booknum in _newnums: \n",
    "                        all_sentencesinbook =\\\n",
    "                        self._cleansegment_book(booknum=booknum, precise_clean=precise_clean)\n",
    "\n",
    "                        if len(all_sentencesinbook) > sent_num/self.min_books:\n",
    "                            _author_cleanbooks[booknum] = all_sentencesinbook\n",
    "                    _tries += 1\n",
    "                except: # break the while loop if sampling fails\n",
    "                    break \n",
    "                                \n",
    "        # 2. if min_books still not met, move to return. this effectively excludes author from corpus \n",
    "        if len(_author_cleanbooks) < self.min_books:\n",
    "            return \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # 3. extract k number of sentences from each accepted author's books. k is the total number\n",
    "        #   of sentences required for each author divided by the author min_books set for the corpus \n",
    "        _authors_sentences = dict()\n",
    "        for booknum in _author_cleanbooks: \n",
    "            _sample =\\\n",
    "            random.sample(_author_cleanbooks[booknum], round(sent_num/self.min_books))\n",
    "            # book to temporary dictionary, with the booknum as the key. \n",
    "            _authors_sentences[booknum] = _sample\n",
    "        \n",
    "        # 4. write the cleaned book and sampled sentences to file \n",
    "        for booknum in _authors_sentences: \n",
    "            self._write_tofile(booknum, all_sentencesinbook = _author_cleanbooks[booknum], \n",
    "                               sample_all_sentencesinbook = _authors_sentences[booknum])\n",
    "        \n",
    "        # 5. update the processed_subcorpus attribute for the author, with the sampled sentences\n",
    "        self.processed_subcorpus.update(_authors_sentences)\n",
    "        \n",
    "        \n",
    "    def _write_tofile(self, booknum, all_sentencesinbook, sample_all_sentencesinbook): \n",
    "        '''\n",
    "        A helper function to export processed texts and lists of sentences to csv and plaintext files. \n",
    "        Called by _build_subcorpus. \n",
    "        '''\n",
    "        # 1. write the entire cleaned and segmented book to a csv file. \n",
    "        if not os.path.isdir('./data/wholebook_csv'):\n",
    "            os.mkdir(\"data/wholebook_csv\")\n",
    "        with open(\"./data/wholebook_csv/\"+self.number+\"_\"+booknum+'.csv', 'a') as csv_file:\n",
    "            # we set file open mode to 'a' to append to file instead of overwriting\n",
    "            write_file = csv.writer(csv_file, dialect = 'excel')\n",
    "            write_file.writerow(all_sentencesinbook)\n",
    "            del csv_file # delete to free memory\n",
    "\n",
    "        # 2a. write the book sample to a csv file\n",
    "        if not os.path.isdir('./data/booksample_csv'):\n",
    "            os.mkdir(\"data/booksample_csv\")\n",
    "        with open(\"./data/booksample_csv/\"+self.number+\"_\"+booknum+'.csv', 'a') as csv_file:\n",
    "            write_file = csv.writer(csv_file, dialect = 'excel')\n",
    "            write_file.writerow(sample_all_sentencesinbook)\n",
    "            del csv_file # delete to free memory\n",
    "\n",
    "        # 2b. write the book sample to a txt file\n",
    "        if not os.path.isdir('./data/booksample_txt'):\n",
    "            os.mkdir(\"data/booksample_txt\")\n",
    "        with open(\"./data/booksample_txt/\"+self.number+\"_\"+booknum+'.txt', 'a') as txt_file:\n",
    "            txt_file.writelines(\"\\t\".join(sample_all_sentencesinbook))\n",
    "            del txt_file # delete to free memory\n",
    "\n",
    "            \n",
    "    def _cleansegment_book(self, booknum, precise_clean, \n",
    "                           urlpath = \"https://www.gutenberg.org/files/{}/{}.txt\"):\n",
    "        '''\n",
    "        takes a booknum, navigates to the PG page with the .txt file for this book. uses urlopen to \n",
    "        retrieve the contents of this file. if precise_clean = False, only retrieves lines between the\n",
    "        last \"*START\" and first \"*END\" line in the file. \n",
    "        \n",
    "        Inputs | booknum: int - the unique number on PG for a book, urlpath: str - the url structure for a book's  \n",
    "        page on PG, precise_clean: boolean \n",
    "        Outputs | all_sentencesinbook: list -  a list of sentences after the basic pre-processing \n",
    "        '''    \n",
    "        book_content = []\n",
    "        \n",
    "        # open target_url with the urllib.request.urlopen() method,\n",
    "        # for each line in response, decodes with the expected \n",
    "        # encoding format PG uses for plain .txt book files. \n",
    "        # see https://www.gutenberg.org/wiki/Gutenberg:Readers%27_FAQ#R.35._What_do_the_filenames_of_the_texts_mean.3F\n",
    "        for extenc_pair in [('', 'ascii'), ('-0', \"utf-8\"), ('-8', 'ISO 8859-1')]: \n",
    "            # iterate through likely filename endings and associated encodings on PG\n",
    "            try: \n",
    "                target_url = urlpath.format(booknum,booknum+extenc_pair[0])\n",
    "                with urllib.request.urlopen(target_url) as response: \n",
    "                    for line in response: \n",
    "                        # urlopen reads as bytes, to ease processing, we decode to string.\n",
    "                        # most PG .txt files are encoded in latin-1/ascii format. \n",
    "                        try:\n",
    "                            book_content.append(line.decode(extenc_pair[1]))\n",
    "                        except: # revert to latin-1 in the event of unexpected PG encoding behaviour \n",
    "                            book_content.append(line.decode(\"latin-1\"))\n",
    "                    response.close()\n",
    "                    del response\n",
    "            except HTTPError: \n",
    "                continue\n",
    "                \n",
    "        # remove PG metadata precisely, but slower to execute\n",
    "        if precise_clean == True: \n",
    "            start_index = 0                # index for the start of the text\n",
    "            stop_index = -1  # index for the end of the text  \n",
    "\n",
    "            # Each PG book .txt file is ended with metadata marked with \"* START\" and \"* END\" or \n",
    "            # minor variations. * START-tagged metadata tend to, but don't always just, appear in \n",
    "            # the first 25% of the .txt file, and vice-versa for * END tagged metadata. we split \n",
    "            # the file in the top and bottom thirds and run searches for * START and * END (for \n",
    "            # some savings in search time)\n",
    "            \n",
    "            _2third_marker = round(len(book_content)*0.67)\n",
    "                                         \n",
    "            #1. search for *END tags from the back of the file, for two-thirds of the file\n",
    "            for index_num in range(_2third_marker):\n",
    "                if re.match(r'\\*+\\s*END ', book_content[-index_num]):\n",
    "                    stop_index = -index_num\n",
    "            \n",
    "            #2. search for anomalous *START tags in the last two-thirds of the file, \n",
    "            #   but begining from the, possibly new, stop_index \n",
    "            for index_num in range(-stop_index, _2third_marker):\n",
    "                # searching for the last * END from the back, in the last half of the file \n",
    "                if re.match(r'\\*+\\s*START ', book_content[-index_num]):\n",
    "                    stop_index = -index_num\n",
    "            \n",
    "            #3. finally, search for the last START tag from the front, within the first two-thirds\n",
    "            for index_num in range(_2third_marker):\n",
    "                # searching for the last * START in the first half of the file \n",
    "                if re.match(r'\\*+\\s*START ', book_content[index_num]):\n",
    "                    start_index = index_num \n",
    "            \n",
    "            # slicing the section of the text between the start_index and stop_index. \n",
    "            book_content = book_content[start_index:stop_index]\n",
    "\n",
    "        # join all the text without \"\\r\\n\" i.e. return carriage and newline \n",
    "        clean_book_content = \" \".join([l.strip(\"\\r\\n\") for l in book_content if l != \"\\r\\n\"])\n",
    "        # use nltk's sent_tokenise\n",
    "        all_sentencesinbook = sent_tokenize(clean_book_content)\n",
    "\n",
    "        # strip first and last 10% of lines (as a buffer to avoid collecting generic publishing data)\n",
    "        _10pc = round(len(all_sentencesinbook)*0.10)\n",
    "        all_sentencesinbook = all_sentencesinbook[_10pc:-_10pc]\n",
    "\n",
    "        return all_sentencesinbook\n",
    "\n",
    "\n",
    "    def _get_authorabstract(self, author_wiki_info):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. Gets available author abstract from wikipedia using\n",
    "        the wikipedia python package. \n",
    "        \n",
    "        Input | authorwiki_info: dict\n",
    "        Result | stores results to self.authorabstracts\n",
    "        '''\n",
    "        _abstracts = {}\n",
    "\n",
    "        for wikilang in author_wiki_info: \n",
    "        # set the language \n",
    "            wikipedia.set_lang(wikilang)\n",
    "            wikiname = author_wiki_info[wikilang].split(\"/\")[-1]\n",
    "\n",
    "            try: # without disambiguation: we start with the presumption that PG has \n",
    "                # accurate author wikipedia links. set auto_suggest to False to prevent \n",
    "                # additional (unnecessary) handling of the author page name by the wikipedia package.\n",
    "                wikipage = wikipedia.page(title=wikiname, auto_suggest=False)\n",
    "                _abstracts[wikilang] = wikipage.summary\n",
    "\n",
    "            except PageError: \n",
    "                print(\"There is a PageError resulting with this wikiname: {}\".format(wiki_name) )\n",
    "                pass \n",
    "            except DisambiguationError: \n",
    "                print(\"There is a DisambiguationError resulting with this wikiname: {}\".format(wiki_name))\n",
    "                pass \n",
    "\n",
    "        self.authorabstracts = _abstracts    \n",
    "    \n",
    "      \n",
    "    def _get_literarymovement(self, authorwiki_info):\n",
    "        '''\n",
    "        A helper function for .populate_attributes. takes an author's name, makes a DBpedia query \n",
    "        with the name using the SPARQLWrapper package, \n",
    "        returns the literary movements that the author is associated with. \n",
    "        \n",
    "        Input | authorwiki_info: dict\n",
    "        Result | stores results to self.literarymovements\n",
    "        '''\n",
    "        if len(authorwiki_info) > 0:\n",
    "            # since dbpedia is based off wikipedia, we will use the author's name as in \n",
    "            # the wikipedia link obtained from PG. \n",
    "            _authorwiki_info = authorwiki_info.copy().popitem()\n",
    "            wikiname = _authorwiki_info[1].split('/')[-1].replace(\"_\", \" \")\n",
    "            wikilang = _authorwiki_info[0]\n",
    "\n",
    "            sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "            query = '''SELECT ?text\n",
    "                WHERE {\n",
    "                ?writer rdf:type dbo:Writer ;\n",
    "                foaf:name %r @%s.\n",
    "                {?writer dbo:genre ?genre .}\n",
    "                UNION\n",
    "                {?writer dbo:movement ?genre .}\n",
    "                ?genre rdfs:label ?text\n",
    "                FILTER (lang(?text) = \"en\")\n",
    "                }''' %(wikiname, wikilang)\n",
    "            # using %r for names to handle non-ascii wikinames that get passed as bytes in %s\n",
    "            # see https://pyformat.info/ for e.g. \"Bahá'u'lláh\" becomes \"Bahá\\'u\\'lláh\"\n",
    "            sparql.setQuery(query)\n",
    "            sparql.setReturnFormat(JSON)\n",
    "            results = sparql.query().convert()\n",
    "            genres = set()\n",
    "            for i in range (len(results['results']['bindings'])):\n",
    "                genre = results['results']['bindings'][i]['text']['value']\n",
    "                genres.add((re.sub(r'\\([^)]*\\)', '', genre.lower())).rstrip())\n",
    "            self.literarymovements = list(genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9jRyG8qz1R1"
   },
   "source": [
    "### 3. Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hzx6BXsmMQMk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 authors from the 'a' alphabetical category have been added.\n",
      "297 authors from the 'b' alphabetical category have been added.\n",
      "212 authors from the 'c' alphabetical category have been added.\n",
      "141 authors from the 'd' alphabetical category have been added.\n",
      "61 authors from the 'e' alphabetical category have been added.\n",
      "122 authors from the 'f' alphabetical category have been added.\n",
      "141 authors from the 'g' alphabetical category have been added.\n",
      "210 authors from the 'h' alphabetical category have been added.\n",
      "16 authors from the 'i' alphabetical category have been added.\n",
      "63 authors from the 'j' alphabetical category have been added.\n",
      "68 authors from the 'k' alphabetical category have been added.\n",
      "148 authors from the 'l' alphabetical category have been added.\n",
      "265 authors from the 'm' alphabetical category have been added.\n",
      "39 authors from the 'n' alphabetical category have been added.\n",
      "47 authors from the 'o' alphabetical category have been added.\n",
      "129 authors from the 'p' alphabetical category have been added.\n",
      "3 authors from the 'q' alphabetical category have been added.\n",
      "117 authors from the 'r' alphabetical category have been added.\n",
      "276 authors from the 's' alphabetical category have been added.\n",
      "89 authors from the 't' alphabetical category have been added.\n",
      "17 authors from the 'u' alphabetical category have been added.\n",
      "35 authors from the 'v' alphabetical category have been added.\n",
      "174 authors from the 'w' alphabetical category have been added.\n",
      "1 authors from the 'x' alphabetical category have been added.\n",
      "17 authors from the 'y' alphabetical category have been added.\n",
      "4 authors from the 'z' alphabetical category have been added.\n",
      "2 authors from the 'other' alphabetical category have been added.\n",
      "100 authors have been processed, out of 2789 authors in selections\n",
      "200 authors have been processed, out of 2789 authors in selections\n",
      "300 authors have been processed, out of 2789 authors in selections\n",
      "400 authors have been processed, out of 2789 authors in selections\n",
      "500 authors have been processed, out of 2789 authors in selections\n",
      "600 authors have been processed, out of 2789 authors in selections\n",
      "700 authors have been processed, out of 2789 authors in selections\n",
      "800 authors have been processed, out of 2789 authors in selections\n",
      "900 authors have been processed, out of 2789 authors in selections\n",
      "1000 authors have been processed, out of 2789 authors in selections\n",
      "1100 authors have been processed, out of 2789 authors in selections\n",
      "1200 authors have been processed, out of 2789 authors in selections\n",
      "1300 authors have been processed, out of 2789 authors in selections\n",
      "1400 authors have been processed, out of 2789 authors in selections\n",
      "1500 authors have been processed, out of 2789 authors in selections\n",
      "1600 authors have been processed, out of 2789 authors in selections\n",
      "1700 authors have been processed, out of 2789 authors in selections\n",
      "1800 authors have been processed, out of 2789 authors in selections\n",
      "1900 authors have been processed, out of 2789 authors in selections\n",
      "2000 authors have been processed, out of 2789 authors in selections\n",
      "2100 authors have been processed, out of 2789 authors in selections\n",
      "2200 authors have been processed, out of 2789 authors in selections\n",
      "2300 authors have been processed, out of 2789 authors in selections\n",
      "2400 authors have been processed, out of 2789 authors in selections\n",
      "2500 authors have been processed, out of 2789 authors in selections\n",
      "2600 authors have been processed, out of 2789 authors in selections\n",
      "2700 authors have been processed, out of 2789 authors in selections\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    min_books = 3\n",
    "    max_books = 30\n",
    "    sent_num = 250 \n",
    "    precise_clean=True\n",
    "    # instantiate a GutenbergCorpusBuilder \n",
    "    PGcorpus = GutenbergCorpusBuilder(corpusname=\"PG-eng-author-min{}\".format(min_books))\n",
    "    # start collecting and filtering author and book details from the Project Gutenberg site\n",
    "    PGcorpus.get_library(min_books = min_books, max_books = max_books, \n",
    "                         languages = [\"english\"], roles = [\"as author\"])\n",
    "    # read text files, select sentences, pre-process sentences, store to subcorpora\n",
    "    PGcorpus.populate_corpus(sent_num=sent_num, precise_clean=precise_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 General check of the content of the corpus files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This selected sentences file ./data/booksample_txt/a32063_26521.txt may still contain Gutenberg metadata\n",
      "This selected sentences file ./data/booksample_txt/a3826_19000.txt may still contain Gutenberg metadata\n",
      "This selected sentences file ./data/booksample_txt/a32063_29948.txt may still contain Gutenberg metadata\n",
      "This selected sentences file ./data/booksample_txt/a8240_50876.txt may still contain Gutenberg metadata\n",
      "There are 336 files in total within the corpus\n"
     ]
    }
   ],
   "source": [
    "# general sense\n",
    "import glob\n",
    "filenames = glob.glob(\"./data/booksample_txt/*.txt\")\n",
    "_to_delete = []\n",
    "for i in filenames: \n",
    "    with open(i) as file:\n",
    "        if \"gutenberg\" in file.read():\n",
    "            _to_delete.append(i)\n",
    "            print (\"This selected sentences file {} may still contain Gutenberg metadata\".format(i))\n",
    "print(\"There are {} files in total within the corpus\".format(len(filenames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__4 out of the 336 files in the corpus, about 1.2%, still contains Gutenberg metadata. These files belong to 3 authors. The Gutenberg metadata likely got selected as part of the sentences from the books because these books' file structure deviate substantially from the typical structure variants we observed. We will manually delete these 3 authors and their books  from the corpus.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/booksample_txt/a32063_26521.txt',\n",
       " './data/booksample_txt/a3826_19000.txt',\n",
       " './data/booksample_txt/a32063_29948.txt',\n",
       " './data/booksample_txt/a8240_50876.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the associated processed_subcorpus stored in the Author instances for the files/authors above\n",
    "for author in [\"a32063\", \"a3826\", \"a8240\"]:\n",
    "    PGcorpus.corpus[author].processed_subcorpus = dict()\n",
    "    \n",
    "# delete the associated processed_subcorpus stored in the /data directory\n",
    "for filename in _to_delete: \n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKrllplDkyqb"
   },
   "source": [
    "### 4. Test code - informal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJCo6Hhxkxn4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 109 authors entered in this corpus\n"
     ]
    }
   ],
   "source": [
    "# check that corpus contains only english books. it should return nothing. \n",
    "for i in PGcorpus.authors.keys(): \n",
    "    for i2 in PGcorpus.authors[i][\"books_info\"]:\n",
    "        if \"English\" not in PGcorpus.authors[i][\"books_info\"][i2]:\n",
    "            print(PGcorpus.authors[i][\"books_info\"][i2])\n",
    "\n",
    "# check that corpus contains only books where author role is as Author. it should return nothing.\n",
    "for i in PGcorpus.authors.keys(): \n",
    "    for i2 in PGcorpus.authors[i][\"books_info\"]:\n",
    "        if \"Author\" not in PGcorpus.authors[i][\"books_info\"][i2]:\n",
    "            print(PGcorpus.authors[i][\"books_info\"][i2])\n",
    "                                     \n",
    "# return number of authors selected into corpus \n",
    "counter = 0\n",
    "for i in PGcorpus.authors.keys():\n",
    "    if len(PGcorpus.corpus[i].processed_subcorpus) ==3:\n",
    "        counter +=1\n",
    "print (\"There are %d authors entered in this corpus\"%counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pickling the GutenbergCorpus object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picklemaker(filename, objectname):\n",
    "    # open the file for writing\n",
    "    fileObject = open(filename,'wb')\n",
    "\n",
    "    pickle.dump(objectname,fileObject)\n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()\n",
    "    \n",
    "picklemaker(PGcorpus.corpusname+PGcorpus.corpusversion+\".pickle\", PGcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a convenience function to load a previously pickled GutenbergCorpusBuilder object\n",
    "def pickleloader(filename):\n",
    "    # # open the file for writing\n",
    "    fileObject = open(filename,'rb')\n",
    "    \n",
    "    return pickle.load(fileObject)  \n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()\n",
    "\n",
    "PGcorpus = pickleloader(\"PG-eng-author-min3v2019420.pickle\")\n",
    "# run/re-run cells containing GutenbergCorpusBuilder and Author class before loading the pickle file\n",
    "# to provide the unpickler attribute structures of the class for unpickling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UOe2Z-guIIG"
   },
   "source": [
    "### 6. pymongo implementation to store the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAZsdVRoz0xA"
   },
   "outputs": [],
   "source": [
    "# instantiate a MongoClient object. using the URI for the Mongo server. If it is locally hosted, \n",
    "# it is by default on the 27017 port. If using cloud, use the provided URI\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "corpusdb = client[\"corpus\"]\n",
    "authorscollection = corpusdb[\"authors\"]\n",
    "bookscollection = corpusdb[\"books\"]\n",
    "\n",
    "authorscollection.drop()\n",
    "bookscollection.drop()\n",
    "\n",
    "# insert the dicationary with each author's information into the authors collection. Only authors \n",
    "# whose books were eventually admitted into the corpus will be added to the database. \n",
    "for authornum in PGcorpus.authors: \n",
    "    PGcorpus.authors[authornum][\"authornum\"]=authornum\n",
    "    authorscollection.insert_one(PGcorpus.authors[authornum])\n",
    "\n",
    "# insert each of the selected books and their selected sentences into the books collection \n",
    "for authornum in PGcorpus.corpus: \n",
    "    author_subcorpus = PGcorpus.corpus[authornum].processed_subcorpus\n",
    "    if len(author_subcorpus) > 0:   \n",
    "        for booknum in author_subcorpus.keys():\n",
    "            bookscollection.insert_one({\"booknum\": booknum, \"authornum\":authornum, \n",
    "                                        \"selected_sents\": author_subcorpus[booknum]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5cba6218936c7d25094da28a'),\n",
       " 'authorname': 'Abbott Edwin Abbott',\n",
       " 'books_info': {'97': 'Flatland: A Romance of Many Dimensions (English) (as Author)',\n",
       "  '45506': 'Flatland: A Romance of Many Dimensions (English) (as Author)',\n",
       "  '201': 'Flatland: A Romance of Many Dimensions (Illustrated) (English) (as Author)',\n",
       "  '22600': 'How to Write Clearly: Rules and Exercises on English Composition (English) (as Author)',\n",
       "  '54223': 'Onesimus: Memoirs of a Disciple of St. Paul (English) (as Author)',\n",
       "  '48843': 'Philochristus: Memoirs of a Disciple of the Lord (English) (as Author)',\n",
       "  '56843': 'Silanus the Christian (English) (as Author)'},\n",
       " 'wiki_info': {'en': 'http://en.wikipedia.org/wiki/Edwin_Abbott_Abbott'},\n",
       " 'authorabstracts': {},\n",
       " 'literarymovements': [],\n",
       " 'authornum': 'a64'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some test code to check insertions \n",
    "authorscollection.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5cba6219936c7d25094dad6f'),\n",
       " 'booknum': '17249',\n",
       " 'authornum': 'a4501',\n",
       " 'selected_sents': ['Here, as in all of God\\'s creations, like assemble, dislike keep apart; \"for intelligence cleaveth unto intelligence; wisdom receiveth wisdom; truth embraceth truth; virtue loveth virtue; light cleaveth unto light; mercy hath compassion on mercy, and claimeth her own.\"',\n",
       "  'He was always received as a friend, and, indeed, was treated as one of the family.',\n",
       "  'Their heads were close together, the dark-brown one and the one of soft, silken tresses.',\n",
       "  'His companion clasped his arm as if to be protected from some impending danger.',\n",
       "  'Their earthly training was at fault.',\n",
       "  'Early that morning she had tearfully kissed them all good-by and had begun her journey to that haven of rest from old country oppressions--America.',\n",
       "  'At dark he rode into a village at the mouth of a gorge.',\n",
       "  'In his letters he had said nothing about the change in his affairs.',\n",
       "  'asked Remand in a low voice.',\n",
       "  'The man had to give it up.',\n",
       "  'Mr. Janson did right by Rupert, and together they worked and prospered.',\n",
       "  'Now, there must be a plan by which we may be rid of these imperfections, for if we are ever to live in the presence of God, it seems to me that we must be pure and holy, without sin.\"',\n",
       "  'Come with us.\"',\n",
       "  'Not much.\"',\n",
       "  '\"How are you, Signe?\"',\n",
       "  'His conscience smote him a little as he thought of what the young American had said.',\n",
       "  'She opened the book and pointed to the fourth verse of the thirty-eighth chapter of the book of Job.',\n",
       "  '\"I was not the least offended,\" smiled the other.',\n",
       "  'That was where Signe had been born, and had lived most of the eighteen years of her life.',\n",
       "  '\"I have been told that as Jesus entered the prison of the condemned in the spirit world, a murmur of greeting welcomed Him.',\n",
       "  \"Bogstad, I don't believe in deceiving anyone.\",\n",
       "  'These conditions were easily agreed to.',\n",
       "  'Was he losing his mind?',\n",
       "  'His wife must be a pure, perfect creature.',\n",
       "  '\"To secure our claim.',\n",
       "  'Widow Ames died about two weeks after.',\n",
       "  'exclaimed he.',\n",
       "  'Such a beautiful spirit of harmony brooded over the place!',\n",
       "  'He replied that at their next visit to America, they would surely give Dry Bench a call.',\n",
       "  'One evening they were all sitting around the fire in the living room.',\n",
       "  'In the beginning,\" explained the speaker, \"those who enter this order of equality are required to consecrate all their property to the Lord.',\n",
       "  \"I must milk first; and say, I guess the mud's washed off the roof by the looks of things.\",\n",
       "  'Signe gazed towards a rocky island before her.',\n",
       "  'Rupert Ames was in love.',\n",
       "  'asked the brother.',\n",
       "  '\"I like it.\"',\n",
       "  'Once in Minnesota, a young man had made love to her, but she could not return that love, so she was in duty bound not to encourage him.',\n",
       "  'Hills upon hills, covered with pine and fir, stretch away from the lowlands to the distant glacier-clad mountains, and patches of green meadow gleam through the dark pine depths.',\n",
       "  'Marie arose.',\n",
       "  '\"But at last the time of mercy and deliverance came.',\n",
       "  'With quick, eager words he asked us if we had seen someone whom he named and described.',\n",
       "  'asked Rupert, quite fiercely.',\n",
       "  '\"I am a traveling minister of the gospel and I stay wherever there is an opportunity.\"',\n",
       "  'Next morning he was out early, and entered the canyon as the sun began to illumine its rocky domes and cast long shafts of light across the chasm.',\n",
       "  'On each side of the throne were tiers of seats, rising one above the other.',\n",
       "  'True love had awakened in two hearts.',\n",
       "  '*       *       *       *       * Rupert Ames was again the owner of Dry Bench farm, and the next spring they moved into the old home.',\n",
       "  '\"Yes, I see,\" and he looked oddly at her.',\n",
       "  '\"Read here,\" said she.',\n",
       "  '\"We have some lions and few of the rarest animals on these grounds--but I am forgetting that these scenes must be strange to you.',\n",
       "  'I\\'m too weak to do you much good yet.\"',\n",
       "  'I cannot read English good, so you must do de reading.\"',\n",
       "  \"He came up to them and rested his arm tenderly on his mother's shoulder.\",\n",
       "  'We must not disturb him now, but we will sit down here and observe him.',\n",
       "  'A young man was feeding the cows for the night, and Rupert went up to him, and said: \"Good evening, sir; have you any objection to my sleeping in your barn tonight?\"',\n",
       "  'Some months previous he had been discharged, and since then he had operated a small \"tinker\" shop of his own.',\n",
       "  '\"And then His actual coming!',\n",
       "  '\"You will in time,\" said Paulus.',\n",
       "  'As years were added to years, their good works increased, until the Lord said to each of them, Enough.',\n",
       "  'Remand expressed his thought to the guide.',\n",
       "  'These must have the saving ordinances of the gospel performed for them, so that when they some time receive the truth, the necessary rites will have been performed.',\n",
       "  'A man appeared and inquired of the travelers where they were going.',\n",
       "  '\"Then I have no objection, though I don\\'t like tramps around the place.\"',\n",
       "  'Then on one of his visits to the West he found another helpmate for himself and children--a kind-hearted, sweet-souled young woman, born of Danish parents, and reared among the Saints in the valleys of the mountains.',\n",
       "  '\"Yes, I understand that he is going East to study.',\n",
       "  'Signe looked down into the still, deep water and saw her own reflection asking the question over again.',\n",
       "  'The mother and Nina tried to lift her, but they failed.',\n",
       "  'Before, it had been simply a new wonder-land, with untold possibilities in a material way; but added to this there was now the fact that in America the Latter-day Zion was to be built; there the people of God were gathering, were building temples, preparatory to the glorious coming of the Lord.',\n",
       "  'Sometimes the mountains, the houses, and the fences became so jumbled together that he could not distinguish one from the other.',\n",
       "  'The latter gladly accepted the offer, for he had by this time learned that Rupert Ames could give him many practical lessons in farming.',\n",
       "  'Her earth-life had ended sadly, and existence had been bitter ever since, \"Restless and hopeless, I have wandered for a long time,\" she said.',\n",
       "  '\"I\\'ve been a farmer all my life,\" was the reply.',\n",
       "  '\"\\'Yesterday this city was visited by a most destructive fire.',\n",
       "  'Alderman Rupert Ames had been attending the protracted meetings of the city council; this, with other business, kept him away from home for a week.',\n",
       "  'Why that upland strip bordering the mountains should be called \"Dry Bench,\" Miss Wilton, at first, did not understand.',\n",
       "  'She had no desire for food, and the crowded steerage had no attractions.',\n",
       "  'They turned out, it must be admitted, more because of Hr.',\n",
       "  'All that day he walked.',\n",
       "  'In the shanty, Rupert found an old stove glowing with a hot fire, by the side of which he seated himself.',\n",
       "  'The man moved off, but turned again.',\n",
       "  '\"Oh, I\\'m not there all the time,\" he laughed.',\n",
       "  '\"All my life, I could.',\n",
       "  'Christ certainly existed as an intelligent being before He came to this earth--yes, even before the world was.\"']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookscollection.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exporting and importing the database as json dumps and loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the entire database into jsons\n",
    "\n",
    "if not os.path.isdir('./data/mongo_dumps'):  \n",
    "    os.mkdir(\"data/mongo_dumps\")\n",
    "with open(\"./data/mongo_dumps/jsondump_authors_mongo.json\", \"w+\") as jsondump_authors_mongo: \n",
    "    jsondump_authors_mongo.write(dumps(authorscollection.find()))\n",
    "    jsondump_authors_mongo.close()\n",
    "    \n",
    "with open(\"./data/mongo_dumps/jsondump_books_mongo.json\", \"w+\") as jsondump_books_mongo: \n",
    "    jsondump_books_mongo.write(dumps(bookscollection.find()))\n",
    "    jsondump_books_mongo.close()\n",
    "\n",
    "# # loading the exported jsons into the db\n",
    "# with open(\"/data/mongo_dumps/jsondump_books_mongo.json\", \"r\") as jsondump_authors_mongo: \n",
    "    \n",
    "#     try: # try inserting all at a go\n",
    "#         authorscollection.insert_many(loads(jsondump_authors_mongo.read()))\n",
    "#     except: # insert one by one in event of duplicates already existing in db\n",
    "#         try: \n",
    "#             for doc in loads(jsondump_authors_mongo.read()):\n",
    "#             authorscollection.insert_one(doc)\n",
    "#         except:\n",
    "#             continue \n",
    "    \n",
    "# with open(\"/data/mongo_dumps/jsondump_books_mongo.json\", \"r\") as jsondump_books_mongo: \n",
    "#     try: # try inserting all at a go\n",
    "#         bookscollection.insert_many(loads(jsondump_books_mongo.read()))\n",
    "#     except: # insert one by one in event of duplicates already existing in db\n",
    "#         try: \n",
    "#             for doc in loads(jsondump_books_mongo.read()):\n",
    "#             bookscollection.insert_one(doc)\n",
    "#         except:\n",
    "#             continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Exploring the raw corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science fiction', 29),\n",
       " ('romanticism', 18),\n",
       " (\"children's literature\", 14),\n",
       " ('fantasy', 14),\n",
       " ('poetry', 9),\n",
       " ('romance novel', 8),\n",
       " ('fiction', 7),\n",
       " ('horror fiction', 7),\n",
       " ('fantasy fiction', 6),\n",
       " ('naturalism', 6)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find number of literary movements in corpus, and the most common \n",
    "literary_movements = []\n",
    "for i in authorscollection.find():\n",
    "    literary_movements.extend(i[\"literarymovements\"])\n",
    "\n",
    "literary_movements_counter = collections.Counter(literary_movements)\n",
    "literary_movements_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most frequent lengths:  [(26, 256), (29, 248), (27, 240), (3, 239), (11, 239), (24, 232), (38, 231), (28, 230), (22, 229), (14, 226)] \n",
      " max length:  1382 \n",
      " min length:  1\n"
     ]
    }
   ],
   "source": [
    "# find the average sentence length in the corpus \n",
    "sentence_char_lengths = [] \n",
    "for book in bookscollection.find():\n",
    "    # find the number of characters in each sentence within the corpus \n",
    "    sentence_char_lengths.extend(len(sentence) for sentence in book[\"selected_sents\"])\n",
    "\n",
    "sentence_char_lengths_counter = collections.Counter(sentence_char_lengths)\n",
    "print(\"10 most frequent lengths: \", sentence_char_lengths_counter.most_common(10), \"\\n\",\n",
    "      \"max length: \", max(sentence_char_lengths_counter), \"\\n\",\n",
    "      \"min length: \", min(sentence_char_lengths_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most frequent lengths:  [(1, 1284), (5, 1224), (6, 1207), (4, 1198), (7, 1110), (2, 1099), (8, 1055), (9, 1038), (3, 1027), (10, 918)] \n",
      " max length:  222 \n",
      " min length:  1\n"
     ]
    }
   ],
   "source": [
    "# find the average token length in the corpus \n",
    "sentence_token_lengths = [] \n",
    "for book in bookscollection.find():\n",
    "    # find the number of characters in each sentence within the corpus \n",
    "    sentence_token_lengths.extend(len(sentence.split()) for sentence in book[\"selected_sents\"])\n",
    "\n",
    "sentence_token_lengths_counter = collections.Counter(sentence_token_lengths)\n",
    "print(\"10 most frequent lengths: \", sentence_token_lengths_counter.most_common(10), \"\\n\",\n",
    "      \"max length: \", max(sentence_token_lengths_counter), \"\\n\",\n",
    "      \"min length: \", min(sentence_token_lengths_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataScienceProject-Session1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
